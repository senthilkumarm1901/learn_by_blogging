{"title":"Understanding Deep Learning Fundamentals - from a coder's viewpoint","markdown":{"yaml":{"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Coding","DL","Python"],"date":"2022-03-04","description":"This blog is inspired from my notes on Kaggle Learn Course on DL. It has code snippets in Keras and Pytorch to put concepts into practices","output-file":"2022-03-04-Intro-to-DL.html","title":"Understanding Deep Learning Fundamentals - from a coder's viewpoint","image":"images/DL_fundamentals_keras_pytorch/DL_NN.png","toc":true},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n<hr>\n\n\nIn this blog, we will code and understand the following: <br>\n- (1) a linear, single neuron network (no hidden layer), <br>\n- (2) the forward propagation function of a deep (non-linear) neuron network with 2 hidden layers,<br>\n- (3) loss function (example considered here is `cross entropy`) <br>\n- (4) the workings of Gradient Descent, <br>\n- (5) how to train the model, and finally, <br>\n- (6) apply all the learnings in building a two-class NN classifier in both `Keras` and `PyTorch` <br>\n\n## 1. Single Neuron\n- A linear unit with 1 input\n\n![image](https://user-images.githubusercontent.com/24909551/156300415-b206cfab-f925-4606-859f-e75208695ee1.png)\n\n- A liniear unit with 3 inputs\n> y = w<sub>0</sub>x<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + b\n\n![image](https://user-images.githubusercontent.com/24909551/156300573-b83fb302-a2be-42c4-a90c-160387c46bb3.png)\n\n\n- In Keras, the input_shape is a list \n> `model = keras.Sequential([layers.Dense(units=1, input_shape=[3]])`\n> where `unit` represents the number of neurons in the `Dense` layer\n> `input_shape` determines the size of input\n- where for \n    - tabular data:\n    > `input_shape = [num_columns]`\n    - image_data:\n    > `input_shape = [height, width, channels]`\n\n- In PyTorch, the same model is defined as follows:\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(3, 1)\n    \n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\nmodel = Model()\n```\n\n- In PyTorch, the model architecture is explicitly given by subclassing nn.Module and implementing the `__init__` and `forward` methods\n\n## 2. Deep Neural Network\n- A dense layer consists of multiple neurons\n\n![image](https://user-images.githubusercontent.com/24909551/156300748-7fab5237-4b16-4195-9b38-8963b61abecf.png)\n\n- Empirical fact: Two dense layers with no activation function is not better than one dense layer\n- Why Activation functions? <br>\n\n![image](https://user-images.githubusercontent.com/24909551/156301224-bc2e01b1-95d3-4194-b35b-4c1dcda01c77.png)\n\n![image](https://user-images.githubusercontent.com/24909551/156304054-df308632-3c04-4497-af93-55bddfb33ebd.png)\n\n- Rectifier function \"rectifies\" the negative values to zero. ReLu puts a \"bend\" in the data and it is better than simple linear regression lines\n- A single neuron with ReLu\n\n![image](https://user-images.githubusercontent.com/24909551/156304468-fc4f589f-3b1a-4722-93d1-414a1d67d605.png)\n\n- A Stack of Dense Layers with ReLu for non-linearity. An example of a Fully-Connected NN:\n\n![image](https://user-images.githubusercontent.com/24909551/156304549-5e184743-523b-428a-8f2b-fc454d5789fa.png)\n\n- the final layer is linear for a regression problem; can have softmax for a classification problem \n\n**Keras Version**: \n\n```python\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# defining a model\nmodel = keras.Sequential([\n    # the hidden ReLu layers\n    layers.Dense(units=4, activation='relu', input_shape=[2]),\n    layers.Dense(units=3, activation='relu'),\n    layers.Dense(unit=1),\n    ])\n```\n\n- The above multilayer NN code in PyTorch can be written as:\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(2, 4)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(4, 3)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(3, 1)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        return x\n\nmodel = Model()\n```\n\n## 3. Loss Function\n\n- Accuracy cannot be used as loss function in NN because as ratio (`num_correct / total predictions`) changes in \"jumps\". We need a loss function that changes smoothly.\n- `Cross Entropy` = - <sup>1</sup>/<sub>N</sub> &sum; (i=1 to N) {(y_actual(i) * log(y_predicted(i)) + (1-y_actual(i)) * log(1-y_predicted(i)) } \n- CE is measure to compute distance between probabilities. \n    - If y_predicted(i) is farther from y_actual(i), CE(i) will be closer to 1. Vice versa, if y_predicted(i) is closer to y_actual(i), then CE(i) will be close to 0 \n\n![image](https://user-images.githubusercontent.com/24909551/156698252-11c2bbbc-ddc8-4204-bbfa-33cab65d15c0.png)\n\n\n## 4. Gradient Descent\n- Gradient Descent is an optimization algorithm that tells the NN \n     - how to change its weight so that\n     - the loss curve shows a descending trend\n\n![image](https://user-images.githubusercontent.com/24909551/156314264-5de54383-e106-4d76-b700-f98a9870dd81.png)\n\nDefinition of terms: \n\n- `Gradient`: Tells us in what direction the NN needs to adjust its weights. It is computed as a partial derivative of a multivariable `cost func`\n- `cost_func`: Simplest one: Mean_absolute_error: mean(abs(y_true-y_pred))\n- `Gradient Descent`: You descend the loss curve to a minimum by reducing the weights `w = w - learning_rate * gradient`\n- `stochastic` - occuring by random chance. batch_size = 1 (OR)\n- `mini batch`: The selection of samples in each mini_batch is by random chance. 1 < mini_batch < size_of_the_data (OR)\n- `batch`: When batch_size == size_of_the_data\n\n\n \n**How GD works**:\n\n- 1. Sample some training data (called `minibatch`) and predict the output by doing forward propagation on the NN architecture\n- 2. Compute loss between predicted_values and target for those samples\n- 3. Adjust weights so that the above loss is minimized in the next iteration\n- Repeat steps 1, 2, and 3 for an entire round of data, then one `epoch` of training is over\n- For every minibatch there is only a small shift in the weights. The size of the shifting of weights is determined by `learning_rate` parameter\n\n\n## 5. How to train the Model\n\n### 5.A. Instantiating the Model\n**Keras Version**:\n\n```python\n# define the optimizer\nmodel.compile(optimizer=\"adam\", loss=\"mae\")\n```\n\n**PyTorch Version**:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Instantiate the model, refer to the Model class created above\nmodel = Model()\n\n# Define the loss function\nloss_function = nn.L1Loss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters())\n```\n\n### 5.B. Training the Model with data\n**Keras Version**:\n\n```python\n# fitting the model\nhistory = model.fit(X_train, y_train, \n    validation_data=(X_valid,y_valid),\n    batch_size=256,\n    epoch=10,\n    )\n\n# plotting the loss curve\nhistory_df = pd.DataFrame(history.history)\nhistory_df['loss'].plot()\n```\n\n**PyTorch Version**:\n```python\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute the loss\n    loss = loss_function(outputs, targets)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n### 5.C. Underfitting and Overfitting\n\n**Underfitting**\n- Capacity Increase\n    - If you increase the number of neurons in each layer (making it wider), it will learn the \"linear\" relationships in the features better\n    - If you add more layers to the network (making it deeper), it will learn the \"non-linear\" relationships in the features better\n    - Decision on `Wider` or `Deeper` networks depends on the dataset \n\n**Overfitting**\n- Early Stopping: Interrupt the training process when the validation loss stops decreasing (stagnant)\n- Early stopping ensures the model is not learning the noises and generalizes well\n\n![image](https://user-images.githubusercontent.com/24909551/156336481-6c2ceb9b-97dc-494f-8e0d-07cc389664f3.png)\n\n- Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured.\n\n**Keras Version**:\n\n```python\nfrom tensorflow.keras.callbacks import EarlyStopping\n# a callback is just a function you want run every so often while the network trains\n\n# defining the early_stopping class\nearly_stopping = EarlyStopping(min_delta = 0.001, # minimum about of change to qualify as improvement\n                               restore_best_weights=True,\n                               patience=20, # number of epochs to wait before stopping\n                              )\n\n\nhistory = model.fit(X_train, y_train, \n    validation_data=(X_valid,y_valid),\n    batch_size=256,\n    epoch=500,\n    callbacks=[early_stopping],\n    verbose=0 #turn off logging\n    )\n    \nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n```\n\n![image](https://user-images.githubusercontent.com/24909551/156341007-74fa6d34-652d-49b4-a238-5d2a802b08bb.png)\n\n\n**PyTorch Version**:\n- In PyTorch, there is no built-in EarlyStopping callback like in Keras\n\n```python\n\n# Define the early stopping criteria\nclass EarlyStopping:\n    def __init__(self, min_delta=0.001, restore_best_weights=True, patience=20):\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.patience = patience\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, loss):\n        if self.best_loss is None:\n            self.best_loss = loss\n        elif loss > self.best_loss + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = loss\n            self.counter = 0\n\n# Instantiate the early stopping class\nearly_stopping = EarlyStopping()\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Train the model and compute the loss\n    model.train()\n    # ...\n    loss = loss_function(outputs, targets)\n    \n    # Call the early stopping function and check for early stopping\n    early_stopping(loss)\n    if early_stopping.early_stop:\n        print(\"Early stopping!\")\n        break\n\n    # ...\n    # Other training loop code\n# After training, if restore_best_weights=True, you can load the best weights\nif early_stopping.restore_best_weights:\n    model.load_state_dict(torch.load('best_model_weights.pt'))\n\n```\n\n### 5.D. Batch Normalization\n\n**Why `BatchNorm`?**\n- Can prevent unstable training behaviour\n    - the changes in weights are proportion to how large the activations of neurons produce\n    - If some unscaled feature causes so much fluctuation in weights after gradient descend, it can cause unstable training behaviour\n- Can cut short the path to reaching the minima in the loss curve (hasten training)\n    - models with `BatchNorm` tend to need fewer epochs for training \n\n\n**What is `BatchNorm`?**\n- On every batch of data subjected to training \n    - normalize the batch data with the batch's mean and standard deviation\n    - multiply them with rescaling parameters that are learnt while training the model\n \n**Keras Version**: <br>\n**Three places where `BatchNorm` can be used**\n1. After a layer\n\n```python\nkeras.Sequential([\n    layers.Dense(16,activation='relu'),\n    layers.BatchNormalization(),\n    ])\n```\n\n2. in-between the linear dense and activation function\n\n```python\nkeras.Sequential([\n    layers.Dense(16),\n    layers.BatchNormalization(),\n    layers.Activation('relu')\n    ])\n```\n3. As the first layer of a network (role would then be similar to similar to Sci-Kit Learn's preprocessor modules like `StandardScaler`)\n\n```python\nkeras.Sequential([\n    layers.BatchNormalization(),\n    layers.Dense(16),\n    layers.Activation('relu')\n    ])\n```\n\n**PyTorch Version**\n\n```python\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n```\n\n### 5.E. LayerNormalization\n\n> It seems that it has been the standard to use batchnorm in CV tasks, and layernorm in NLP tasks\n> [Source](https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm)\n\n> Layer normalization normalizes input across the features instead of normalizing input features across the batch dimension in batch normalization. ... The authors of the paper claims that layer normalization performs better than batch norm in case of RNNs.\n> [Source](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)\n\n\n### 5.F. Dropout\n\nWhat is `Dropout`?\n- It is NN way of regularizing data (to avoid **overfitting**) by\n    - randomly dropping certain proportion of neurons in a layer\n\nHow `Dropout` regularizes?\n- It makes it harder for neural network to overfit for the noise\n\n![image](https://user-images.githubusercontent.com/24909551/156499832-bebdc076-9f1b-4163-b24e-8650988e2fc2.png)\n\n\n```python\nkeras.Sequential([\n    # ....\n    layers.Dropout(0.5), # add dropout before the next layer\n    layers.Dense(512, activation='relu'),\n    # ...\n\n])\n```\n\nWhen adding Dropout, it is important to add more neurons to the layers\n\n```python\n# define the model\nmodel = keras.Sequential([\n    layers.Dense(1024, activation='relu',input_shape=[11],\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu',\n    layers.Dense(1024),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\n# compile the model\nmodel.compile(optimizer='adam', loss='mae')\n\n# fit the model\nhistory = model.fit(X_train, y_train,\n                    validation_set=(X_valid, y_valid),\n                    batch_size=256,\n                    epochs=100,\n                    verbose=1,\n)\n\n# plot the learning curves\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss','val_loss']].plot()\n```\n\n## 6. Building a NN Two-class Classifier\n\n**Let us apply all the learnings from above in building a Binary Classifier**\n<br>\n\n**Keras Version**: \n- The loss_function used in a binary classifier is `binary_crossentropy`\n- The last layer in a binary classifier is `sigmoid`\n\n![image](https://user-images.githubusercontent.com/24909551/156698746-6f4d2677-f118-4551-8b58-a0d8ec625aa8.png)\n\n```python\n# define the model\nmodel = keras.Sequential([\n    layers.Dense(1024,activation='relu',input_shape=[13]), #13 features\n    layers.Dense(512,activation='relu'), # hidden layer\n    layers.Dense(1,avtiation='sigmoid'), # output sigmoid layer for binary classification\n  ])\n  \n # compile the model with optimizer, loss function and metric function\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metric=['binary_accuracy'] # accuracy metric is not used in the training of the model but just for evaluation\n              )\n     \n# define callback function which is called periodically while training the NN     \nearly_stopping = keras.callbacks.EarlyStopping(min_delta=0.001, #minimum amount of change in loss to qualify as improvement \n                                               patience=10, # no. of epochs with no change happening but to keep trying before stopping\n                                               restor_best_weights=True\n                                               )\n  \n# train the model\nhistory = model.fit(X_train, y_train,,\n                    validation_set=(X_valid,y_valid),\n                    batch_size=512,\n                    epochs=1000,\n                    callbacks=[early_stopping]),\n                    verbose=0, # hide the logging because we have so many epochs\n)\n\n\n# plot the curve after training is over\nhistory_df = pd.DataFrame(history.history)\n\n# plotting the loss and accuracy curves from epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:,['binary_accuracy','val_binary_accuracy']].plot()\n\nprint(\"Best Training Accuracy {:.04f}\".format(history_df['binary_accuracy'].max())\nprint(\"Best Validation Accuracy {:.04f}\".format(history_df['val_binary_accuracy'].max())\n\nprint(\"Best Training Loss {:04f}\".format(history_df['loss'].min())\nprint(\"Best Validation Loss {:.04f}\".format(history_df['val_loss'].min())\n\n\n# predicting from a trained model\ny_test_predicted = model.predict_classes(X_test)\nprint(y_test_predicted[0:5])\n# [0, 1, 1, 0, 0]\n\ny_test_predicted_proba = model.predict_proba(X_test)\nprint(y_test_predicted_proba[0:5])\n# [0.08, 0.82, 0.78, 0.01, 0.0]\n\n```\n\n**PyTorch Version**:\n\n```python\nimport torch\nimport torch.nn as nn\n\n# building the model class by sub-classing nn.Module\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n# instantiating the model \nmodel = BinaryClassifier()\n\n\n# defining the loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Training loop\n\nmodel.train()  # Enable training mode\n\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n    \nmodel.eval()  # Enable evaluation mode when evaluating\n\n# Use the model for evaluation or inference\n\n```\n\n- We have incorporated both `batch_noamralization` and `dropout` to reduce overfitting in the above PyTorch model\n\n\n**Source**: <br>\n- Kaggle.com/learn (for Keras version of the codes)\n\n","srcMarkdownNoYaml":"\n\n<hr>\n\n# Introduction\n\nIn this blog, we will code and understand the following: <br>\n- (1) a linear, single neuron network (no hidden layer), <br>\n- (2) the forward propagation function of a deep (non-linear) neuron network with 2 hidden layers,<br>\n- (3) loss function (example considered here is `cross entropy`) <br>\n- (4) the workings of Gradient Descent, <br>\n- (5) how to train the model, and finally, <br>\n- (6) apply all the learnings in building a two-class NN classifier in both `Keras` and `PyTorch` <br>\n\n## 1. Single Neuron\n- A linear unit with 1 input\n\n![image](https://user-images.githubusercontent.com/24909551/156300415-b206cfab-f925-4606-859f-e75208695ee1.png)\n\n- A liniear unit with 3 inputs\n> y = w<sub>0</sub>x<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + b\n\n![image](https://user-images.githubusercontent.com/24909551/156300573-b83fb302-a2be-42c4-a90c-160387c46bb3.png)\n\n\n- In Keras, the input_shape is a list \n> `model = keras.Sequential([layers.Dense(units=1, input_shape=[3]])`\n> where `unit` represents the number of neurons in the `Dense` layer\n> `input_shape` determines the size of input\n- where for \n    - tabular data:\n    > `input_shape = [num_columns]`\n    - image_data:\n    > `input_shape = [height, width, channels]`\n\n- In PyTorch, the same model is defined as follows:\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(3, 1)\n    \n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\nmodel = Model()\n```\n\n- In PyTorch, the model architecture is explicitly given by subclassing nn.Module and implementing the `__init__` and `forward` methods\n\n## 2. Deep Neural Network\n- A dense layer consists of multiple neurons\n\n![image](https://user-images.githubusercontent.com/24909551/156300748-7fab5237-4b16-4195-9b38-8963b61abecf.png)\n\n- Empirical fact: Two dense layers with no activation function is not better than one dense layer\n- Why Activation functions? <br>\n\n![image](https://user-images.githubusercontent.com/24909551/156301224-bc2e01b1-95d3-4194-b35b-4c1dcda01c77.png)\n\n![image](https://user-images.githubusercontent.com/24909551/156304054-df308632-3c04-4497-af93-55bddfb33ebd.png)\n\n- Rectifier function \"rectifies\" the negative values to zero. ReLu puts a \"bend\" in the data and it is better than simple linear regression lines\n- A single neuron with ReLu\n\n![image](https://user-images.githubusercontent.com/24909551/156304468-fc4f589f-3b1a-4722-93d1-414a1d67d605.png)\n\n- A Stack of Dense Layers with ReLu for non-linearity. An example of a Fully-Connected NN:\n\n![image](https://user-images.githubusercontent.com/24909551/156304549-5e184743-523b-428a-8f2b-fc454d5789fa.png)\n\n- the final layer is linear for a regression problem; can have softmax for a classification problem \n\n**Keras Version**: \n\n```python\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# defining a model\nmodel = keras.Sequential([\n    # the hidden ReLu layers\n    layers.Dense(units=4, activation='relu', input_shape=[2]),\n    layers.Dense(units=3, activation='relu'),\n    layers.Dense(unit=1),\n    ])\n```\n\n- The above multilayer NN code in PyTorch can be written as:\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(2, 4)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(4, 3)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(3, 1)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        return x\n\nmodel = Model()\n```\n\n## 3. Loss Function\n\n- Accuracy cannot be used as loss function in NN because as ratio (`num_correct / total predictions`) changes in \"jumps\". We need a loss function that changes smoothly.\n- `Cross Entropy` = - <sup>1</sup>/<sub>N</sub> &sum; (i=1 to N) {(y_actual(i) * log(y_predicted(i)) + (1-y_actual(i)) * log(1-y_predicted(i)) } \n- CE is measure to compute distance between probabilities. \n    - If y_predicted(i) is farther from y_actual(i), CE(i) will be closer to 1. Vice versa, if y_predicted(i) is closer to y_actual(i), then CE(i) will be close to 0 \n\n![image](https://user-images.githubusercontent.com/24909551/156698252-11c2bbbc-ddc8-4204-bbfa-33cab65d15c0.png)\n\n\n## 4. Gradient Descent\n- Gradient Descent is an optimization algorithm that tells the NN \n     - how to change its weight so that\n     - the loss curve shows a descending trend\n\n![image](https://user-images.githubusercontent.com/24909551/156314264-5de54383-e106-4d76-b700-f98a9870dd81.png)\n\nDefinition of terms: \n\n- `Gradient`: Tells us in what direction the NN needs to adjust its weights. It is computed as a partial derivative of a multivariable `cost func`\n- `cost_func`: Simplest one: Mean_absolute_error: mean(abs(y_true-y_pred))\n- `Gradient Descent`: You descend the loss curve to a minimum by reducing the weights `w = w - learning_rate * gradient`\n- `stochastic` - occuring by random chance. batch_size = 1 (OR)\n- `mini batch`: The selection of samples in each mini_batch is by random chance. 1 < mini_batch < size_of_the_data (OR)\n- `batch`: When batch_size == size_of_the_data\n\n\n \n**How GD works**:\n\n- 1. Sample some training data (called `minibatch`) and predict the output by doing forward propagation on the NN architecture\n- 2. Compute loss between predicted_values and target for those samples\n- 3. Adjust weights so that the above loss is minimized in the next iteration\n- Repeat steps 1, 2, and 3 for an entire round of data, then one `epoch` of training is over\n- For every minibatch there is only a small shift in the weights. The size of the shifting of weights is determined by `learning_rate` parameter\n\n\n## 5. How to train the Model\n\n### 5.A. Instantiating the Model\n**Keras Version**:\n\n```python\n# define the optimizer\nmodel.compile(optimizer=\"adam\", loss=\"mae\")\n```\n\n**PyTorch Version**:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Instantiate the model, refer to the Model class created above\nmodel = Model()\n\n# Define the loss function\nloss_function = nn.L1Loss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters())\n```\n\n### 5.B. Training the Model with data\n**Keras Version**:\n\n```python\n# fitting the model\nhistory = model.fit(X_train, y_train, \n    validation_data=(X_valid,y_valid),\n    batch_size=256,\n    epoch=10,\n    )\n\n# plotting the loss curve\nhistory_df = pd.DataFrame(history.history)\nhistory_df['loss'].plot()\n```\n\n**PyTorch Version**:\n```python\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute the loss\n    loss = loss_function(outputs, targets)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n### 5.C. Underfitting and Overfitting\n\n**Underfitting**\n- Capacity Increase\n    - If you increase the number of neurons in each layer (making it wider), it will learn the \"linear\" relationships in the features better\n    - If you add more layers to the network (making it deeper), it will learn the \"non-linear\" relationships in the features better\n    - Decision on `Wider` or `Deeper` networks depends on the dataset \n\n**Overfitting**\n- Early Stopping: Interrupt the training process when the validation loss stops decreasing (stagnant)\n- Early stopping ensures the model is not learning the noises and generalizes well\n\n![image](https://user-images.githubusercontent.com/24909551/156336481-6c2ceb9b-97dc-494f-8e0d-07cc389664f3.png)\n\n- Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured.\n\n**Keras Version**:\n\n```python\nfrom tensorflow.keras.callbacks import EarlyStopping\n# a callback is just a function you want run every so often while the network trains\n\n# defining the early_stopping class\nearly_stopping = EarlyStopping(min_delta = 0.001, # minimum about of change to qualify as improvement\n                               restore_best_weights=True,\n                               patience=20, # number of epochs to wait before stopping\n                              )\n\n\nhistory = model.fit(X_train, y_train, \n    validation_data=(X_valid,y_valid),\n    batch_size=256,\n    epoch=500,\n    callbacks=[early_stopping],\n    verbose=0 #turn off logging\n    )\n    \nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n```\n\n![image](https://user-images.githubusercontent.com/24909551/156341007-74fa6d34-652d-49b4-a238-5d2a802b08bb.png)\n\n\n**PyTorch Version**:\n- In PyTorch, there is no built-in EarlyStopping callback like in Keras\n\n```python\n\n# Define the early stopping criteria\nclass EarlyStopping:\n    def __init__(self, min_delta=0.001, restore_best_weights=True, patience=20):\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.patience = patience\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, loss):\n        if self.best_loss is None:\n            self.best_loss = loss\n        elif loss > self.best_loss + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = loss\n            self.counter = 0\n\n# Instantiate the early stopping class\nearly_stopping = EarlyStopping()\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Train the model and compute the loss\n    model.train()\n    # ...\n    loss = loss_function(outputs, targets)\n    \n    # Call the early stopping function and check for early stopping\n    early_stopping(loss)\n    if early_stopping.early_stop:\n        print(\"Early stopping!\")\n        break\n\n    # ...\n    # Other training loop code\n# After training, if restore_best_weights=True, you can load the best weights\nif early_stopping.restore_best_weights:\n    model.load_state_dict(torch.load('best_model_weights.pt'))\n\n```\n\n### 5.D. Batch Normalization\n\n**Why `BatchNorm`?**\n- Can prevent unstable training behaviour\n    - the changes in weights are proportion to how large the activations of neurons produce\n    - If some unscaled feature causes so much fluctuation in weights after gradient descend, it can cause unstable training behaviour\n- Can cut short the path to reaching the minima in the loss curve (hasten training)\n    - models with `BatchNorm` tend to need fewer epochs for training \n\n\n**What is `BatchNorm`?**\n- On every batch of data subjected to training \n    - normalize the batch data with the batch's mean and standard deviation\n    - multiply them with rescaling parameters that are learnt while training the model\n \n**Keras Version**: <br>\n**Three places where `BatchNorm` can be used**\n1. After a layer\n\n```python\nkeras.Sequential([\n    layers.Dense(16,activation='relu'),\n    layers.BatchNormalization(),\n    ])\n```\n\n2. in-between the linear dense and activation function\n\n```python\nkeras.Sequential([\n    layers.Dense(16),\n    layers.BatchNormalization(),\n    layers.Activation('relu')\n    ])\n```\n3. As the first layer of a network (role would then be similar to similar to Sci-Kit Learn's preprocessor modules like `StandardScaler`)\n\n```python\nkeras.Sequential([\n    layers.BatchNormalization(),\n    layers.Dense(16),\n    layers.Activation('relu')\n    ])\n```\n\n**PyTorch Version**\n\n```python\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n```\n\n### 5.E. LayerNormalization\n\n> It seems that it has been the standard to use batchnorm in CV tasks, and layernorm in NLP tasks\n> [Source](https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm)\n\n> Layer normalization normalizes input across the features instead of normalizing input features across the batch dimension in batch normalization. ... The authors of the paper claims that layer normalization performs better than batch norm in case of RNNs.\n> [Source](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)\n\n\n### 5.F. Dropout\n\nWhat is `Dropout`?\n- It is NN way of regularizing data (to avoid **overfitting**) by\n    - randomly dropping certain proportion of neurons in a layer\n\nHow `Dropout` regularizes?\n- It makes it harder for neural network to overfit for the noise\n\n![image](https://user-images.githubusercontent.com/24909551/156499832-bebdc076-9f1b-4163-b24e-8650988e2fc2.png)\n\n\n```python\nkeras.Sequential([\n    # ....\n    layers.Dropout(0.5), # add dropout before the next layer\n    layers.Dense(512, activation='relu'),\n    # ...\n\n])\n```\n\nWhen adding Dropout, it is important to add more neurons to the layers\n\n```python\n# define the model\nmodel = keras.Sequential([\n    layers.Dense(1024, activation='relu',input_shape=[11],\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu',\n    layers.Dense(1024),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\n# compile the model\nmodel.compile(optimizer='adam', loss='mae')\n\n# fit the model\nhistory = model.fit(X_train, y_train,\n                    validation_set=(X_valid, y_valid),\n                    batch_size=256,\n                    epochs=100,\n                    verbose=1,\n)\n\n# plot the learning curves\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss','val_loss']].plot()\n```\n\n## 6. Building a NN Two-class Classifier\n\n**Let us apply all the learnings from above in building a Binary Classifier**\n<br>\n\n**Keras Version**: \n- The loss_function used in a binary classifier is `binary_crossentropy`\n- The last layer in a binary classifier is `sigmoid`\n\n![image](https://user-images.githubusercontent.com/24909551/156698746-6f4d2677-f118-4551-8b58-a0d8ec625aa8.png)\n\n```python\n# define the model\nmodel = keras.Sequential([\n    layers.Dense(1024,activation='relu',input_shape=[13]), #13 features\n    layers.Dense(512,activation='relu'), # hidden layer\n    layers.Dense(1,avtiation='sigmoid'), # output sigmoid layer for binary classification\n  ])\n  \n # compile the model with optimizer, loss function and metric function\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metric=['binary_accuracy'] # accuracy metric is not used in the training of the model but just for evaluation\n              )\n     \n# define callback function which is called periodically while training the NN     \nearly_stopping = keras.callbacks.EarlyStopping(min_delta=0.001, #minimum amount of change in loss to qualify as improvement \n                                               patience=10, # no. of epochs with no change happening but to keep trying before stopping\n                                               restor_best_weights=True\n                                               )\n  \n# train the model\nhistory = model.fit(X_train, y_train,,\n                    validation_set=(X_valid,y_valid),\n                    batch_size=512,\n                    epochs=1000,\n                    callbacks=[early_stopping]),\n                    verbose=0, # hide the logging because we have so many epochs\n)\n\n\n# plot the curve after training is over\nhistory_df = pd.DataFrame(history.history)\n\n# plotting the loss and accuracy curves from epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:,['binary_accuracy','val_binary_accuracy']].plot()\n\nprint(\"Best Training Accuracy {:.04f}\".format(history_df['binary_accuracy'].max())\nprint(\"Best Validation Accuracy {:.04f}\".format(history_df['val_binary_accuracy'].max())\n\nprint(\"Best Training Loss {:04f}\".format(history_df['loss'].min())\nprint(\"Best Validation Loss {:.04f}\".format(history_df['val_loss'].min())\n\n\n# predicting from a trained model\ny_test_predicted = model.predict_classes(X_test)\nprint(y_test_predicted[0:5])\n# [0, 1, 1, 0, 0]\n\ny_test_predicted_proba = model.predict_proba(X_test)\nprint(y_test_predicted_proba[0:5])\n# [0.08, 0.82, 0.78, 0.01, 0.0]\n\n```\n\n**PyTorch Version**:\n\n```python\nimport torch\nimport torch.nn as nn\n\n# building the model class by sub-classing nn.Module\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n# instantiating the model \nmodel = BinaryClassifier()\n\n\n# defining the loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Training loop\n\nmodel.train()  # Enable training mode\n\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n    \nmodel.eval()  # Enable evaluation mode when evaluating\n\n# Use the model for evaluation or inference\n\n```\n\n- We have incorporated both `batch_noamralization` and `dropout` to reduce overfitting in the above PyTorch model\n\n\n**Source**: <br>\n- Kaggle.com/learn (for Keras version of the codes)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2022-03-04-Intro-to-DL.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Coding","DL","Python"],"date":"2022-03-04","description":"This blog is inspired from my notes on Kaggle Learn Course on DL. It has code snippets in Keras and Pytorch to put concepts into practices","title":"Understanding Deep Learning Fundamentals - from a coder's viewpoint","image":"images/DL_fundamentals_keras_pytorch/DL_NN.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}