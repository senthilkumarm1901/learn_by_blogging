{"title":"PyTorch Fundamentals for NLP - Part 2","markdown":{"yaml":{"aliases":["/pytorch_for_nlp/2021/09/15/Pytorch_for_NLP_PART_II"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP","Coding"],"date":"2021-09-15","description":"This blog post explains how to build Linear Text Classifiers using PyTorch's modules such as `nn.EmbeddingBag` and `nn.Embedding` functions to convert tokenized text into embeddings","hide":false,"image":"images/pytorch_nn/linear_classifier_w_embedding.png","output-file":"2021-09-15-pytorch_for_nlp_part_ii.html","title":"PyTorch Fundamentals for NLP - Part 2","toc":true},"headingText":"1. Introduction","containsRefs":false,"markdown":"\n\n\n\n\nIn this blog piece, let us cover how we can build a\n- `text classification` application using an embedding + fc layer\n\n## 2.Representing Text as Tensors - A Quick Introduction\n\n**How do computers represent text?**\n- Using encodings such as ASCII values to represent each character\n\n![](https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/images/ascii-character-map.png)\n\nSource: github.com/MicrosoftDocs/pytorchfundamentals\n\n\n> Still computers cannot `interpret` the meaning of the words , they just `represent` text as ascii numbers in the above image\n\n\n**How is text converted into embeddings?** <br>\n\n- Two types of representations to convert text into numbers\n\n    - Character-level representation\n    - Word-level representation\n    - Token or sub-word level representation\n    \n- While Character-level and Word-level representations are self explanatory, Token-level representation is a combination of the above two approaches. \n\n<u>Some important terms</u>: <br>\n\n- **Tokenization** (sentence/text --> tokens): In the case sub-word level representations, for example, `unfriendly` will be **tokenized** as `un, #friend, #ly` where `#` indicates the token is a continuation of previous token. \n- This way of tokenization can make the model learnt/trained representations for `friend` and `unfriendly` to be closer to each other in the vector spacy\n\n- **Numericalization** (tokens --> numericals): This is the step where we convert tokens into integers.\n\n- **Vectorization** (numericals --> vectors): This is the process of creating vectors (typically sparse and equal to the length of the vocabulary of the corpus analyzed)\n\n- **Embedding** (numericals --> embeddings): For text data, embedding is a lower dimensional equivalent of a higher dimensional sparse vector. Embeddings are typically dense. Vectors are sparse. \n\n<br>\n\n**Typical Process of Embedding Creation** <br>\n- `text_data` >> `tokens` >> `numericals` >> sparse `vectors` or dense `embeddings` \n\n## 3. Difference between `nn.EmbeddingBag` vs `nn.Embedding` \n\n- `nn.Embedding`: A simple lookup table that looks up embeddings in a fixed dictionary and size.\n\n- `nn.EmbeddingBag`: Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.\n\nSource: PyTorch Official Documentation\n\n![](https://jamesmccaffrey.files.wordpress.com/2021/03/regular_embedding_vs_embedding_bag_diagram.jpg?w=640&h=394)\n\n\n**`nn.Embedding` Explanation**: \n- In the above pic, we can see that the encoding of `men write code` being embedded as `[(0.312,0.385), (0.543, 0.481), (0.203, 0.404)]` where `embed_dim=2`. \n- Looking closer, `men` is embedded as `(0.312,0.385)` and the trailing `<pad>` token is embedded as ` (0.203, 0.404)`\n\n**`nn.EmbeddingBag` Explanation**: <br> \n- In here, there is no padding token. The sentences in a batch are connnected together and saved with their `offsets` array\n- Instead of each word being represented by an embedding vector, each sentence is embedded into a embedding vector\n- This above process of \"computing a single vector for an entire sentence\" is possible also from `nn.Embedding` followed by `torch.mean(dim=1)` or `torch.sum(dim=1)` or `torch.max(dim=1)`\n\n\n**So, when to use `nn.EmbeddingBag`?**\n- nn.EmbeddingBag works better when sequential information of words is not needed. \n- Hence can be used with simple Feed forward NN and not with LSTMs or Transformers (all the embedded words are sent at once and they are sequentially processed either from both directions or unidirectional)\n\nSources: <br>\n- `nn.EmbeddingBag` vs `nn.Embedding` | [link](https://jamesmccaffrey.wordpress.com/2021/04/14/explaining-the-pytorch-embeddingbag-layer/)\n- `nn.Emedding` followed by `torch.mean(dim=1)` | [link](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag)\n\n\n## 4. A Text Classification Pipeline using `nn.EmbeddingBag` + `nn.linear` Layer\n\n- Dataset considered: **AG_NEWS** dataset that consists of 4 classes - `World, Sports, Business and Sci/Tech`\n\n┣━━ **1.Loading dataset** <br>\n┃   ┣━━ `torch.data.utils.datasets.AG_NEWS` <br>\n┣━━ **2.Load Tokenization** <br>\n┃   ┣━━ `torchtext.data.utils.get_tokenizer('basic_english')` <br>\n┣━━ **3.Build vocabulary** <br>\n┃   ┣━━ `torchtext.vocab.build_vocab_from_iterator(train_iterator)` <br>\n┣━━ **4.Create `EmbeddingsBag`**<br>\n┃   ┣━━ Create `collate_fn` to create triplets of label-feature-offsets tensors for every minibatch <br>\n┣━━ **5.Create train, validation and test `DataLoaders`**<br>\n┣━━ **6.Define `Model_Architecture`**<br>\n┣━━ **7.define `training_loop` and `testing_loop` functions**<br>\n┣━━ **8.Train the model and Evaluate on Test Data**<br>\n┣━━ **9.Test the model on sample text**<br>\n\nImporting basic modules\n\n#### 4.1. Loading dataset\n\n#### 4.2. Loading Tokenizer\n\n#### 4.3. Building Vocabulary\n\nLooking at some sample data\n\n#### 4.4. Creating `EmbeddingsBag` related pipelines\n\n- The text pipeline purpose is `to convert text into tokens`\n- the label pipeline is to have labels from 0 to 3\n\n#### 4.4.1 Exploring arguments for nn.EmbeddingBag\n`nn.EmbeddingBag()(input_tensor, offsets)` \n\n##### 4.4.2 Create Collate Function\n\n![](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-4.png)\n\n#### 4.5. Prepare DataLoaders\n\n### 4.6. Model Architecture\n\nInitializing model and embedding dimension\n\n### 4.7. Define `train_loop` and `test_loop` functions\n\n### 4.8 Training the Model\n\n### 4.9.Test the model on sample text \n\n## 5. A Text Classification Pipeline using `nn.Embedding` + `nn.linear` Layer\n\n- Dataset considered: **AG_NEWS** dataset that consists of 4 classes - `World, Sports, Business and Sci/Tech`\n\n(same architecture as previous one, except the change in step 4)\n\n┣━━ **1.Loading dataset** <br>\n┃   ┣━━ `torch.data.utils.datasets.AG_NEWS` <br>\n┣━━ **2.Load Tokenization** <br>\n┃   ┣━━ `torchtext.data.utils.get_tokenizer('basic_english')` <br>\n┣━━ **3.Build vocabulary** <br>\n┃   ┣━━ `torchtext.vocab.build_vocab_from_iterator(train_iterator)` <br>\n┣━━ **4.Create `Embedding`** layer<br>\n┃   ┣━━ Create `collate_fn` (`padify`) to create pairs of label-feature tensors for every minibatch <br>\n┣━━ **5.Create train, validation and test `DataLoaders`**<br>\n┣━━ **6.Define `Model_Architecture`**<br>\n┣━━ **7.define `training_loop` and `testing_loop` functions**<br>\n┣━━ **8.Train the model and Evaluate on Test Data**<br>\n┣━━ **9.Test the model on sample text**<br>\n\nImporting basic modules\n\n**5.1. Loading dataset**\n\n**5.2. Loading Tokenizer**\n\n**5.3. Building Vocabulary**\n\n**5.4. Creating `nn.Embedding` related pipelines**\n\n- The text pipeline purpose is `to convert text into tokens`\n- the label pipeline is to have labels from 0 to 3\n\n**5.4.1 Exploring arguments for `nn.Embedding`**\n\n- `nn.Embedding`: A simple lookup table that stores embeddings of a fixed dictionary and size.\n- Let us create an Embedding module containing 10 tensors of size 3\n\n**5.4.2. Create Collate Function**\n\n**Dealing with Variable Sequence Size**\n\n- Every data point in a text corpus could have different number of tokens\n- For maintaining uniform number of input tokens in texts, we `padify` the text\n- `torch.nn.functional.pad` on a tokenized dataset can `padify` the dataset\n\n![](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-2.png)\nSource: Microsoft PyTorch Docs\n\n**5.5. Prepare DataLoaders**\n\n**5.6 Model Architecture**\n\nInitializing the model hyperaparameters\n\n**5.7. Define train_loop and test_loop functions**\n\n**5.8 Training the model**\n\n**5.9. Test the model on sample text**\n\n## 6. Conclusion\n\n- In this blog piece, we looked at how we can build linear classifiers (without non-linear activation functions) over `nn.EmbeddingBag` and `nn.Embedding` modules\n- In the `nn.EmbeddingBag` method of embedding creation, we did not create `padding tokens` but have to track `offsets` for every minibatch.\n- In the `nn.Embedding` method of creating embeddings, we used `torch.nn.functional.pad` function to ensure all text sequences have fixed length\n\nSources <br>\n\n- MSFT PyTorch NLP Course | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/)\n- Official PyTorch Tutorial on Text Classification using `nn.EmbeddingBag` | [link](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n- MSFT PyTorch Text Classification using `nn.Embedding` | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/4-embeddings)\n\n<hr>\n","srcMarkdownNoYaml":"\n\n\n\n## 1. Introduction\n\nIn this blog piece, let us cover how we can build a\n- `text classification` application using an embedding + fc layer\n\n## 2.Representing Text as Tensors - A Quick Introduction\n\n**How do computers represent text?**\n- Using encodings such as ASCII values to represent each character\n\n![](https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/images/ascii-character-map.png)\n\nSource: github.com/MicrosoftDocs/pytorchfundamentals\n\n\n> Still computers cannot `interpret` the meaning of the words , they just `represent` text as ascii numbers in the above image\n\n\n**How is text converted into embeddings?** <br>\n\n- Two types of representations to convert text into numbers\n\n    - Character-level representation\n    - Word-level representation\n    - Token or sub-word level representation\n    \n- While Character-level and Word-level representations are self explanatory, Token-level representation is a combination of the above two approaches. \n\n<u>Some important terms</u>: <br>\n\n- **Tokenization** (sentence/text --> tokens): In the case sub-word level representations, for example, `unfriendly` will be **tokenized** as `un, #friend, #ly` where `#` indicates the token is a continuation of previous token. \n- This way of tokenization can make the model learnt/trained representations for `friend` and `unfriendly` to be closer to each other in the vector spacy\n\n- **Numericalization** (tokens --> numericals): This is the step where we convert tokens into integers.\n\n- **Vectorization** (numericals --> vectors): This is the process of creating vectors (typically sparse and equal to the length of the vocabulary of the corpus analyzed)\n\n- **Embedding** (numericals --> embeddings): For text data, embedding is a lower dimensional equivalent of a higher dimensional sparse vector. Embeddings are typically dense. Vectors are sparse. \n\n<br>\n\n**Typical Process of Embedding Creation** <br>\n- `text_data` >> `tokens` >> `numericals` >> sparse `vectors` or dense `embeddings` \n\n## 3. Difference between `nn.EmbeddingBag` vs `nn.Embedding` \n\n- `nn.Embedding`: A simple lookup table that looks up embeddings in a fixed dictionary and size.\n\n- `nn.EmbeddingBag`: Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.\n\nSource: PyTorch Official Documentation\n\n![](https://jamesmccaffrey.files.wordpress.com/2021/03/regular_embedding_vs_embedding_bag_diagram.jpg?w=640&h=394)\n\n\n**`nn.Embedding` Explanation**: \n- In the above pic, we can see that the encoding of `men write code` being embedded as `[(0.312,0.385), (0.543, 0.481), (0.203, 0.404)]` where `embed_dim=2`. \n- Looking closer, `men` is embedded as `(0.312,0.385)` and the trailing `<pad>` token is embedded as ` (0.203, 0.404)`\n\n**`nn.EmbeddingBag` Explanation**: <br> \n- In here, there is no padding token. The sentences in a batch are connnected together and saved with their `offsets` array\n- Instead of each word being represented by an embedding vector, each sentence is embedded into a embedding vector\n- This above process of \"computing a single vector for an entire sentence\" is possible also from `nn.Embedding` followed by `torch.mean(dim=1)` or `torch.sum(dim=1)` or `torch.max(dim=1)`\n\n\n**So, when to use `nn.EmbeddingBag`?**\n- nn.EmbeddingBag works better when sequential information of words is not needed. \n- Hence can be used with simple Feed forward NN and not with LSTMs or Transformers (all the embedded words are sent at once and they are sequentially processed either from both directions or unidirectional)\n\nSources: <br>\n- `nn.EmbeddingBag` vs `nn.Embedding` | [link](https://jamesmccaffrey.wordpress.com/2021/04/14/explaining-the-pytorch-embeddingbag-layer/)\n- `nn.Emedding` followed by `torch.mean(dim=1)` | [link](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag)\n\n\n## 4. A Text Classification Pipeline using `nn.EmbeddingBag` + `nn.linear` Layer\n\n- Dataset considered: **AG_NEWS** dataset that consists of 4 classes - `World, Sports, Business and Sci/Tech`\n\n┣━━ **1.Loading dataset** <br>\n┃   ┣━━ `torch.data.utils.datasets.AG_NEWS` <br>\n┣━━ **2.Load Tokenization** <br>\n┃   ┣━━ `torchtext.data.utils.get_tokenizer('basic_english')` <br>\n┣━━ **3.Build vocabulary** <br>\n┃   ┣━━ `torchtext.vocab.build_vocab_from_iterator(train_iterator)` <br>\n┣━━ **4.Create `EmbeddingsBag`**<br>\n┃   ┣━━ Create `collate_fn` to create triplets of label-feature-offsets tensors for every minibatch <br>\n┣━━ **5.Create train, validation and test `DataLoaders`**<br>\n┣━━ **6.Define `Model_Architecture`**<br>\n┣━━ **7.define `training_loop` and `testing_loop` functions**<br>\n┣━━ **8.Train the model and Evaluate on Test Data**<br>\n┣━━ **9.Test the model on sample text**<br>\n\nImporting basic modules\n\n#### 4.1. Loading dataset\n\n#### 4.2. Loading Tokenizer\n\n#### 4.3. Building Vocabulary\n\nLooking at some sample data\n\n#### 4.4. Creating `EmbeddingsBag` related pipelines\n\n- The text pipeline purpose is `to convert text into tokens`\n- the label pipeline is to have labels from 0 to 3\n\n#### 4.4.1 Exploring arguments for nn.EmbeddingBag\n`nn.EmbeddingBag()(input_tensor, offsets)` \n\n##### 4.4.2 Create Collate Function\n\n![](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-4.png)\n\n#### 4.5. Prepare DataLoaders\n\n### 4.6. Model Architecture\n\nInitializing model and embedding dimension\n\n### 4.7. Define `train_loop` and `test_loop` functions\n\n### 4.8 Training the Model\n\n### 4.9.Test the model on sample text \n\n## 5. A Text Classification Pipeline using `nn.Embedding` + `nn.linear` Layer\n\n- Dataset considered: **AG_NEWS** dataset that consists of 4 classes - `World, Sports, Business and Sci/Tech`\n\n(same architecture as previous one, except the change in step 4)\n\n┣━━ **1.Loading dataset** <br>\n┃   ┣━━ `torch.data.utils.datasets.AG_NEWS` <br>\n┣━━ **2.Load Tokenization** <br>\n┃   ┣━━ `torchtext.data.utils.get_tokenizer('basic_english')` <br>\n┣━━ **3.Build vocabulary** <br>\n┃   ┣━━ `torchtext.vocab.build_vocab_from_iterator(train_iterator)` <br>\n┣━━ **4.Create `Embedding`** layer<br>\n┃   ┣━━ Create `collate_fn` (`padify`) to create pairs of label-feature tensors for every minibatch <br>\n┣━━ **5.Create train, validation and test `DataLoaders`**<br>\n┣━━ **6.Define `Model_Architecture`**<br>\n┣━━ **7.define `training_loop` and `testing_loop` functions**<br>\n┣━━ **8.Train the model and Evaluate on Test Data**<br>\n┣━━ **9.Test the model on sample text**<br>\n\nImporting basic modules\n\n**5.1. Loading dataset**\n\n**5.2. Loading Tokenizer**\n\n**5.3. Building Vocabulary**\n\n**5.4. Creating `nn.Embedding` related pipelines**\n\n- The text pipeline purpose is `to convert text into tokens`\n- the label pipeline is to have labels from 0 to 3\n\n**5.4.1 Exploring arguments for `nn.Embedding`**\n\n- `nn.Embedding`: A simple lookup table that stores embeddings of a fixed dictionary and size.\n- Let us create an Embedding module containing 10 tensors of size 3\n\n**5.4.2. Create Collate Function**\n\n**Dealing with Variable Sequence Size**\n\n- Every data point in a text corpus could have different number of tokens\n- For maintaining uniform number of input tokens in texts, we `padify` the text\n- `torch.nn.functional.pad` on a tokenized dataset can `padify` the dataset\n\n![](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-2.png)\nSource: Microsoft PyTorch Docs\n\n**5.5. Prepare DataLoaders**\n\n**5.6 Model Architecture**\n\nInitializing the model hyperaparameters\n\n**5.7. Define train_loop and test_loop functions**\n\n**5.8 Training the model**\n\n**5.9. Test the model on sample text**\n\n## 6. Conclusion\n\n- In this blog piece, we looked at how we can build linear classifiers (without non-linear activation functions) over `nn.EmbeddingBag` and `nn.Embedding` modules\n- In the `nn.EmbeddingBag` method of embedding creation, we did not create `padding tokens` but have to track `offsets` for every minibatch.\n- In the `nn.Embedding` method of creating embeddings, we used `torch.nn.functional.pad` function to ensure all text sequences have fixed length\n\nSources <br>\n\n- MSFT PyTorch NLP Course | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/)\n- Official PyTorch Tutorial on Text Classification using `nn.EmbeddingBag` | [link](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n- MSFT PyTorch Text Classification using `nn.Embedding` | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/4-embeddings)\n\n<hr>\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2021-09-15-pytorch_for_nlp_part_ii.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"aliases":["/pytorch_for_nlp/2021/09/15/Pytorch_for_NLP_PART_II"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP","Coding"],"date":"2021-09-15","description":"This blog post explains how to build Linear Text Classifiers using PyTorch's modules such as `nn.EmbeddingBag` and `nn.Embedding` functions to convert tokenized text into embeddings","hide":false,"image":"images/pytorch_nn/linear_classifier_w_embedding.png","title":"PyTorch Fundamentals for NLP - Part 2"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}