{"title":"Part 2 - A Primer on Statistics","markdown":{"yaml":{"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Statistics"],"date":"2022-02-27","description":"My notes from a LinkedIn Course on Statistics","output-file":"2022-02-03-Primer-on-Statistics.html","title":"Part 2 - A Primer on Statistics","image":"images/statistics/std_dev_image.jpeg","toc":true},"headingText":"1. What is the starting point to unravel a data story?","containsRefs":false,"markdown":"\n\n- Look for the **middle** (mean, median and mode)\n\n### 2. How **spread out** is the data? \n\nAlong with \"Middle\" point, look for `variability`\n- Range: (Max - Min) <br>\n\n    Telling stories with mean and median is still limited. With `Range` it becomes better\n\n    |        | Value |\n    |--------|-------|\n    | Mean   | 60    |\n    | Median | 58    |\n    | Range  | 70    |\n\n- `Standard Deviation`:\n    -  Approx Definition: Average of all data point's distances from the mean\n    - Proper Definition: Square Root of { the mean  of {Square of the difference between each data point and the mean of the data}}\n\n        Std Deviation of the population: <br> &sigma; = &radic;( <sup>&Sigma;(X - &mu;)<sup>2</sup></sup>/ <sub>N</sub> );  \n\n        X  = Value in the data distribution <br>\n        &mu; = Mean of the population <br> \n        N = Number of data points\n\n        Std Deviatin of Sample: <br>\n        s = &radic; ( <sup> &Sigma;(X - x&#772; )<sup>2</sup> </sup> / <sub> (n-1)</sub> )\n\n    - Why the denominator is `n - 1` in `Sample Std Deviation`? <br>\n        - x&#772; is the mean of the sample\n        - By empirical evidence (observed in many datasets), <br> &Sigma;<sub>i=1</sub><sup>N</sup> (x<sub>i</sub> - x&#772;)<sup>2</sup> << &Sigma;<sub>i=1</sub><sup>N</sup> (x<sub>i</sub> - &mu;)<sup>2</sup> <br>\n        - Hence dividing the sample std deviation by (n-1) makes it \"unbiased\" and more towards population std deviation <br>\n        - Another Explanation: There are only (n-1) degrees of freedom in the calculation of (x<sub>i</sub> - x&#772;)\n\n- `Z-Score`: \n    - A particular datapoint's distance from the mean measured in standard deviations\n         Z-score = ( <sup>X - &mu;</sup> / <sub> &sigma; </sub>)\n\n         = (231 - 139) / 41 = 2.24\n         = 231 is 2.24 std deviations from the mean\n         = 112 is -0.66 std deviations from the mean \n\n- Interesting points: \n    - Std deviations of two different datasets cannot be compared (e.g.: Salaries of Data Scientists and Consumption of Fuel in cars)  \n\n### 3. Empirical Rule (or the 68-95-99.7 rule)\n- Most of the datapoints (68%, 95%, 99.7% ) fall within some std deviations (1,2, and 3 respectively) from the mean\n- In other words, 99.7% of the data that is normally distributed will lie 3 standard deviations from the mean.\n- What is normal distribution? <br>\n    The dataset distribution mimics a bell curve\n- Application of the Empirical Rule: \n   - Understanding if a particular data point being an outlier or not \n\n### 4. Central Limit Theorem \n - Given a population of unknown distribution with mean &mu; and finite variance &sigma;<sup>2</sup>, \n     - If we keep sampling `n` values from the the distribution, and compute sample mean as <br>\n     X&#772;<sub>n</sub> ~= ( <sup> X<sub>1</sub> + X<sub>1</sub> + X<sub>n</sub> </sup> / <sub>n</sub>) \n     - As n-> &#8734;, the distribution of the sample means tend to be normal or gaussian (following the bell curve)\n- In simple words, <br>\n    - If you have a population with unknown distribution but with a mean of &mu; and std deviation of &sigma; and take sufficiently large number of samples `n` (with replacement), the distribution of means will be approximately normally distributed \n    <br>\n    <br>\n\n- With the help of CLT, we need not wait for the entire population's data (and the subsequent identification of the population's unknown distribution), we can apply normal distribution principles (like the empirical rule and many more statistical techniques) on the sample means and draw a conclusion about the population\n\nMore about CLT with an example: <br>\n- [Central Limit Theorem’s super power - \"You don’t need to know the population distribution\"](https://towardsdatascience.com/central-limit-theorem-a-real-life-application-f638657686e1)\n\n![](https://upload.wikimedia.org/wikipedia/commons/7/7b/IllustrationCentralTheorem.png)\n\n\n\n### 5. Outlier:\n- Outlier is a relative term. There is no absolute definition (like if a datapoint is 2 or 3 &sigma; away from the mean)\n\n- How to investigate outliers: <br>\n(one should not simply ignore/remove it) \n    - Is this really an outlier?\n    - How did this happen?\n    - What can we learn?\n    - What needs to change (to make it fit into the distribution)?\n        \n        \n**Source**:\n- LinkedIn Courses - Statistics Foundations: Probability and Statistics Foundations: The Basics | [refer](https://github.com/senthilkumarm1901/MyCourseWorkNotes/blob/master/Statistics/CertificatesOfCompletion_Statistics_and_Probability_Foundations.pdf)\n","srcMarkdownNoYaml":"\n\n### 1. What is the starting point to unravel a data story?\n- Look for the **middle** (mean, median and mode)\n\n### 2. How **spread out** is the data? \n\nAlong with \"Middle\" point, look for `variability`\n- Range: (Max - Min) <br>\n\n    Telling stories with mean and median is still limited. With `Range` it becomes better\n\n    |        | Value |\n    |--------|-------|\n    | Mean   | 60    |\n    | Median | 58    |\n    | Range  | 70    |\n\n- `Standard Deviation`:\n    -  Approx Definition: Average of all data point's distances from the mean\n    - Proper Definition: Square Root of { the mean  of {Square of the difference between each data point and the mean of the data}}\n\n        Std Deviation of the population: <br> &sigma; = &radic;( <sup>&Sigma;(X - &mu;)<sup>2</sup></sup>/ <sub>N</sub> );  \n\n        X  = Value in the data distribution <br>\n        &mu; = Mean of the population <br> \n        N = Number of data points\n\n        Std Deviatin of Sample: <br>\n        s = &radic; ( <sup> &Sigma;(X - x&#772; )<sup>2</sup> </sup> / <sub> (n-1)</sub> )\n\n    - Why the denominator is `n - 1` in `Sample Std Deviation`? <br>\n        - x&#772; is the mean of the sample\n        - By empirical evidence (observed in many datasets), <br> &Sigma;<sub>i=1</sub><sup>N</sup> (x<sub>i</sub> - x&#772;)<sup>2</sup> << &Sigma;<sub>i=1</sub><sup>N</sup> (x<sub>i</sub> - &mu;)<sup>2</sup> <br>\n        - Hence dividing the sample std deviation by (n-1) makes it \"unbiased\" and more towards population std deviation <br>\n        - Another Explanation: There are only (n-1) degrees of freedom in the calculation of (x<sub>i</sub> - x&#772;)\n\n- `Z-Score`: \n    - A particular datapoint's distance from the mean measured in standard deviations\n         Z-score = ( <sup>X - &mu;</sup> / <sub> &sigma; </sub>)\n\n         = (231 - 139) / 41 = 2.24\n         = 231 is 2.24 std deviations from the mean\n         = 112 is -0.66 std deviations from the mean \n\n- Interesting points: \n    - Std deviations of two different datasets cannot be compared (e.g.: Salaries of Data Scientists and Consumption of Fuel in cars)  \n\n### 3. Empirical Rule (or the 68-95-99.7 rule)\n- Most of the datapoints (68%, 95%, 99.7% ) fall within some std deviations (1,2, and 3 respectively) from the mean\n- In other words, 99.7% of the data that is normally distributed will lie 3 standard deviations from the mean.\n- What is normal distribution? <br>\n    The dataset distribution mimics a bell curve\n- Application of the Empirical Rule: \n   - Understanding if a particular data point being an outlier or not \n\n### 4. Central Limit Theorem \n - Given a population of unknown distribution with mean &mu; and finite variance &sigma;<sup>2</sup>, \n     - If we keep sampling `n` values from the the distribution, and compute sample mean as <br>\n     X&#772;<sub>n</sub> ~= ( <sup> X<sub>1</sub> + X<sub>1</sub> + X<sub>n</sub> </sup> / <sub>n</sub>) \n     - As n-> &#8734;, the distribution of the sample means tend to be normal or gaussian (following the bell curve)\n- In simple words, <br>\n    - If you have a population with unknown distribution but with a mean of &mu; and std deviation of &sigma; and take sufficiently large number of samples `n` (with replacement), the distribution of means will be approximately normally distributed \n    <br>\n    <br>\n\n- With the help of CLT, we need not wait for the entire population's data (and the subsequent identification of the population's unknown distribution), we can apply normal distribution principles (like the empirical rule and many more statistical techniques) on the sample means and draw a conclusion about the population\n\nMore about CLT with an example: <br>\n- [Central Limit Theorem’s super power - \"You don’t need to know the population distribution\"](https://towardsdatascience.com/central-limit-theorem-a-real-life-application-f638657686e1)\n\n![](https://upload.wikimedia.org/wikipedia/commons/7/7b/IllustrationCentralTheorem.png)\n\n\n\n### 5. Outlier:\n- Outlier is a relative term. There is no absolute definition (like if a datapoint is 2 or 3 &sigma; away from the mean)\n\n- How to investigate outliers: <br>\n(one should not simply ignore/remove it) \n    - Is this really an outlier?\n    - How did this happen?\n    - What can we learn?\n    - What needs to change (to make it fit into the distribution)?\n        \n        \n**Source**:\n- LinkedIn Courses - Statistics Foundations: Probability and Statistics Foundations: The Basics | [refer](https://github.com/senthilkumarm1901/MyCourseWorkNotes/blob/master/Statistics/CertificatesOfCompletion_Statistics_and_Probability_Foundations.pdf)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2022-02-03-Primer-on-Statistics.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Statistics"],"date":"2022-02-27","description":"My notes from a LinkedIn Course on Statistics","title":"Part 2 - A Primer on Statistics","image":"images/statistics/std_dev_image.jpeg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}