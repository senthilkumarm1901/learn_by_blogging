{"title":"Evolution of RNN Architectures for Transfer Learning","markdown":{"yaml":{"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP","DL","ML"],"date":"2020-07-17","description":"Highlighting the non-mathematical essentials in the evolution of RNN architectures in Transfer Learning (before Transformer-based models came to the fore)","output-file":"2020-07-17-evolution_of_RNN_architectures.html","title":"Evolution of RNN Architectures for Transfer Learning","toc":true,"image":"images/RNN/an_unrolled_RNN.png"},"headingText":"Agenda","containsRefs":false,"markdown":"\n\n\n- Why Language Modeling?\n- A short introduction to Language Modeling \n- How Transfer Learning Evolved\n- Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\n- The RNN-based Transfer Learning Architectures - ULMFiT & ELMo\n\n## Why Language Modeling?\n\n- The crux of Transfer Learning in 2 steps: <br> \n - (1) Build a Language Model* that understands the underlying features of the text \n - (2) Fine-tune the Language Model with additional layers for downstream tasks\n \n> Why Language Model for pre-training?<br>\n> *Language modeling can be seen as the ideal source task as it captures many facets of language relevant for downstream tasks, such as long-term dependencies, hierarchical relations and sentiment* (also being self-supervised) <br>\n> \n>Ruder et al in the ULMFiT paper\n_______________________________________________________________________________________________________________\n\n\n## Introduction to Language Modeling\n\nLanguage Model: **A model of the probability of a sequence of words**\n\n- A language model can assign probability to each possible next word. And also, help in assigning a probability to an entire sentence. \n\n### Applications of Language Model\n\n* Speech Recognition: E.g.: P('recognize speech') >> P('wreck a nice beach')\n* Spelling Correction: E.g.: P('I have a gub') << P('I have a gun')\n* Machine Translation: E.g.: P('strong winds') > P('large winds')\n* Optical Character Recognition/ Handwriting Recognition\n* Autoreply Suggestions\n* Text Classification (discussed with python implementation of a simple N-gram model)\n* Text Generation (discussed this with Char-level and Word-level language models)\n_______________________________________________________________________________________________________________\n\n### Evaluation Metrics for LM\n\n#### (1) Perplexity\n- A low perplexity indicates a better Language Model\n\n#### (2) Log Probability\n- Higher the log probability value of a LM in predicting a sample, higher is the confidence for that sample to occur in the distribution\n\nmy detailed notes on these 2 evaluation metrics of Language model is [here](https://senthilkumarm1901.quarto.pub/learn-by-blogging/posts/2020-03-17-introduction_to_statistical_language_modeling.html#evaluation)\n\n\n## How Transfer Learning Evolved\n\n- Stage1: NLP started with rule-based and statistical methodologies\n- Stage2: ML algos such as Naive Bayes, SVM, Trees coupled with bag-of-words word representations\n- Stage3: Recurrent Neural Networks such as LSTM\n- Stage4: RNN based Seq2Seq Transfer Learning Architectures (ULMFit, ELMo, etc.,)\n- Stage 5: Transformers --> 'ImageNet' moment in NLP\n\n![Transformer and Language Models](./images/RNN/TL_and_LM_flow_chart.png)\nSource: Evolution of TL in NLP https://arxiv.org/pdf/1910.07370v1.pdf\n\n## Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\n\nWhy RNNs came into existence?\n- Models such as the **Multi-layer Perceptron Network, vector machines and logistic regression** did not perform well on sequence modelling tasks (e.g.: text_sequence2sentiment_classification)\n- Why? **Lack of memory element** ; **No information retention**\n\n### Cometh the RNNs: \n\n- RNNs attempted to redress this shortcoming by introducing loops within the network, thus allowing the retention of information.\n\n**An unrolled RNN**\n![An un-rolled RNN Cell](./images/RNN/an_unrolled_RNN.png)\n\n#### Advantage of a vanilla RNN:\n- Better than traditional ML algos in retaining information\n\n#### Limitations of a vanilla RNN:\n- RNNs fail to model long term dependencies.\n - the information was often **\"forgotten\"** after the unit activations were multiplied several times by small numbers\n- Vanishing gradient and exploding gradient problems\n\n### Long Short Term Memory (LSTM): \n- a special type of RNN architecture\n- designed to keep information retained for extended number of timesteps\n- each LSTM cell consists of 4 layers (3 sigmoid functioins or gates and 1 tanh function)\n- The 3 sigmoid functions are called `forget`, `update` and `output` gates\n\n### Gated Recurrent Unit (GRU)\n\n- a curtailed version of LSTM\n- retains the resisting vanishing gradient properties of LSTM but GRUs are internally simpler and faster than LSTMs.\n> 1/ `forget` and `update` gates from LSTM are merged into a single `update` gate <br>\n> 2/ The update gate decides how much of previous memory to keep around. <br>\n> 3/ There is a `reset` gate which defines how to combine new input with previous value.\n\n\n\nIf interested in the math behind the RNN architectures, refer [this notebook I wrote in 2019](https://nbviewer.org/github/senthilkumarm1901/GeneralNLP/blob/master/General%20NLP%20Snippets%20and%20Notes/notes/LanguageModels_and_TransferLearning/Part1_Evolution_of_RNN_architectures_in_NLP.ipynb)\n\n### Comparison of performance between GRU and LSTM:\n- GRUs are almost on par with LSTM but with efficient computation. \n- However, with large data LSTMs with higher expressiveness may lead to better results\n\n## The RNN-based Transfer Learning Architectures - ULMFiT & ELMo\n\n### Some history and comparison with CV\n\nHistorically (before the Transformer era), \n- Fine-tuning a LM required millions of in-domain corpus (in other words, transfer learning was not possible)\n- LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier\n\nSource: \n- Evolution of TL in NLP: https://arxiv.org/pdf/1910.07370v1.pdf\n- ULMFiT paper: https://arxiv.org/pdf/1801.06146.pdf\n\n### ULMFit\n\n- Universal Language Model Fine-tuning (ULMFiT) for Text Classification\n - This paper introduces techniques that are essential to fine-tune an LSTM-based Language Model\n - This paper specifically the superior performance of ULMFiT approach  in 6 text classification datasets\n\n### What does ULMFiT propose?\n- Pretrain a LM on a large general-domain corpus and fine-tune it on the target task (here, text classification) using novel* techniques\n- Why called **Universal** (the following have become synonymous with what a TL model is):\n - 1) It works across tasks varying in document size, number, and label type\n - 2) it uses a single architecture and training process; \n - 3) it requires no custom feature engineering or preprocessing; and \n - 4) it does not require additional in-domain documents or labels\n- What are the **novel** techniques: \n - discriminative fine-tuning,\n - slanted triangular learning rates, and \n - gradual unfreezing\n\n### The Fine-tuning Differences in Computer Vision vs NLP \n\n- Compared to CV models (which are several layers deep), **NLP models are typically more shallow** and thus require different fine-tuning methods\n- Features in deep neural networks in CV have been observed to transition **from general to task-specific** from the **first to the last layer**. \n- For this reason, most work in CV focuses on transferring the first layers of the model and fine-tuning the last or several of the last layers and leaving the remaining layers frozen\n\n> ULMFiT uses AWD-LSTM cell based Language Model\n\n### About AWD LSTM\n- Average SGD Weight Dropped (AWD) LSTM\n- It uses `DropConnect` and a variant of Average-SGD (`NT-ASGD`) along with several other well-known regularization strategies\n\n**Why `dropout` won't work?**\n - Dropout, an algorithm that randomly(with a probability p) ignore units’ activations during the training phase allows for the regularization of a neural network.\n - By diminishing the probability of neurons developing inter-dependencies, it increases the individual power of a neuron and thus reduces overfitting. \n - However, dropout inhibits the RNNs capability of developing long term dependencies as there is loss of information caused due to randomly ignoring units activations.\n\n**Hence `drop connect`**\n- the drop connect algorithm randomly drops weights instead of neuron activations. It does so by randomly(with probability 1-p) setting weights of the neural network to zero during the training phase. \n- Thus **redressing the issue of information loss** in the Recurrent Neural Network **while still performing regularization.**\n\n![](https://yashuseth.files.wordpress.com/2018/09/nn_do1.jpg?w=685)\nSource: Yashu Seth on AWD LSTM - https://yashuseth.blog/2018/09/12/awd-lstm-explanation-understanding-language-model/\n\nIf interested in understanding the architecture of ULMFit in-depth, checkout my notebook from 2019 [here](https://github.com/senthilkumarm1901/myNLPnotes/blob/master/General%20NLP%20Snippets%20and%20Notes/notes/LanguageModels_and_TransferLearning/Part2-AWD-LSTM.ipynb)\n\n### ELMo\n\n- ELMo comes up with better `word representations/embeddings` using Language Models that learn the `context` of the word in focus\n![](https://ahmedhanibrahim.files.wordpress.com/2019/07/52861-1pb5hxsxogjrnda_si4nj9q.png?w=775)\n*Ignore the hidden vectors predicting the padding tokens and only focus on the vectors that predict on the words*\nsource: https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27\n\nELMo uses the Bi-directional Language Model to get a new embedding that will be concatenated with the initialized word embedding. The word “are” in the above figure will have a representation formed with the following embedding vectors\n\n- Original embedding, GloVe, Word2Vec or FastText for example\n- Forward pass hidden layer representation vector\n- Backward pass hidden layer representation vector\n\n### About ELMo Word Vectors: \nELMo models both \n- (1) complex characteristics of word use (e.g., syntax and semantics)\n- (2) how these uses vary across linguistic contexts (i.e., to model polysemy)\n<br>\n<br>\n- ELMo `word vectors` are **learned functions of the internal states of a deep bidirectional language model (biLM)**, which is pretraind on a large text corpus\n<br>\n<br>\n- ELMo assigns each token/word **a representation that is function of the entire input sentence**\n<br>\n<br>\n- ELMo representations are **deep**, in the sense that they are **a function of all of the internal layers of the biLM**\n- In other words, ELMo doesn't just use the top LSTM layer, but all the internal layers\n<br>\n<br>\n- **higher-level LSTM states** capture **context-dependent aspects of word meaning**\n- **lower-level states** model aspects of **syntax**\n\nELMo does well in 6 diverse NLP tasks\n\n| Task | Description | Comments about Dataset | Evaluation Parameter | Previous SOTA | ELMo SOTA |\n| ------ |:------: |:------ |------ |------ | ------ | \n| SQuAD | Stanford Question Answering Dataset | a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable | F1 score (harmonic mean of precision and recall) | 84.4 |  85.8 |\n| SNLI | Stanford Natural Language Inference | SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE) | Accuracy | 88.6 | 88.7 |\n| SRL | Semantic Role Labeling | Semantic Role Labeling (SRL) recovers the latent predicate argument structure of a sentence, providing representations that answer basic questions about sentence meaning, including “who” did “what” to “whom,” etc | F1 Score | 81.7 | 84.6 |\n| Coref | Coreference resolution | Coreference resolution is the task of finding all expressions that refer to the same entity in a text. | Average F1 | 67.2 | 70.4 |\n| NER | Named Entity Recognition | The named entity recognition model identifies named entities (people, locations, organizations, and miscellaneous) in the input text | F1 | 91.93 | 92.22 |\n| SST-5 | 5-class Stanford Sentiment Treebank Dataset | fine-grained sentiment classification task uses 5 discrete classes: Strongly positive, Weakly positive, Neutral, Weakly negative, Strongly negative | Accuracy | 53.7 | 54.7 |\n<br>\n\n\n**sources for the Task Description:**\n- https://rajpurkar.github.io/SQuAD-explorer/\n- https://nlp.stanford.edu/projects/snli/\n- https://demo.allennlp.org/semantic-role-labeling/MTIzODQzNg==\n- https://demo.allennlp.org/coreference-resolution/MTIzODQzNA==\n- https://demo.allennlp.org/named-entity-recognition/MTIzODQzOA==\n- https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4\n\n**Pre-trained Bidirectional LM Architecture of ELMo**: \n\n![](./images/RNN/ELMo_pretrained_bidirectionalLM_architecture.PNG)\n\n**Advantages of ELMo:** \n- high-quality deep context-dependent representations are learned from biLMs\n- the biLM layers efficiently encode different types of syntactic and semantic information about words-in-context\n\n\n### Conclusion:\n- I hope this blog gives a good understanding of the pre-transformer era history of Transfer Learning architectures in NLP\n- I will cover more about BERT and Transformers in the upcoming articles\n","srcMarkdownNoYaml":"\n\n## Agenda\n\n- Why Language Modeling?\n- A short introduction to Language Modeling \n- How Transfer Learning Evolved\n- Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\n- The RNN-based Transfer Learning Architectures - ULMFiT & ELMo\n\n## Why Language Modeling?\n\n- The crux of Transfer Learning in 2 steps: <br> \n - (1) Build a Language Model* that understands the underlying features of the text \n - (2) Fine-tune the Language Model with additional layers for downstream tasks\n \n> Why Language Model for pre-training?<br>\n> *Language modeling can be seen as the ideal source task as it captures many facets of language relevant for downstream tasks, such as long-term dependencies, hierarchical relations and sentiment* (also being self-supervised) <br>\n> \n>Ruder et al in the ULMFiT paper\n_______________________________________________________________________________________________________________\n\n\n## Introduction to Language Modeling\n\nLanguage Model: **A model of the probability of a sequence of words**\n\n- A language model can assign probability to each possible next word. And also, help in assigning a probability to an entire sentence. \n\n### Applications of Language Model\n\n* Speech Recognition: E.g.: P('recognize speech') >> P('wreck a nice beach')\n* Spelling Correction: E.g.: P('I have a gub') << P('I have a gun')\n* Machine Translation: E.g.: P('strong winds') > P('large winds')\n* Optical Character Recognition/ Handwriting Recognition\n* Autoreply Suggestions\n* Text Classification (discussed with python implementation of a simple N-gram model)\n* Text Generation (discussed this with Char-level and Word-level language models)\n_______________________________________________________________________________________________________________\n\n### Evaluation Metrics for LM\n\n#### (1) Perplexity\n- A low perplexity indicates a better Language Model\n\n#### (2) Log Probability\n- Higher the log probability value of a LM in predicting a sample, higher is the confidence for that sample to occur in the distribution\n\nmy detailed notes on these 2 evaluation metrics of Language model is [here](https://senthilkumarm1901.quarto.pub/learn-by-blogging/posts/2020-03-17-introduction_to_statistical_language_modeling.html#evaluation)\n\n\n## How Transfer Learning Evolved\n\n- Stage1: NLP started with rule-based and statistical methodologies\n- Stage2: ML algos such as Naive Bayes, SVM, Trees coupled with bag-of-words word representations\n- Stage3: Recurrent Neural Networks such as LSTM\n- Stage4: RNN based Seq2Seq Transfer Learning Architectures (ULMFit, ELMo, etc.,)\n- Stage 5: Transformers --> 'ImageNet' moment in NLP\n\n![Transformer and Language Models](./images/RNN/TL_and_LM_flow_chart.png)\nSource: Evolution of TL in NLP https://arxiv.org/pdf/1910.07370v1.pdf\n\n## Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\n\nWhy RNNs came into existence?\n- Models such as the **Multi-layer Perceptron Network, vector machines and logistic regression** did not perform well on sequence modelling tasks (e.g.: text_sequence2sentiment_classification)\n- Why? **Lack of memory element** ; **No information retention**\n\n### Cometh the RNNs: \n\n- RNNs attempted to redress this shortcoming by introducing loops within the network, thus allowing the retention of information.\n\n**An unrolled RNN**\n![An un-rolled RNN Cell](./images/RNN/an_unrolled_RNN.png)\n\n#### Advantage of a vanilla RNN:\n- Better than traditional ML algos in retaining information\n\n#### Limitations of a vanilla RNN:\n- RNNs fail to model long term dependencies.\n - the information was often **\"forgotten\"** after the unit activations were multiplied several times by small numbers\n- Vanishing gradient and exploding gradient problems\n\n### Long Short Term Memory (LSTM): \n- a special type of RNN architecture\n- designed to keep information retained for extended number of timesteps\n- each LSTM cell consists of 4 layers (3 sigmoid functioins or gates and 1 tanh function)\n- The 3 sigmoid functions are called `forget`, `update` and `output` gates\n\n### Gated Recurrent Unit (GRU)\n\n- a curtailed version of LSTM\n- retains the resisting vanishing gradient properties of LSTM but GRUs are internally simpler and faster than LSTMs.\n> 1/ `forget` and `update` gates from LSTM are merged into a single `update` gate <br>\n> 2/ The update gate decides how much of previous memory to keep around. <br>\n> 3/ There is a `reset` gate which defines how to combine new input with previous value.\n\n\n\nIf interested in the math behind the RNN architectures, refer [this notebook I wrote in 2019](https://nbviewer.org/github/senthilkumarm1901/GeneralNLP/blob/master/General%20NLP%20Snippets%20and%20Notes/notes/LanguageModels_and_TransferLearning/Part1_Evolution_of_RNN_architectures_in_NLP.ipynb)\n\n### Comparison of performance between GRU and LSTM:\n- GRUs are almost on par with LSTM but with efficient computation. \n- However, with large data LSTMs with higher expressiveness may lead to better results\n\n## The RNN-based Transfer Learning Architectures - ULMFiT & ELMo\n\n### Some history and comparison with CV\n\nHistorically (before the Transformer era), \n- Fine-tuning a LM required millions of in-domain corpus (in other words, transfer learning was not possible)\n- LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier\n\nSource: \n- Evolution of TL in NLP: https://arxiv.org/pdf/1910.07370v1.pdf\n- ULMFiT paper: https://arxiv.org/pdf/1801.06146.pdf\n\n### ULMFit\n\n- Universal Language Model Fine-tuning (ULMFiT) for Text Classification\n - This paper introduces techniques that are essential to fine-tune an LSTM-based Language Model\n - This paper specifically the superior performance of ULMFiT approach  in 6 text classification datasets\n\n### What does ULMFiT propose?\n- Pretrain a LM on a large general-domain corpus and fine-tune it on the target task (here, text classification) using novel* techniques\n- Why called **Universal** (the following have become synonymous with what a TL model is):\n - 1) It works across tasks varying in document size, number, and label type\n - 2) it uses a single architecture and training process; \n - 3) it requires no custom feature engineering or preprocessing; and \n - 4) it does not require additional in-domain documents or labels\n- What are the **novel** techniques: \n - discriminative fine-tuning,\n - slanted triangular learning rates, and \n - gradual unfreezing\n\n### The Fine-tuning Differences in Computer Vision vs NLP \n\n- Compared to CV models (which are several layers deep), **NLP models are typically more shallow** and thus require different fine-tuning methods\n- Features in deep neural networks in CV have been observed to transition **from general to task-specific** from the **first to the last layer**. \n- For this reason, most work in CV focuses on transferring the first layers of the model and fine-tuning the last or several of the last layers and leaving the remaining layers frozen\n\n> ULMFiT uses AWD-LSTM cell based Language Model\n\n### About AWD LSTM\n- Average SGD Weight Dropped (AWD) LSTM\n- It uses `DropConnect` and a variant of Average-SGD (`NT-ASGD`) along with several other well-known regularization strategies\n\n**Why `dropout` won't work?**\n - Dropout, an algorithm that randomly(with a probability p) ignore units’ activations during the training phase allows for the regularization of a neural network.\n - By diminishing the probability of neurons developing inter-dependencies, it increases the individual power of a neuron and thus reduces overfitting. \n - However, dropout inhibits the RNNs capability of developing long term dependencies as there is loss of information caused due to randomly ignoring units activations.\n\n**Hence `drop connect`**\n- the drop connect algorithm randomly drops weights instead of neuron activations. It does so by randomly(with probability 1-p) setting weights of the neural network to zero during the training phase. \n- Thus **redressing the issue of information loss** in the Recurrent Neural Network **while still performing regularization.**\n\n![](https://yashuseth.files.wordpress.com/2018/09/nn_do1.jpg?w=685)\nSource: Yashu Seth on AWD LSTM - https://yashuseth.blog/2018/09/12/awd-lstm-explanation-understanding-language-model/\n\nIf interested in understanding the architecture of ULMFit in-depth, checkout my notebook from 2019 [here](https://github.com/senthilkumarm1901/myNLPnotes/blob/master/General%20NLP%20Snippets%20and%20Notes/notes/LanguageModels_and_TransferLearning/Part2-AWD-LSTM.ipynb)\n\n### ELMo\n\n- ELMo comes up with better `word representations/embeddings` using Language Models that learn the `context` of the word in focus\n![](https://ahmedhanibrahim.files.wordpress.com/2019/07/52861-1pb5hxsxogjrnda_si4nj9q.png?w=775)\n*Ignore the hidden vectors predicting the padding tokens and only focus on the vectors that predict on the words*\nsource: https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27\n\nELMo uses the Bi-directional Language Model to get a new embedding that will be concatenated with the initialized word embedding. The word “are” in the above figure will have a representation formed with the following embedding vectors\n\n- Original embedding, GloVe, Word2Vec or FastText for example\n- Forward pass hidden layer representation vector\n- Backward pass hidden layer representation vector\n\n### About ELMo Word Vectors: \nELMo models both \n- (1) complex characteristics of word use (e.g., syntax and semantics)\n- (2) how these uses vary across linguistic contexts (i.e., to model polysemy)\n<br>\n<br>\n- ELMo `word vectors` are **learned functions of the internal states of a deep bidirectional language model (biLM)**, which is pretraind on a large text corpus\n<br>\n<br>\n- ELMo assigns each token/word **a representation that is function of the entire input sentence**\n<br>\n<br>\n- ELMo representations are **deep**, in the sense that they are **a function of all of the internal layers of the biLM**\n- In other words, ELMo doesn't just use the top LSTM layer, but all the internal layers\n<br>\n<br>\n- **higher-level LSTM states** capture **context-dependent aspects of word meaning**\n- **lower-level states** model aspects of **syntax**\n\nELMo does well in 6 diverse NLP tasks\n\n| Task | Description | Comments about Dataset | Evaluation Parameter | Previous SOTA | ELMo SOTA |\n| ------ |:------: |:------ |------ |------ | ------ | \n| SQuAD | Stanford Question Answering Dataset | a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable | F1 score (harmonic mean of precision and recall) | 84.4 |  85.8 |\n| SNLI | Stanford Natural Language Inference | SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE) | Accuracy | 88.6 | 88.7 |\n| SRL | Semantic Role Labeling | Semantic Role Labeling (SRL) recovers the latent predicate argument structure of a sentence, providing representations that answer basic questions about sentence meaning, including “who” did “what” to “whom,” etc | F1 Score | 81.7 | 84.6 |\n| Coref | Coreference resolution | Coreference resolution is the task of finding all expressions that refer to the same entity in a text. | Average F1 | 67.2 | 70.4 |\n| NER | Named Entity Recognition | The named entity recognition model identifies named entities (people, locations, organizations, and miscellaneous) in the input text | F1 | 91.93 | 92.22 |\n| SST-5 | 5-class Stanford Sentiment Treebank Dataset | fine-grained sentiment classification task uses 5 discrete classes: Strongly positive, Weakly positive, Neutral, Weakly negative, Strongly negative | Accuracy | 53.7 | 54.7 |\n<br>\n\n\n**sources for the Task Description:**\n- https://rajpurkar.github.io/SQuAD-explorer/\n- https://nlp.stanford.edu/projects/snli/\n- https://demo.allennlp.org/semantic-role-labeling/MTIzODQzNg==\n- https://demo.allennlp.org/coreference-resolution/MTIzODQzNA==\n- https://demo.allennlp.org/named-entity-recognition/MTIzODQzOA==\n- https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4\n\n**Pre-trained Bidirectional LM Architecture of ELMo**: \n\n![](./images/RNN/ELMo_pretrained_bidirectionalLM_architecture.PNG)\n\n**Advantages of ELMo:** \n- high-quality deep context-dependent representations are learned from biLMs\n- the biLM layers efficiently encode different types of syntactic and semantic information about words-in-context\n\n\n### Conclusion:\n- I hope this blog gives a good understanding of the pre-transformer era history of Transfer Learning architectures in NLP\n- I will cover more about BERT and Transformers in the upcoming articles\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2020-07-17-evolution_of_RNN_architectures.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP","DL","ML"],"date":"2020-07-17","description":"Highlighting the non-mathematical essentials in the evolution of RNN architectures in Transfer Learning (before Transformer-based models came to the fore)","title":"Evolution of RNN Architectures for Transfer Learning","image":"images/RNN/an_unrolled_RNN.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}