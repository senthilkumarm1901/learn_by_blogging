{"title":"Building Frugal Open Source LLM Applications using Serverless Cloud","markdown":{"yaml":{"author":"Senthil Kumar","badges":true,"branch":"master","categories":["LLM","Serverless","AWS","NLP"],"date":"2024-05-16","description":"This blog covers 4 Serverless LLM Recipes useful from the point of view of learning and building PoCs","output-file":"2024-05-16-serverless-llm.html","title":"Building Frugal Open Source LLM Applications using Serverless Cloud","toc":true,"image":"images/serverless_llm/Serverless_LLM.png"},"headingText":"Motivation","containsRefs":false,"markdown":"\n\n\n- Want to build LLM applications? \n- Wondering what is the most cost effective way to learn and build them in cloud?\n\n> Think OpenSource LLM. <br>\n> Think Serverless\n\n<hr>\n\n# Purpose of this Presentation\n\nLet us see how the intermingling of 2 concepts - Serverless + Open Source LLMs - help you build demo-able PoC LLM applications, at minimal cost. \n\n\n```\n#LLMOps\n#MLOps\n#AWSLambda\n#LLMonServerless\n#OpenSourceLLMs\n```\n\n<hr>\n\n# LLM Recipes we are discussing today\n\n- 1) A Lambda to run inference on a purpose-built Transformer ML Model\n     - A Lambda to **Anonymize Text** using a Huggingface BERT Transformer-based Language Model for PII De-identification \n- 2) A Lambda to run a **Small Language Model** like Microsoft's Phi3\n- 3) A Lambda to run a **RAG** Implementation on a Small Language Model like Phi3 \n- 4) A Lambda to invoke **a LLM like Mistral 7B Instruct** that is running in a SageMaker Endpoint\n\n<hr>\n\n## 1. Lambda to Anonymize Text\n\n\n- A Lambda to run inference on a purpose-built ML Model\n     - This lambda can **Anonymize Text** \n     - using a Huggingface BERT Transformer-based Fine-tuned Model\n\n### 1.A. Architecture\n\n![](./images/serverless_llm/container_lambda_with_api_gateway.png)\n\n### 1.B. How to invoke the API Gateway Endpoint \n\n![](./images/serverless_llm/anonymize_output.png)\n\n### 1.C. How the output looks in a Streamlit App\n\n![](./images/serverless_llm/anonymize_text.png)\n\n### 1.D. AWS CLI commands to create the Architecture\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/</a></h5>\n\n\n<hr>\n\n## 2. Small Language Model\n\n- A Lambda to run a **Small Language Model** like Microsoft's Phi3\n\n### 2.A. Architecture\n\n![](./images/serverless_llm/container_lambda_with_api_gateway_diag2.png)\n\n### 2.B. How to invoke the API Gateway Endpoint\n\n![](./images/serverless_llm/slm_output.png)\n\n### 2.C. How the output looks in a Streamlit App\n\n![](./images/serverless_llm/phi3_mini_llm_text_cls.png)\n\n### 2.D. AWS CLI commands to create the Architecture\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/</a></h5>\n\n\n<hr>\n\n## 3. Small Language Model with RAG\n\n- A Lambda to run a RAG Implementation on a Small Language Model like Phi3, that gives better context\n\n### 3.A. A Brief Overview on RAG\n\n**What is RAG**, **How does RAG improve LLM Accuracy**?\n\n> Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data. \n\nSource: [Databricks](https://www.databricks.com/glossary/retrieval-augmented-generation-rag)\n\n**How does LLM work?**\n\n<img src=\"https://images.ctfassets.net/xjan103pcp94/3TBU5BOctjuaPyxuA8PGul/1c1b0b0129be5fef9eaef73063491582/image1.png\" width=\"500\" />\n\nSource: [AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)\n\n**How does RAG in LLM work?**\n\n<img src=\"https://files.realpython.com/media/Screenshot_2023-10-28_at_2.05.18_PM.92b839a5972b.png\" width=\"700\" />\n\n\nSource: [RealPython Blog: chromadb-vector-database](https://realpython.com/chromadb-vector-database/)\n\n**How is a Vector DB created**\n\n<img src=\"./images/serverless_llm/how_is_vector_db_created.png\" width=\"700\" />\n\nSource: [AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)\n\n**Detour: If you wish to use other Vector databases**\n    \n<img src=\"https://thedataquarry.com/posts/vector-db-1/vector-db-source-available.png\" width=\"700\" />\n\n\nSource: [Data Quarry Blog: Vector databases - What makes each one different?](https://thedataquarry.com/posts/vector-db-1/)\n\n### 3.B. Architecture\n\n![](./images/serverless_llm/slm_with_rag_3.png)\n\n- URL we are testing on is from my favorite DL/NLP Researcher. \n    - https://magazine.sebastianraschka.com/p/understanding-large-language-models\n    \n<img src=\"./images/serverless_llm/article_we_are_using_as_context.png\" width=\"500\" />\n\n### 3.C. How to invoke the API Gateway Endpoint\n\n![](./images/serverless_llm/rag_slm_output.png)\n\n### 3.D. How the output looks in a Streamlit App\n\n![](./images/serverless_llm/phi3_llm_rag_lora.png)\n\n### 3.E. AWS CLI commands to create the Architecture\n\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/</a></h5>\n\n\n<hr>\n\n## 4. Large Language Model  (A Partial Serverless)\n\n- A Lambda to invoke **a LLM like Mistral 7B Instruct** that is running in  SageMaker Endpoint\n\n### 4.A. Architecture\n\n![](./images/serverless_llm/lambda_to_invoke_sagemaker_endpoint.png)\n\n### 4.B. How to invoke the API Gateway Endpoint\n\n![](./images/serverless_llm/sage_maker_output.png)\n\n### 4.C. AWS CLI commands to create the Architecture\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/</a></h5>\n\n\n<hr>\n\n## Key Challenges Faced\n\n- Serverless could mean we end up with low end cpu architecture. Hence, latency high for RAG LLM implementations\n- RAG could mean any big context. But converting the RAG context into a vector store will take time. Hence size of the context needs to be lower for \"AWS Lambda\" implementations\n- Maximum timelimit in Lambda is 15 min. API Gateway times out in 30 seconds. Hence could not be used in RAG LLM implementation\n\n## Key Learnings\n\n**MLOps Concepts**:\n\n- Dockerizing ML Applications. What works in your machine works everywhere. More than 70% of the time building these LLM Apps is in perfecting the dockerfile. \n- The art of storing ML Models in AWS Lambda Containers. Use `cache_dir` well. Otherwise, models get downloaded everytime docker container is created\n\n\n```python\nos.environ['HF_HOME'] = '/tmp/model' #the only `write-able` dir in AWS lambda = `/tmp`\n...\n...\nyour_model=\"ab-ai/pii_model\"\ntokenizer = AutoTokenizer.from_pretrained(your_model,cache_dir='/tmp/model')\nner_model = AutoModelForTokenClassification.from_pretrained(your_model,cache_dir='/tmp/model')\n```\n\n\n**AWS Concepts**:\n\n- `aws cli` is your friend for shorterning deployment time, especially for Serverless\n- API Gateway is a frustratingly beautiful service. But a combination of `aws cli` and `OpenAPI` spec makes it replicable\n- AWS Lambda Costing is cheap for PoCs\n\nFinally, the **LLM Concepts**:\n\n- Frameworks: Llama cpp, LangChain, LlamaIndex, Huggingface (and so many more!)\n- SLMs work well with Reasoning but are too slow/bad for general knowledge questions\n\n> Models are like wines and these LLM frameworks are like bottles. The important thing is the wine more than the bottle. But getting used to how the wines are stored in the bottles help.  \n\n## Next Steps for the Author\n\n- Codes discussed in recipes may not be fully efficient! We can further reduce cost if run time is reduced\n\n**For Phi3-Mini-RAG**: \n\n- Try leveraging a better embedding model (apart from the ancient `Sentence Transformers`)\n- What about other vector databases? - Like Pinecone Milvus (we have used opensource Chromodb) here\n- Ideas to explore: Rust for LLMs. Rust for Lambda. \n\nSources: \n- Rust ML Minimalist framework - Candle: https://github.com/huggingface/candle\n- Rust for LLM - https://github.com/rustformers/llm\n- Rust for AWS Lambda - https://www.youtube.com/watch?v=He4inXmMZZI\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://github.com/senthilkumarm1901/serverless_nlp_app\">github.com/senthilkumarm1901/serverless_nlp_app</a></h5>\n\n<h5 align=\"center\"> Thank You </h5>\n","srcMarkdownNoYaml":"\n\n# Motivation\n\n- Want to build LLM applications? \n- Wondering what is the most cost effective way to learn and build them in cloud?\n\n> Think OpenSource LLM. <br>\n> Think Serverless\n\n<hr>\n\n# Purpose of this Presentation\n\nLet us see how the intermingling of 2 concepts - Serverless + Open Source LLMs - help you build demo-able PoC LLM applications, at minimal cost. \n\n\n```\n#LLMOps\n#MLOps\n#AWSLambda\n#LLMonServerless\n#OpenSourceLLMs\n```\n\n<hr>\n\n# LLM Recipes we are discussing today\n\n- 1) A Lambda to run inference on a purpose-built Transformer ML Model\n     - A Lambda to **Anonymize Text** using a Huggingface BERT Transformer-based Language Model for PII De-identification \n- 2) A Lambda to run a **Small Language Model** like Microsoft's Phi3\n- 3) A Lambda to run a **RAG** Implementation on a Small Language Model like Phi3 \n- 4) A Lambda to invoke **a LLM like Mistral 7B Instruct** that is running in a SageMaker Endpoint\n\n<hr>\n\n## 1. Lambda to Anonymize Text\n\n\n- A Lambda to run inference on a purpose-built ML Model\n     - This lambda can **Anonymize Text** \n     - using a Huggingface BERT Transformer-based Fine-tuned Model\n\n### 1.A. Architecture\n\n![](./images/serverless_llm/container_lambda_with_api_gateway.png)\n\n### 1.B. How to invoke the API Gateway Endpoint \n\n![](./images/serverless_llm/anonymize_output.png)\n\n### 1.C. How the output looks in a Streamlit App\n\n![](./images/serverless_llm/anonymize_text.png)\n\n### 1.D. AWS CLI commands to create the Architecture\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/</a></h5>\n\n\n<hr>\n\n## 2. Small Language Model\n\n- A Lambda to run a **Small Language Model** like Microsoft's Phi3\n\n### 2.A. Architecture\n\n![](./images/serverless_llm/container_lambda_with_api_gateway_diag2.png)\n\n### 2.B. How to invoke the API Gateway Endpoint\n\n![](./images/serverless_llm/slm_output.png)\n\n### 2.C. How the output looks in a Streamlit App\n\n![](./images/serverless_llm/phi3_mini_llm_text_cls.png)\n\n### 2.D. AWS CLI commands to create the Architecture\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/</a></h5>\n\n\n<hr>\n\n## 3. Small Language Model with RAG\n\n- A Lambda to run a RAG Implementation on a Small Language Model like Phi3, that gives better context\n\n### 3.A. A Brief Overview on RAG\n\n**What is RAG**, **How does RAG improve LLM Accuracy**?\n\n> Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data. \n\nSource: [Databricks](https://www.databricks.com/glossary/retrieval-augmented-generation-rag)\n\n**How does LLM work?**\n\n<img src=\"https://images.ctfassets.net/xjan103pcp94/3TBU5BOctjuaPyxuA8PGul/1c1b0b0129be5fef9eaef73063491582/image1.png\" width=\"500\" />\n\nSource: [AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)\n\n**How does RAG in LLM work?**\n\n<img src=\"https://files.realpython.com/media/Screenshot_2023-10-28_at_2.05.18_PM.92b839a5972b.png\" width=\"700\" />\n\n\nSource: [RealPython Blog: chromadb-vector-database](https://realpython.com/chromadb-vector-database/)\n\n**How is a Vector DB created**\n\n<img src=\"./images/serverless_llm/how_is_vector_db_created.png\" width=\"700\" />\n\nSource: [AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)\n\n**Detour: If you wish to use other Vector databases**\n    \n<img src=\"https://thedataquarry.com/posts/vector-db-1/vector-db-source-available.png\" width=\"700\" />\n\n\nSource: [Data Quarry Blog: Vector databases - What makes each one different?](https://thedataquarry.com/posts/vector-db-1/)\n\n### 3.B. Architecture\n\n![](./images/serverless_llm/slm_with_rag_3.png)\n\n- URL we are testing on is from my favorite DL/NLP Researcher. \n    - https://magazine.sebastianraschka.com/p/understanding-large-language-models\n    \n<img src=\"./images/serverless_llm/article_we_are_using_as_context.png\" width=\"500\" />\n\n### 3.C. How to invoke the API Gateway Endpoint\n\n![](./images/serverless_llm/rag_slm_output.png)\n\n### 3.D. How the output looks in a Streamlit App\n\n![](./images/serverless_llm/phi3_llm_rag_lora.png)\n\n### 3.E. AWS CLI commands to create the Architecture\n\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/</a></h5>\n\n\n<hr>\n\n## 4. Large Language Model  (A Partial Serverless)\n\n- A Lambda to invoke **a LLM like Mistral 7B Instruct** that is running in  SageMaker Endpoint\n\n### 4.A. Architecture\n\n![](./images/serverless_llm/lambda_to_invoke_sagemaker_endpoint.png)\n\n### 4.B. How to invoke the API Gateway Endpoint\n\n![](./images/serverless_llm/sage_maker_output.png)\n\n### 4.C. AWS CLI commands to create the Architecture\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/</a></h5>\n\n\n<hr>\n\n## Key Challenges Faced\n\n- Serverless could mean we end up with low end cpu architecture. Hence, latency high for RAG LLM implementations\n- RAG could mean any big context. But converting the RAG context into a vector store will take time. Hence size of the context needs to be lower for \"AWS Lambda\" implementations\n- Maximum timelimit in Lambda is 15 min. API Gateway times out in 30 seconds. Hence could not be used in RAG LLM implementation\n\n## Key Learnings\n\n**MLOps Concepts**:\n\n- Dockerizing ML Applications. What works in your machine works everywhere. More than 70% of the time building these LLM Apps is in perfecting the dockerfile. \n- The art of storing ML Models in AWS Lambda Containers. Use `cache_dir` well. Otherwise, models get downloaded everytime docker container is created\n\n\n```python\nos.environ['HF_HOME'] = '/tmp/model' #the only `write-able` dir in AWS lambda = `/tmp`\n...\n...\nyour_model=\"ab-ai/pii_model\"\ntokenizer = AutoTokenizer.from_pretrained(your_model,cache_dir='/tmp/model')\nner_model = AutoModelForTokenClassification.from_pretrained(your_model,cache_dir='/tmp/model')\n```\n\n\n**AWS Concepts**:\n\n- `aws cli` is your friend for shorterning deployment time, especially for Serverless\n- API Gateway is a frustratingly beautiful service. But a combination of `aws cli` and `OpenAPI` spec makes it replicable\n- AWS Lambda Costing is cheap for PoCs\n\nFinally, the **LLM Concepts**:\n\n- Frameworks: Llama cpp, LangChain, LlamaIndex, Huggingface (and so many more!)\n- SLMs work well with Reasoning but are too slow/bad for general knowledge questions\n\n> Models are like wines and these LLM frameworks are like bottles. The important thing is the wine more than the bottle. But getting used to how the wines are stored in the bottles help.  \n\n## Next Steps for the Author\n\n- Codes discussed in recipes may not be fully efficient! We can further reduce cost if run time is reduced\n\n**For Phi3-Mini-RAG**: \n\n- Try leveraging a better embedding model (apart from the ancient `Sentence Transformers`)\n- What about other vector databases? - Like Pinecone Milvus (we have used opensource Chromodb) here\n- Ideas to explore: Rust for LLMs. Rust for Lambda. \n\nSources: \n- Rust ML Minimalist framework - Candle: https://github.com/huggingface/candle\n- Rust for LLM - https://github.com/rustformers/llm\n- Rust for AWS Lambda - https://www.youtube.com/watch?v=He4inXmMZZI\n\n<p align=\"center\">\n  <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n</p>\n\n\n<h5 align=\"center\"><a href=\"https://github.com/senthilkumarm1901/serverless_nlp_app\">github.com/senthilkumarm1901/serverless_nlp_app</a></h5>\n\n<h5 align=\"center\"> Thank You </h5>\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2024-05-16-serverless-llm.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"author":"Senthil Kumar","badges":true,"branch":"master","categories":["LLM","Serverless","AWS","NLP"],"date":"2024-05-16","description":"This blog covers 4 Serverless LLM Recipes useful from the point of view of learning and building PoCs","title":"Building Frugal Open Source LLM Applications using Serverless Cloud","image":"images/serverless_llm/Serverless_LLM.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}