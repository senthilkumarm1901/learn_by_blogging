{"title":"Introduction to Statistical Language Modeling","markdown":{"yaml":{"aliases":["/Markov_Langugage_Model/Log_Probability/Perplexity/2020/03/17/Introduction_to_Statistical_Language_Modeling"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Statistics"],"date":"2020-03-17","description":"A gentle introduction to understand the ABCs of NLP in the era of Transformer LMs generating poems.","hide":false,"image":"images/language_model_intro/introduction_to_statistical_language_model.png","output-file":"2020-03-17-introduction_to_statistical_language_modeling.html","title":"Introduction to Statistical Language Modeling","toc":true},"headingText":"1. What is a Language Model?","containsRefs":false,"markdown":"\n\n\n\n- A model that generates a probability distribution over sequences of words\n- In simpler words, LM is a model to predict the next word in a sequence of words\n\n## 2. Applications of LMs\n\n- Predicting upcoming words or estimating probability of a phrase or sentence is useful in noisy, ambiguous text data sources \n    - Speech Recognition: E.g.: P('recognize speech') >> P('wreck a nice beach')\n    - Spelling Correction: E.g.: P('I have a gub') << P('I have a gun')\n    - Machine Translation: E.g.: P('strong winds') > P('large winds')\n    - Optical Character Recognition/ Handwriting Recognition\n    - Autoreply Suggestions\n    - Text Classification (discussed with python implementation of a simple N-gram model)\n    - Text Generation (discussed this with Char-level and Word-level language models)\n\n<hr>\n\n## 3. N-gram Language Modelling\n\n\n**Sample Corpus**: <br>\n> This is **the house** that Jack built.<br>\n> This is **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** cat,<br>\n> That killed **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n\nSource: \"*The house that jack built*\" Nursery Rhyme\n\nWhat is the probability of **house** occurring given **the** before it?\n\nIntuitively, we count,\n\n$$ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } $$\n\nMathematically, the above formula can be writen as $$ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } $$\n\nWhat we have computed above is a **Bigram Language Model**:\n\n$$ Bigram\\:Model : { p\\,( W_t|W_{t-1} ) } $$\n\n**How do ngrams help us calculate the prob of a sentence?**:\n> Sentence, S = 'A B'<br>\n> We know that, probability of this sentence 'A B' as,<br>\n> P (S) = Prob (A before B) = P(A , B) = Joint Probability of A and B = **P(B|A)* P(A)**<br>\n>\n> Let us assume a three word sentence, S = 'A B C'<br>\n> P(S) = Prob (A before B before C ) = P (A , B , C)<br>\n>      = P(C | A , B) * P (A n B)<br>\n>      = P (C| A , B) * P(B | A) * P (A)<br>\n\nEven if there are more words, we could keep applying Conditional Probability and compute the prob of a sentence.\nThe above rule is called **`Chain Rule of Probability`**\n$$ Prob \\ ({ A_n,\\ A_{n-1},\\ ....,A_1}) = Prob\\ ( {A_n \\ |\\ {A_{n-1},\\ ....,A_1}} ) \\  \\times \\  Prob\\ (A_{n-1},\\ ....,A_1) $$\n\nWith four vairables the chain rule of probability is: \n$$ Prob \\ ({ A_4,\\ A_3,\\ A_2,\\ A_1}) = Prob\\ ( {A_4 \\ |\\ {A_{3},\\ A_2,\\ A_1}} ) \\  \\times \\  Prob\\ ( {A_3 \\ |\\ {A_{2},\\ A_1}})\\ \\times \\ Prob\\ ({A_2 \\ |\\ A_1})\\ \\times \\ Prob\\ (A_1) $$\n\n<hr>\n\n## 4. Out of Vocabulary\n\nSuppose we have a new sentence like the below one:<br>\n<br>\n> This is **the dog**,<br>\n> That scared the cat,<br>\n> That killed the rat,<br>\n> That ate the malt,<br>\n> That lay in the house that Jack built.<br>\n\nIf we calculate the Probability of the above sentence, based on the language model trained on the previous **sample corpus (*The house that Jack built* Nursery Rhyme)**, it would be zero (because \"the dog\" has not occurred in the LM training corpus.\n\nBy our **chain rule of probabilities** where we keep multiplying probabilities,\nwe would encounter $ P({dog \\over the}) = 0 $ , hence the overall probability of the above example sentence would be zero\n\n<hr>\n\n## One way to avoid, Smoothing!\n\n$$ MLE: P({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ ) \\over Count\\ (\\ B\\ )} $$\n\n**Why is it called MLE?**\n* Suppose a bigram “the dog” occurs 5 times in 100 documents.\n* Given that we have this 100 document corpus (which the language model represents), the maximum likelihood of the bigram parameter “the dog” appearing in the text is 0.05\n\n**Add 1 Smoothing (Laplace Smoothing)**\n$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ 1 \\over Count\\ (\\ B\\ )\\ +\\ |V|} $$\n* We pretend that each unique bigram occurs once more than it actually did! \n* Since we have added 1 to each bigram, we have added |V| bigrams in total. Hence normalizing by adding |V| to the denominator\n* Disadvantage: It reduces the probability of high frequency words\n\n\n**Add- $ \\delta $ Smoothing!**\n$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ \\delta \\over Count\\ (\\ B\\ )\\ +\\ \\delta * |V|} $$\n* $\\delta$ is any fraction such as 0.1, 0.05, etc.,\n* Unlike Add one smoothing, this solves the zero prob issue and avoids reducing the prob of high frequency words\n\n<hr>\n\n## 5. What is Markov Assumption?  \n\nSo the probability of occurrence of a 4-word sentence, by markov assumption is: \n$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B,\\ C,\\ D}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C,\\ D}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n\n\n\n> “What I see NOW depends only on what I saw in the PREVIOUS step”\n\n$$ P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1}) = P(w_i,\\ |\\ {w_{i-1}}) $$\n\nHence the probability of occurrence of a 4-word sentence with Markov Assumption is: \n$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n\n- What are its advantages?\n - Probability of an entire sentence could be very low but individual phrases could be more probable. <br>\n\tFor e.g.: <br>\n\t- Actual Data: “The quick fox jumps over the lazy dog”  \n\t- Probability of a new sentence: Prob(“The quick fox jumps over the lazy cat”) = 0 (though probable to occur,'cat' is not there in vocab, hence zero)\n\t- In Markov assumption (with additive smoothing), the above sentence will have a realistic probability\n    \n<hr>\n\n## 6. Evaluation \n\n### Perplexity\n\n- Perplexity is a measurement of how well a probability model predicts a sample. \n- It is used to compare probability models. \n- A low perplexity indicates the probability distribution is good at predicting the sample       \n\nDefinition: \n- Perplexity is the inverse probability of  the test set, normalized by the number  of words. <br>\nPerplexity of test data = PP(test data) =\n$$ P(w_1,\\ w_2,\\ ....,\\ w_N)^{1 \\over N}  $$\n$$ $$ \n$$ {\\sqrt[N]{1 \\over P(w_1,\\ w_2,\\ ....,\\ w_N)}} $$\n$$ $$\n$$ {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1})}}}$$ \n\n\n**the lower the perplexity of a LM for predicting a sample, higher is the confidence for that sample to occur in the distribution**\n\n<hr>\n\n### Log Probability\n- But multiplying probabilities like these could end up giving you the result closer to zero. \n - For e.g.: P(D|C) = 0.1, P(C|B) = 0.07, P(B|A) = 0.05, P(A) = 0.2\n - P(A before B before C before D before E) = 0.00007 $-->$ too low or almost zero !\n\n- Logarithm to the rescue ! \n - Logarithm is a monotonically increasing function ! \n - Meaning: If P(E|D) > P(D|C), then log(P(E|D)) > log (P(D|C))\n - Also, log (A *B) = log (A) + log (B) \n\n![LogarithmChart](https://upload.wikimedia.org/wikipedia/commons/1/17/Binary_logarithm_plot_with_ticks.svg)\n\n$$ \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) $$ \n- where N  is the number of words in the sentence. \n- Since log (probabilities) are always negative (see graph: log of values less than 1 is negative), shorter sentences will have higher probability of occurrence. \n\n**To normalize for length of sentences**, \n$$ {1 \\over N} \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n\n**Higher the log probability value of a LM in predicting a sample, higher is the confidence for that sample to occur in the distribution**\n\n<hr>\n\n**Perplexity and Log-Probability metrics for a Bigram Markov Language Model**\n\n$$ Perplexity\\ for\\ a\\ bigram\\ model = {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1}})}}}$$ \n\nComparing with normalized log probability for a bigram model:  \n$$ LogProbability\\ for\\ a\\ bigram\\ model = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n\n<hr>\n\n## 7. Application\n### Text Classification\n\nIn this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Bigram_Models--Text_Classification.ipynb), Markov Bigram model is implemented on a text classification problem. We can build a deterministic classifier where class is attributed to the class with highest log probability score.\n\n```python\nprint(bigram_markov(\"economic growth slows down\"))\n```\n\n```\n# log probability score of different classes\n[('business', -3.9692388514802905),\n ('politics', -6.7677569438805945),\n ('tech', -8.341093396600021),\n ('sport', -9.000286606679415),\n ('entertainment', -9.09610496174359)]\n\nClass attributed: 'business'\n```\n\n\n\n```python\nprint(bigram_markov(\"prime minister has gone to the US on an official trip\"))\n```\n\n```\n# log probability score of different classes\n[('politics', -3.5746871267381852),\n ('business', -7.440089097987748),\n ('tech', -10.252850263607193),\n ('sport', -11.981405120960808),\n ('entertainment', -12.124273481509913)]\n \n Class attributed: 'politics'\n```\n\n### Text Generation\n\nIn this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Char-level%20N-gram%20Language%20Generation%20Model.ipynb), a char-level LM, using trigrams and bigrams of characters, was used to generate dianosour names.\n\n```python\n>> print(names.iloc[:,0].head())\n\n# Names of Dianosours used as input\n0     Aachenosaurus\n1          Aardonyx\n2    Abdallahsaurus\n3       Abelisaurus\n4     Abrictosaurus\n```\n\n\n```python\n>> char_level_trigram_markov_model(names)\n\n# Names of Dianosours generated as output\nbroplisaurus\nprorkhorpsaurus\njintagasaurus\ncarsaurus\ngapophongasaurus\nteglangsaurus\nborudomachsaurus\nzheisaurus\nhorachillisaurus\naveiancsaurus\n```\n\n\nReferences:\n\n- Wikipedia\n- [Udemy Course on Deep NLP by Lazy Programmer](https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/) <br>\n- [NLP Course on Coursera by National Research University Higher School of Economics](https://www.coursera.org/learn/language-processing)\n","srcMarkdownNoYaml":"\n\n\n\n## 1. What is a Language Model?\n- A model that generates a probability distribution over sequences of words\n- In simpler words, LM is a model to predict the next word in a sequence of words\n\n## 2. Applications of LMs\n\n- Predicting upcoming words or estimating probability of a phrase or sentence is useful in noisy, ambiguous text data sources \n    - Speech Recognition: E.g.: P('recognize speech') >> P('wreck a nice beach')\n    - Spelling Correction: E.g.: P('I have a gub') << P('I have a gun')\n    - Machine Translation: E.g.: P('strong winds') > P('large winds')\n    - Optical Character Recognition/ Handwriting Recognition\n    - Autoreply Suggestions\n    - Text Classification (discussed with python implementation of a simple N-gram model)\n    - Text Generation (discussed this with Char-level and Word-level language models)\n\n<hr>\n\n## 3. N-gram Language Modelling\n\n\n**Sample Corpus**: <br>\n> This is **the house** that Jack built.<br>\n> This is **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** cat,<br>\n> That killed **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n\nSource: \"*The house that jack built*\" Nursery Rhyme\n\nWhat is the probability of **house** occurring given **the** before it?\n\nIntuitively, we count,\n\n$$ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } $$\n\nMathematically, the above formula can be writen as $$ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } $$\n\nWhat we have computed above is a **Bigram Language Model**:\n\n$$ Bigram\\:Model : { p\\,( W_t|W_{t-1} ) } $$\n\n**How do ngrams help us calculate the prob of a sentence?**:\n> Sentence, S = 'A B'<br>\n> We know that, probability of this sentence 'A B' as,<br>\n> P (S) = Prob (A before B) = P(A , B) = Joint Probability of A and B = **P(B|A)* P(A)**<br>\n>\n> Let us assume a three word sentence, S = 'A B C'<br>\n> P(S) = Prob (A before B before C ) = P (A , B , C)<br>\n>      = P(C | A , B) * P (A n B)<br>\n>      = P (C| A , B) * P(B | A) * P (A)<br>\n\nEven if there are more words, we could keep applying Conditional Probability and compute the prob of a sentence.\nThe above rule is called **`Chain Rule of Probability`**\n$$ Prob \\ ({ A_n,\\ A_{n-1},\\ ....,A_1}) = Prob\\ ( {A_n \\ |\\ {A_{n-1},\\ ....,A_1}} ) \\  \\times \\  Prob\\ (A_{n-1},\\ ....,A_1) $$\n\nWith four vairables the chain rule of probability is: \n$$ Prob \\ ({ A_4,\\ A_3,\\ A_2,\\ A_1}) = Prob\\ ( {A_4 \\ |\\ {A_{3},\\ A_2,\\ A_1}} ) \\  \\times \\  Prob\\ ( {A_3 \\ |\\ {A_{2},\\ A_1}})\\ \\times \\ Prob\\ ({A_2 \\ |\\ A_1})\\ \\times \\ Prob\\ (A_1) $$\n\n<hr>\n\n## 4. Out of Vocabulary\n\nSuppose we have a new sentence like the below one:<br>\n<br>\n> This is **the dog**,<br>\n> That scared the cat,<br>\n> That killed the rat,<br>\n> That ate the malt,<br>\n> That lay in the house that Jack built.<br>\n\nIf we calculate the Probability of the above sentence, based on the language model trained on the previous **sample corpus (*The house that Jack built* Nursery Rhyme)**, it would be zero (because \"the dog\" has not occurred in the LM training corpus.\n\nBy our **chain rule of probabilities** where we keep multiplying probabilities,\nwe would encounter $ P({dog \\over the}) = 0 $ , hence the overall probability of the above example sentence would be zero\n\n<hr>\n\n## One way to avoid, Smoothing!\n\n$$ MLE: P({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ ) \\over Count\\ (\\ B\\ )} $$\n\n**Why is it called MLE?**\n* Suppose a bigram “the dog” occurs 5 times in 100 documents.\n* Given that we have this 100 document corpus (which the language model represents), the maximum likelihood of the bigram parameter “the dog” appearing in the text is 0.05\n\n**Add 1 Smoothing (Laplace Smoothing)**\n$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ 1 \\over Count\\ (\\ B\\ )\\ +\\ |V|} $$\n* We pretend that each unique bigram occurs once more than it actually did! \n* Since we have added 1 to each bigram, we have added |V| bigrams in total. Hence normalizing by adding |V| to the denominator\n* Disadvantage: It reduces the probability of high frequency words\n\n\n**Add- $ \\delta $ Smoothing!**\n$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ \\delta \\over Count\\ (\\ B\\ )\\ +\\ \\delta * |V|} $$\n* $\\delta$ is any fraction such as 0.1, 0.05, etc.,\n* Unlike Add one smoothing, this solves the zero prob issue and avoids reducing the prob of high frequency words\n\n<hr>\n\n## 5. What is Markov Assumption?  \n\nSo the probability of occurrence of a 4-word sentence, by markov assumption is: \n$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B,\\ C,\\ D}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C,\\ D}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n\n\n\n> “What I see NOW depends only on what I saw in the PREVIOUS step”\n\n$$ P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1}) = P(w_i,\\ |\\ {w_{i-1}}) $$\n\nHence the probability of occurrence of a 4-word sentence with Markov Assumption is: \n$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n\n- What are its advantages?\n - Probability of an entire sentence could be very low but individual phrases could be more probable. <br>\n\tFor e.g.: <br>\n\t- Actual Data: “The quick fox jumps over the lazy dog”  \n\t- Probability of a new sentence: Prob(“The quick fox jumps over the lazy cat”) = 0 (though probable to occur,'cat' is not there in vocab, hence zero)\n\t- In Markov assumption (with additive smoothing), the above sentence will have a realistic probability\n    \n<hr>\n\n## 6. Evaluation \n\n### Perplexity\n\n- Perplexity is a measurement of how well a probability model predicts a sample. \n- It is used to compare probability models. \n- A low perplexity indicates the probability distribution is good at predicting the sample       \n\nDefinition: \n- Perplexity is the inverse probability of  the test set, normalized by the number  of words. <br>\nPerplexity of test data = PP(test data) =\n$$ P(w_1,\\ w_2,\\ ....,\\ w_N)^{1 \\over N}  $$\n$$ $$ \n$$ {\\sqrt[N]{1 \\over P(w_1,\\ w_2,\\ ....,\\ w_N)}} $$\n$$ $$\n$$ {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1})}}}$$ \n\n\n**the lower the perplexity of a LM for predicting a sample, higher is the confidence for that sample to occur in the distribution**\n\n<hr>\n\n### Log Probability\n- But multiplying probabilities like these could end up giving you the result closer to zero. \n - For e.g.: P(D|C) = 0.1, P(C|B) = 0.07, P(B|A) = 0.05, P(A) = 0.2\n - P(A before B before C before D before E) = 0.00007 $-->$ too low or almost zero !\n\n- Logarithm to the rescue ! \n - Logarithm is a monotonically increasing function ! \n - Meaning: If P(E|D) > P(D|C), then log(P(E|D)) > log (P(D|C))\n - Also, log (A *B) = log (A) + log (B) \n\n![LogarithmChart](https://upload.wikimedia.org/wikipedia/commons/1/17/Binary_logarithm_plot_with_ticks.svg)\n\n$$ \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) $$ \n- where N  is the number of words in the sentence. \n- Since log (probabilities) are always negative (see graph: log of values less than 1 is negative), shorter sentences will have higher probability of occurrence. \n\n**To normalize for length of sentences**, \n$$ {1 \\over N} \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n\n**Higher the log probability value of a LM in predicting a sample, higher is the confidence for that sample to occur in the distribution**\n\n<hr>\n\n**Perplexity and Log-Probability metrics for a Bigram Markov Language Model**\n\n$$ Perplexity\\ for\\ a\\ bigram\\ model = {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1}})}}}$$ \n\nComparing with normalized log probability for a bigram model:  \n$$ LogProbability\\ for\\ a\\ bigram\\ model = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n\n<hr>\n\n## 7. Application\n### Text Classification\n\nIn this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Bigram_Models--Text_Classification.ipynb), Markov Bigram model is implemented on a text classification problem. We can build a deterministic classifier where class is attributed to the class with highest log probability score.\n\n```python\nprint(bigram_markov(\"economic growth slows down\"))\n```\n\n```\n# log probability score of different classes\n[('business', -3.9692388514802905),\n ('politics', -6.7677569438805945),\n ('tech', -8.341093396600021),\n ('sport', -9.000286606679415),\n ('entertainment', -9.09610496174359)]\n\nClass attributed: 'business'\n```\n\n\n\n```python\nprint(bigram_markov(\"prime minister has gone to the US on an official trip\"))\n```\n\n```\n# log probability score of different classes\n[('politics', -3.5746871267381852),\n ('business', -7.440089097987748),\n ('tech', -10.252850263607193),\n ('sport', -11.981405120960808),\n ('entertainment', -12.124273481509913)]\n \n Class attributed: 'politics'\n```\n\n### Text Generation\n\nIn this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Char-level%20N-gram%20Language%20Generation%20Model.ipynb), a char-level LM, using trigrams and bigrams of characters, was used to generate dianosour names.\n\n```python\n>> print(names.iloc[:,0].head())\n\n# Names of Dianosours used as input\n0     Aachenosaurus\n1          Aardonyx\n2    Abdallahsaurus\n3       Abelisaurus\n4     Abrictosaurus\n```\n\n\n```python\n>> char_level_trigram_markov_model(names)\n\n# Names of Dianosours generated as output\nbroplisaurus\nprorkhorpsaurus\njintagasaurus\ncarsaurus\ngapophongasaurus\nteglangsaurus\nborudomachsaurus\nzheisaurus\nhorachillisaurus\naveiancsaurus\n```\n\n\nReferences:\n\n- Wikipedia\n- [Udemy Course on Deep NLP by Lazy Programmer](https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/) <br>\n- [NLP Course on Coursera by National Research University Higher School of Economics](https://www.coursera.org/learn/language-processing)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2020-03-17-introduction_to_statistical_language_modeling.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"aliases":["/Markov_Langugage_Model/Log_Probability/Perplexity/2020/03/17/Introduction_to_Statistical_Language_Modeling"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Statistics"],"date":"2020-03-17","description":"A gentle introduction to understand the ABCs of NLP in the era of Transformer LMs generating poems.","hide":false,"image":"images/language_model_intro/introduction_to_statistical_language_model.png","title":"Introduction to Statistical Language Modeling"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}