{"title":"Understanding Machine Learning Fundamentals - from a coder's viewpoint","markdown":{"yaml":{"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Coding","ML","Python"],"date":"2022-03-04","description":"This blog is inspired from my notes on Kaggle Learn Course on ML. It has code snippets in Scikit-learn and Pandas to put concepts into practices","output-file":"2022-03-04-Intro-to-ML.html","title":"Understanding Machine Learning Fundamentals - from a coder's viewpoint","image":"images/ML_fundamentals/ML.png","toc":true},"headingText":"Key Learnings from `Intro to ML Course` in Kaggle Learn","containsRefs":false,"markdown":"\n\n<hr>\n\n- How to `train_test_split` the data\n- Briefly discussed the concept of underfitting and overfitting (Loss vs model complexity curve)\n- How to train a typical scikit-learn model like `DecisionTreeRegressor` or `RandomForestRegressor` \n    - both need no scaling of continuous or discrete data;\n    - for sklearn might have to convert categorical data into encoded values\n- After finding out the best parameters, one should train with the identified hyperparameters on the whole data \n    - (so that model will learn a bit more from held out data too) \n       \n## Key Learnings from `Intermediary ML Course` in Kaggle Learn\n\n### Missing Value Treatment:\n- Remove the Null Rows OR Columns (by column meaning the whole feature containing the missing value)\n- Impute (by some strategy like Mean, Median, some regression like KNN)\n- Impute + Add a boolean variable for every column imputed (so as to make the model hopefully treat the imputed row differently)\n- Do removing missing values help or imputing missing values help more for the model accuracy?\n- Opinion shared by the Author: SimpleImputer works as effectively as a complex imputing algorithm when used inside sophisticated ML models\n\n<hr>\n\n### Categorical Column Treatment:\n    - `Drop Categorical columns` (worst approach)\n    - `OrdinalEncoder` \n    - `OneHotEncoder` (most cases, the best approach)\n- Learnt the concept of \"good_category_cols\" and \"bad_category_columns\" <br> (if a particular class occurs new in the unseen dataset; `handle_unknown` argument in \"OneHotEncoder\" is possible)\n- Think twice before applying onehot encoding because of \"high cardinality columns\"\n\n<hr>\n\n### Data Leakage\n- An example of Data Leakage: \n    - Doing Inputer before train_test_split. Validation data then would have \"seen\" training data  \n\n**Example 1 - `Nike`**: <br>\n\n- Objective: How much shoelace material will be used?\n- Situation: With the feature `Leather used this month` , the prediction accuracy is 98%+. Without this featur, the accuracy is just ~80%\n- IsDataLeakage?: `Depends` ! \n    - ❌ `Leather used this month` is a bad feature if the number is populated during the month (which makes it not available to predict the amount of shoe lace material needed)\n    - ✔️ `Leather used this month` is a okay feature to use if the number determined during the beginning of the month (making it available during predition time on unseen data)\n\n<hr>\n\n**Example 2 - `Nike`**: <br>\n\n- Objective: How much shoelace material will be used?\n- Situation: Can we use the feature `Leather order this month`?\n- IsDataLeakage? `Most likely no, however ...`\n    - ❌ If `Shoelaces ordered` (our Target Variable) is determined first and then only `Leather Ordered` is planned, <br>\n   then we won't have `Leather Ordered` during the time of prediction of unseen data\n    - ✔️ If `Leather Ordered` is determined before `Shoelaces Ordered`, then it is a useful feature\n\n\n<hr>\n\n**Example 3 - `Cryptocurrency`**: <br>\n\n- Objective: Predicting tomo's crypto price with a error of <$1\n- Situation: Are the following features susceptible to leakage?\n    - `Current price of Crypto`\n    - `Change in the price of crypto from 1 hour ago`\n    - `Avg Price of crypto in the largest 24 h0urs`\n    - `Macro-economic Features`\n    - `Tweets in the last 24 hours `\n- IsDataLeakage? `No`, none of the features seem to cause leakage.\n- However, more useful Target Variable `Change in Price (pos/neg) the next day`. If this can be consistently predicted higher, then it is a useful model\n\n<hr>\n\n**Example 4 - `Surgeon's Infection Rate Performance`**: <br>\n\n- Objective: How to predict if a patient who has undergone a surgery will get infected post surgery?\n- Situation: How can information about each surgeon's infection rate performance be carefully utilized while training?\n    - The independent features are strictly data points collected until the surgery had taken place\n    - The dependent variable - whether infected or not - should be post surgery measurement \n- IsDataLeakage? `Depends` on the what are the features used.\n    - If a surgeon's infection rate is used as a feature while training the model (that predicts whether a patient will be infected post surgery), that will lead to data leakage \n\n## Key Learnings from `Feature Engineering` Course in Kaggle Learn\n\n- Key Topics of this course: \n    - Mutual Information\n    - Inventing New Features (like `apparent temparature` = {Air Temparature + Humidity + Wind Speed})\n    - Segmentation Features (using K-Means Clustering)\n    - Variance in the Dataset based features (using Principal Component Analysis)\n    - Encode (high cardinality) category variables using `Target Encoding`\n- Why Feature Engineering?\n    - To improve model performance\n    - To reduce computational complexity by combining many features into a few\n    - To improve interpretability of results\n- Wherever the model cannot identify a proper relationship between a dependent and a particular independent variable, <br>\n    - we can engineer/transform 1 or more of the independent variables \n    - so as to let model learn a better relationship between the engineered features and dependent variable \n- E.g.: In `compressive_strength` prediction in `cement` data, synthetic feature - ratio of Water to Cement helps\n\n### Mutual Information\n- Mutual information is similar to correlation but correlation only looks for linear relationship whereas Mutual information can talk about any relationship\n- `Mutual Information` decribes relationship between two variables in terms of uncertainty (or certainty)\n    - For e.g.: Knowing `ExteriorQuality` of a house (one of 4 values - Fair, Typical, Good and Excellent) can help one reduce uncertainty over `SalePrice`. Better the ExteriorQuality, more the SalesPrice\n    - Typical values: If two variables have a MI score of 0.0 - they are totally indepndent. \n    - Mutual Information is a logarithmic quantity. So it increases slowly\n    - Mutual Information is a univariate metric. MI can't detect interactions between features Meaning, if multiple features together make sense to a dependent variable but not independently, then MI cannot determine that. Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects\n- Parallel Read for MI like metrics: \n    - [Feature Importances from fitted attribute](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)  \n    - [Recursive Feature Elimination](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n\n### Types of New Features:\n- Mathematical Transformations (Ratio, Log)\n- Grouping Columns Features\n      - `df['New_Group_Feature'] = df[list_of_boolean_features].sum(axis=1)`\n      - `df['New_Group_Feature'] = df[list_of_numerical_features].gt(0).sum(axis=1)` gt -- greater than \n- Grouping `numerical` Rows Features\n      - `customer['Avg_Income_by_State'] = customer.groupby('State')['Income'].transform('mean')`\n- Grouping `categorical` columns features\n      - `customer['StateFreq'] = customer.groupby('State')['State'].transform('count')/customer.State.count()`\n- Split Features\n      - df[['Type', 'Count']] = df['some_var'].str.split(\" \",expand=True)\n- Combine Features\n      - `df['new_feture'] = df['var1'] + \"_\" + df['var2']` \n       \n### Useful Tips on Feature Engineering:\n- Linear models learn sum and differences naturally\n- Neural Networks work better with scaled features\n- Ratios are difficult for many models, so can yeild better results when incorporated as additional feature\n- Tree models do not have the ability to factor in cound feature\n- Clustering as `feature discovery` tool (add a categorical feature based on clustering of a subset of features)\n\n### Principal Component Analysis\n- PCA is like partitioning of the variation in data\n- Instead of describing the data with the original features,\n- you do an orthogonl transformation of the features and compute \"principal components\" \n- which are used to explain the variation in the data. \n- Convert the correlated variables into mutually orthogonal (uncorrelated) `principal components`\n- Principal components can be more informative than the original features\n- Advantages of PCA:\n      - Dimensionality Reducton\n      - Anamoly Detection\n      - Boosting signal to noise ratio\n      - Decorrelation\n- PCA works only for numeric variables; works best for scaled data  \n- **Pipeline for PCA**: original_features --> Scaled_features --> PCA Features --> MI_computed_on_PCA_features\n\n### Target Encoding\n- `Target Encoding`: A Supervised Feature Engineering technique for encoding categorical variables by including the target labels\n- Target Encoding is basically assigning a number to a categorical variable where in the number is derived from target variable\n      - `autos['target_encoded_make'] =  autos.groupby('make')['Price'].transform('mean')`\n- Disadvantages of Target Encoding:\n      - Overfits for low volume (rare) classes\n      - What if there are missing values  \n- Where is Target Encoding most suitable?\n      - For High cardinality features\n      - Domain-motivated features (features that could have been scored poorly using feature importance metric function, we can unearth its real usefulness using target encoding)\n\n## ML Coding Tips\n- **sklearn**: \n    - `Pipleine`: Bundles together `preprocessing` and `modeling` steps | makes codebase easier for productionalizing\n    - `ColumnTransformer`: Bundles together different preprocessing steps\n    - Sklearn classes used often:\n    - **Model**\n        - `from sklearn.tree import DecisionTreeRegressor` AND `from sklearn.ensemble import RandomForestRegressor`\n        - `from sklearn.model_selection import train_test_split`\n        - `from sklearn.metrics import mean_absolute_error`\n        - `from sklearn.model_selection import cross_val_score`\n             - cross_val_score(my_pipeline, X, y, scorung='neg_mean_absolute_error')\n        - `from xgboost import XGBRegressor`\n             - `n_estimators`: Number of estimators is same as the number of cycles the data is processed by the model (`100-1000`)\n             - `early_stopping_rounds`: Early stopping stops the iteration when the validation score stops improving \n             - `learning_rate`\n                   - xgboost_model = XGBRegressor(n_estimators=500)\n                   - xgboost_model.fit(X_train,y_train,early_stopping_rounds=5,eval_set=[(X_valid, y_valid)], verbose=False)\n\n    - **Preprocessing & Feature Engineering**\n        - `from sklearn.feature_selection import mutual_info_regression, mutual_info_classif`\n        - `from sklearn.decomposition import PCA`\n        - `from sklearn.impute import SimpleImputer, KNNImputer`\n        - `from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n \n    \n- **Pandas**: \n    - X = df.copy() y = X.pop('TheDependentVariable') # remove the dependent variable from the X (features) and save in y)\n    - `df[encoded_colname], unique_values = df[colname].factorize()` # for converting a categorical list of values into encoded numbers using pandas\n    - `df[list_of_oh_encoded_col_name_values] = pd.get_dummies(df[colname])` # for converting a categorical variable into a list of oh-encoded-values using pandas\n    - Exclude all categorical columns at once:\n        - `df = df.select_dtypes(exclude=['object'])`\n    - Creating a new column just to show the model if which row of a particular column have null values\n        - df[col + '__ismissing'] = df[col].isnull() \n    - Isolate all categorical columns: \n        - `object_cols = [col for col in df.columns if df[col].dtype == \"object\"]`\n    - Segregate good and bad object columns (defined by the presence of \"unknown\" or new categories in validation or test dataset)\n        - `good_object_cols = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col])]`\n        - `bad_object_cols = list(set(good_object_cols) - set(bad_object_cols))`\n    - Getting number of unique entries (`cardinality`) across `object` or categorical columns\n        - `num_of_uniques_in_object_cols = list(map(lambda col: df[col].nunique(), object_cols))`  \n        - `sorted(list(zip(object_cols, num_of_uniques_in_object_cols)), key=lambda x: x[1], reverse=True)`\n\n**Source**: <br>\n- Kaggle.com/learn\n","srcMarkdownNoYaml":"\n\n<hr>\n\n## Key Learnings from `Intro to ML Course` in Kaggle Learn\n- How to `train_test_split` the data\n- Briefly discussed the concept of underfitting and overfitting (Loss vs model complexity curve)\n- How to train a typical scikit-learn model like `DecisionTreeRegressor` or `RandomForestRegressor` \n    - both need no scaling of continuous or discrete data;\n    - for sklearn might have to convert categorical data into encoded values\n- After finding out the best parameters, one should train with the identified hyperparameters on the whole data \n    - (so that model will learn a bit more from held out data too) \n       \n## Key Learnings from `Intermediary ML Course` in Kaggle Learn\n\n### Missing Value Treatment:\n- Remove the Null Rows OR Columns (by column meaning the whole feature containing the missing value)\n- Impute (by some strategy like Mean, Median, some regression like KNN)\n- Impute + Add a boolean variable for every column imputed (so as to make the model hopefully treat the imputed row differently)\n- Do removing missing values help or imputing missing values help more for the model accuracy?\n- Opinion shared by the Author: SimpleImputer works as effectively as a complex imputing algorithm when used inside sophisticated ML models\n\n<hr>\n\n### Categorical Column Treatment:\n    - `Drop Categorical columns` (worst approach)\n    - `OrdinalEncoder` \n    - `OneHotEncoder` (most cases, the best approach)\n- Learnt the concept of \"good_category_cols\" and \"bad_category_columns\" <br> (if a particular class occurs new in the unseen dataset; `handle_unknown` argument in \"OneHotEncoder\" is possible)\n- Think twice before applying onehot encoding because of \"high cardinality columns\"\n\n<hr>\n\n### Data Leakage\n- An example of Data Leakage: \n    - Doing Inputer before train_test_split. Validation data then would have \"seen\" training data  \n\n**Example 1 - `Nike`**: <br>\n\n- Objective: How much shoelace material will be used?\n- Situation: With the feature `Leather used this month` , the prediction accuracy is 98%+. Without this featur, the accuracy is just ~80%\n- IsDataLeakage?: `Depends` ! \n    - ❌ `Leather used this month` is a bad feature if the number is populated during the month (which makes it not available to predict the amount of shoe lace material needed)\n    - ✔️ `Leather used this month` is a okay feature to use if the number determined during the beginning of the month (making it available during predition time on unseen data)\n\n<hr>\n\n**Example 2 - `Nike`**: <br>\n\n- Objective: How much shoelace material will be used?\n- Situation: Can we use the feature `Leather order this month`?\n- IsDataLeakage? `Most likely no, however ...`\n    - ❌ If `Shoelaces ordered` (our Target Variable) is determined first and then only `Leather Ordered` is planned, <br>\n   then we won't have `Leather Ordered` during the time of prediction of unseen data\n    - ✔️ If `Leather Ordered` is determined before `Shoelaces Ordered`, then it is a useful feature\n\n\n<hr>\n\n**Example 3 - `Cryptocurrency`**: <br>\n\n- Objective: Predicting tomo's crypto price with a error of <$1\n- Situation: Are the following features susceptible to leakage?\n    - `Current price of Crypto`\n    - `Change in the price of crypto from 1 hour ago`\n    - `Avg Price of crypto in the largest 24 h0urs`\n    - `Macro-economic Features`\n    - `Tweets in the last 24 hours `\n- IsDataLeakage? `No`, none of the features seem to cause leakage.\n- However, more useful Target Variable `Change in Price (pos/neg) the next day`. If this can be consistently predicted higher, then it is a useful model\n\n<hr>\n\n**Example 4 - `Surgeon's Infection Rate Performance`**: <br>\n\n- Objective: How to predict if a patient who has undergone a surgery will get infected post surgery?\n- Situation: How can information about each surgeon's infection rate performance be carefully utilized while training?\n    - The independent features are strictly data points collected until the surgery had taken place\n    - The dependent variable - whether infected or not - should be post surgery measurement \n- IsDataLeakage? `Depends` on the what are the features used.\n    - If a surgeon's infection rate is used as a feature while training the model (that predicts whether a patient will be infected post surgery), that will lead to data leakage \n\n## Key Learnings from `Feature Engineering` Course in Kaggle Learn\n\n- Key Topics of this course: \n    - Mutual Information\n    - Inventing New Features (like `apparent temparature` = {Air Temparature + Humidity + Wind Speed})\n    - Segmentation Features (using K-Means Clustering)\n    - Variance in the Dataset based features (using Principal Component Analysis)\n    - Encode (high cardinality) category variables using `Target Encoding`\n- Why Feature Engineering?\n    - To improve model performance\n    - To reduce computational complexity by combining many features into a few\n    - To improve interpretability of results\n- Wherever the model cannot identify a proper relationship between a dependent and a particular independent variable, <br>\n    - we can engineer/transform 1 or more of the independent variables \n    - so as to let model learn a better relationship between the engineered features and dependent variable \n- E.g.: In `compressive_strength` prediction in `cement` data, synthetic feature - ratio of Water to Cement helps\n\n### Mutual Information\n- Mutual information is similar to correlation but correlation only looks for linear relationship whereas Mutual information can talk about any relationship\n- `Mutual Information` decribes relationship between two variables in terms of uncertainty (or certainty)\n    - For e.g.: Knowing `ExteriorQuality` of a house (one of 4 values - Fair, Typical, Good and Excellent) can help one reduce uncertainty over `SalePrice`. Better the ExteriorQuality, more the SalesPrice\n    - Typical values: If two variables have a MI score of 0.0 - they are totally indepndent. \n    - Mutual Information is a logarithmic quantity. So it increases slowly\n    - Mutual Information is a univariate metric. MI can't detect interactions between features Meaning, if multiple features together make sense to a dependent variable but not independently, then MI cannot determine that. Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects\n- Parallel Read for MI like metrics: \n    - [Feature Importances from fitted attribute](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)  \n    - [Recursive Feature Elimination](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n\n### Types of New Features:\n- Mathematical Transformations (Ratio, Log)\n- Grouping Columns Features\n      - `df['New_Group_Feature'] = df[list_of_boolean_features].sum(axis=1)`\n      - `df['New_Group_Feature'] = df[list_of_numerical_features].gt(0).sum(axis=1)` gt -- greater than \n- Grouping `numerical` Rows Features\n      - `customer['Avg_Income_by_State'] = customer.groupby('State')['Income'].transform('mean')`\n- Grouping `categorical` columns features\n      - `customer['StateFreq'] = customer.groupby('State')['State'].transform('count')/customer.State.count()`\n- Split Features\n      - df[['Type', 'Count']] = df['some_var'].str.split(\" \",expand=True)\n- Combine Features\n      - `df['new_feture'] = df['var1'] + \"_\" + df['var2']` \n       \n### Useful Tips on Feature Engineering:\n- Linear models learn sum and differences naturally\n- Neural Networks work better with scaled features\n- Ratios are difficult for many models, so can yeild better results when incorporated as additional feature\n- Tree models do not have the ability to factor in cound feature\n- Clustering as `feature discovery` tool (add a categorical feature based on clustering of a subset of features)\n\n### Principal Component Analysis\n- PCA is like partitioning of the variation in data\n- Instead of describing the data with the original features,\n- you do an orthogonl transformation of the features and compute \"principal components\" \n- which are used to explain the variation in the data. \n- Convert the correlated variables into mutually orthogonal (uncorrelated) `principal components`\n- Principal components can be more informative than the original features\n- Advantages of PCA:\n      - Dimensionality Reducton\n      - Anamoly Detection\n      - Boosting signal to noise ratio\n      - Decorrelation\n- PCA works only for numeric variables; works best for scaled data  \n- **Pipeline for PCA**: original_features --> Scaled_features --> PCA Features --> MI_computed_on_PCA_features\n\n### Target Encoding\n- `Target Encoding`: A Supervised Feature Engineering technique for encoding categorical variables by including the target labels\n- Target Encoding is basically assigning a number to a categorical variable where in the number is derived from target variable\n      - `autos['target_encoded_make'] =  autos.groupby('make')['Price'].transform('mean')`\n- Disadvantages of Target Encoding:\n      - Overfits for low volume (rare) classes\n      - What if there are missing values  \n- Where is Target Encoding most suitable?\n      - For High cardinality features\n      - Domain-motivated features (features that could have been scored poorly using feature importance metric function, we can unearth its real usefulness using target encoding)\n\n## ML Coding Tips\n- **sklearn**: \n    - `Pipleine`: Bundles together `preprocessing` and `modeling` steps | makes codebase easier for productionalizing\n    - `ColumnTransformer`: Bundles together different preprocessing steps\n    - Sklearn classes used often:\n    - **Model**\n        - `from sklearn.tree import DecisionTreeRegressor` AND `from sklearn.ensemble import RandomForestRegressor`\n        - `from sklearn.model_selection import train_test_split`\n        - `from sklearn.metrics import mean_absolute_error`\n        - `from sklearn.model_selection import cross_val_score`\n             - cross_val_score(my_pipeline, X, y, scorung='neg_mean_absolute_error')\n        - `from xgboost import XGBRegressor`\n             - `n_estimators`: Number of estimators is same as the number of cycles the data is processed by the model (`100-1000`)\n             - `early_stopping_rounds`: Early stopping stops the iteration when the validation score stops improving \n             - `learning_rate`\n                   - xgboost_model = XGBRegressor(n_estimators=500)\n                   - xgboost_model.fit(X_train,y_train,early_stopping_rounds=5,eval_set=[(X_valid, y_valid)], verbose=False)\n\n    - **Preprocessing & Feature Engineering**\n        - `from sklearn.feature_selection import mutual_info_regression, mutual_info_classif`\n        - `from sklearn.decomposition import PCA`\n        - `from sklearn.impute import SimpleImputer, KNNImputer`\n        - `from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n \n    \n- **Pandas**: \n    - X = df.copy() y = X.pop('TheDependentVariable') # remove the dependent variable from the X (features) and save in y)\n    - `df[encoded_colname], unique_values = df[colname].factorize()` # for converting a categorical list of values into encoded numbers using pandas\n    - `df[list_of_oh_encoded_col_name_values] = pd.get_dummies(df[colname])` # for converting a categorical variable into a list of oh-encoded-values using pandas\n    - Exclude all categorical columns at once:\n        - `df = df.select_dtypes(exclude=['object'])`\n    - Creating a new column just to show the model if which row of a particular column have null values\n        - df[col + '__ismissing'] = df[col].isnull() \n    - Isolate all categorical columns: \n        - `object_cols = [col for col in df.columns if df[col].dtype == \"object\"]`\n    - Segregate good and bad object columns (defined by the presence of \"unknown\" or new categories in validation or test dataset)\n        - `good_object_cols = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col])]`\n        - `bad_object_cols = list(set(good_object_cols) - set(bad_object_cols))`\n    - Getting number of unique entries (`cardinality`) across `object` or categorical columns\n        - `num_of_uniques_in_object_cols = list(map(lambda col: df[col].nunique(), object_cols))`  \n        - `sorted(list(zip(object_cols, num_of_uniques_in_object_cols)), key=lambda x: x[1], reverse=True)`\n\n**Source**: <br>\n- Kaggle.com/learn\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2022-03-04-Intro-to-ML.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"author":"Senthil Kumar","badges":true,"branch":"master","categories":["Coding","ML","Python"],"date":"2022-03-04","description":"This blog is inspired from my notes on Kaggle Learn Course on ML. It has code snippets in Scikit-learn and Pandas to put concepts into practices","title":"Understanding Machine Learning Fundamentals - from a coder's viewpoint","image":"images/ML_fundamentals/ML.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}