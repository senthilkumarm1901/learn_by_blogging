{"title":"How to train a Spacy NER Model","markdown":{"yaml":{"aliases":["/spacy/NER/2021/06/25/Spacy_ML_NER"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP","DL","Coding","Python"],"date":"2021-06-25","description":"In this blog post, I cover the process of creating trained ML NER model from Unlabeled data","hide":false,"image":"images/spacy/spacy_ML_model_training_on_unlabeld_data.png","output-file":"2021-06-25-spacy_ml_ner.html","title":"How to train a Spacy NER Model","toc":true},"headingText":"1. Introduction","containsRefs":false,"markdown":"\n\n\n\n<hr>\n\n\n### TL;DR Summary of the Blog\n\n**How do we create ML NER Model from no labeled data?**: \n\n- Prepare **rules-bootstrapped training data** from unlabeled corpus\n   - If it is possible/easy  to annotate directly, one can do that. \n   - However, if rules taggging is possible \n       - In the **Disease NER dataset** example here\n       - there is an opportunity to use a huge list of words to tag via rules first\n       - then labeling becomes easier than labeling from scratch\n- Rules-boostrapped data is then **reviewed/edited by human annotators** \n- **Stratify Split** the human-reviewed data into train-dev-test at sentences level\n- Optimize and **Train** one or more Spacy ML NER Models \n- **Compare** and **Evaluate** the accuracy of the models \n\n![](spacy_model_ner/spacy_ML_model_training_on_unlabeld_data.png)\n\n**Now, we can list the above steps with a DISEASE NER example ...**\n\n- We have a `ncbi_disease` dataset of `7295` sentences speaking of various entities of which `disease` entity is of focus for us. \n> E.g.: \"Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\" <br>\n> `adenomatous polyposis coli tumour` is a `DISEASE` entity\n\n- Source of this dataset: [link](https://huggingface.co/datasets/lewtun/autoevaluate__ncbi_disease)\n\n- For the sake of the argument of this blog, we assume this dataset does not have labels. \n- In most real world datasets, we are  not likely to have labeled data\n\n<br>\n\n- Hence the below pipeline helps in building an ML model\n> Unlabeled Sentences speaking of various diseases <br>\n> Tag `DISEASE` NER via Rules using a huge list of [disease words](https://raw.githubusercontent.com/Shivanshu-Gupta/web-scrapers/master/medical_ner/medicinenet-diseases.json) <br>\n> Review/Edit the Rules-bootstrapped NER (tagging NER from scratch is a lot tougher) <br>\n> Split the Data into train-dev-test<br>\n> Train an ML model on train and dev datasets and evaluate on unseen test dataset<br>\n> Evaluate & Compare the Rules Model (baseline) and Spacy ML NER models (built from spacy-small and roberta base)\n\n## 2. Prepare Rules-bootstrapped Data from unlabeled data\n\n### 2A. Loading the disease words from an external file \n\n### 2B. Convert Disease Words into Spacy Patterns\n\n### 2C. Create `Disease NER` out of spacy patterns\n\n- Want to know more about creating rules NER? <br> \nRefer the below blog article <br>[learn_by_blogging/How_to_Leverage_Spacy_Rules_NER](https://senthilkumarm1901.github.io/learn_by_blogging/spacy/ner/2021/05/09/Spacy_Rules_NER.html)\n\n### 2D. Infer Spacy Rules NER as token-level results\n\n- Ofcourse, there are mistakes in this `rules_ner output` like in row #11 where `tumour` is not tagged `I-DISEASE`\n- We rectify the mistakes of rules by human annotion\n\n## 3. Human Review of the Rules Output\n\n### 3A. Edit token-level results in csv by human annotation/review\n\n## 4. Stratify Split the Data based on human annotations\n\nFrom the above table, we can infer that <br>\n- there are more multi-token diseases than single-token diseases\n- there are 3.3K sentences with only `O` as the token\n<br>\n<br>\nWe have to ensure all three splits - train, dev and test - have the same percentage of `O`, `B-DISEASE|I-DISEASE|O` and `B-DISEASE|O`\n\nAfter spliting into train-dev-test in a 80-10-10 split ...\n\n## 5. Train ML Model\n\n### 5A. Convert token-level Results 2 Spacy Model-acceptable Conll Data\n\n### 5B. Train a Spacy Small ML Model\n\n**CLI command for Spacy Model Training**:\n\n```\n!python3 -m spacy train $CONFIG_DIR/original_spacy_small_ner_config.cfg \\\n--output $SPACY_SMALL_MODEL_DIR_GPU \\\n--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n--verbose \\\n-g 0\n```\n\n**The output from the Spacy Model Training**:\n\n```\n[2022-05-24 07:09:39,410] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\nℹ Saving to output directory:\n../data/model_weights/spacy_small\nℹ Using GPU: 0\n\n=========================== Initializing pipeline ===========================\n[2022-05-24 07:09:41,789] [INFO] Set up nlp object from config\n[2022-05-24 07:09:41,797] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:09:41,798] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n[2022-05-24 07:09:41,798] [INFO] Pipeline: ['tok2vec', 'ner']\n[2022-05-24 07:09:41,803] [INFO] Created vocabulary\n[2022-05-24 07:09:41,804] [INFO] Finished initializing nlp object\n\n[2022-05-24 07:09:51,839] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n✔ Initialized pipeline\n\n============================= Training pipeline =============================\n[2022-05-24 07:09:51,852] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:09:51,853] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\nℹ Pipeline: ['tok2vec', 'ner']\nℹ Initial learn rate: 0.001\nE    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  ------------  --------  ------  ------  ------  ------\n  0       0          0.00     41.00    0.31    0.33    0.29    0.00\n  0     200        195.83   1820.96   41.44   55.56   33.05    0.41             \n  0     400        106.20   1131.87   47.27   66.58   36.64    0.47             \n  0     600         64.99    969.69   74.94   77.17   72.84    0.75             \n  0     800         87.66   1096.80   74.39   76.96   71.98    0.74             \n  1    1000         82.51   1134.93   77.53   79.28   75.86    0.78             \n  1    1200        113.61   1122.73   80.73   82.36   79.17    0.81             \n  1    1400        128.84   1178.08   84.40   86.10   82.76    0.84             \n...   \n 26    5400        287.07    591.02   85.96   87.04   84.91    0.86             \n 27    5600        382.16    540.78   86.21   87.56   84.91    0.86             \n 28    5800        406.03    615.57   86.29   86.67   85.92    0.86             \nEpoch 29:   0%|                                         | 0/200 [00:00<?, ?it/s]✔ Saved pipeline to output directory\n../data/model_weights/spacy_small/model-last\n```\n\n**Command for Evaluating the Model Results**:\n\n```\n!python3 -m spacy evaluate $SPACY_SMALL_MODEL_DIR_GPU/model-best $SPACY_DATA_DIR/test_data.spacy \\\n--output $SPACY_SMALL_MODEL_DIR_GPU/model-best/spacy_small_model_evaluation.json \\\n--gpu-id 0\n```\n\n**Output of Evaluate Command** :\n\n```\nℹ Using GPU: 0\n\n================================== Results ==================================\n\nTOK     -    \nNER P   89.75\nNER R   82.81\nNER F   86.14\nSPEED   20752\n\n\n=============================== NER (per type) ===============================\n\n              P       R       F\nDISEASE   89.75   82.81   86.14\n\n✔ Saved results to\n../data/model_weights/spacy_small/model-best/spacy_small_model_evaluation.json\n```\n\n### 5C. Train a Spacy Roberta Base ML Model\n\n**CLI command for Spacy Model Training**:\n\n```\n!python3 -m spacy train $CONFIG_DIR/original_trf_config.cfg \\\n--output $SPACY_ROBERTA_MODEL_DIR_GPU \\\n--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n--verbose \\\n-g 0\n```\n\n**The output from the Spacy Model Training**:\n\n```\n\n[2022-05-24 07:44:08,351] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n✔ Created output directory:\n../data/model_weights/spacy_roberta_base_\nℹ Saving to output directory:\n../data/model_weights/spacy_roberta_base_\nℹ Using GPU: 0\n\n=========================== Initializing pipeline ===========================\n[2022-05-24 07:44:11,169] [INFO] Set up nlp object from config\n[2022-05-24 07:44:11,178] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:44:11,180] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n[2022-05-24 07:44:11,180] [INFO] Pipeline: ['transformer', 'ner']\n[2022-05-24 07:44:11,184] [INFO] Created vocabulary\n[2022-05-24 07:44:11,185] [INFO] Finished initializing nlp object\n\n[2022-05-24 07:44:22,286] [INFO] Initialized pipeline components: ['transformer', 'ner']\n✔ Initialized pipeline\n\n============================= Training pipeline =============================\n[2022-05-24 07:44:22,298] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:44:22,299] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\nℹ Pipeline: ['transformer', 'ner']\nℹ Initial learn rate: 0.0\nE    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  -------------  --------  ------  ------  ------  ------\n  0       0        4392.55    285.04    0.21    0.17    0.29    0.00\n  1     200      126102.97  33471.51   84.94   81.78   88.36    0.85\n  3     400        1782.34   2642.55   89.50   90.83   88.22    0.90\n  5     600        1123.23   1596.50   90.69   89.06   92.39    0.91\n...\n 27    2800         153.06    176.41   90.94   90.35   91.52    0.91\n 29    3000         103.80    128.69   90.86   88.98   92.82    0.91\n 30    3200         121.04    141.70   91.47   90.56   92.39    0.91\n 32    3400          90.05    116.95   90.51   89.93   91.09    0.91\n 34    3600         111.25    131.25   91.12   90.15   92.10    0.91\n 36    3800          79.87     82.69   90.30   89.66   90.95    0.90\n 38    4000          82.07     82.97   90.97   90.01   91.95    0.91\n✔ Saved pipeline to output directory\n../data/model_weights/spacy_roberta_base_/model-last\nCPU times: user 11.1 s, sys: 2.78 s, total: 13.9 s\nWall time: 22min 37s\n\n```\n\n**Command for Evaluating the Model Results**:\n\n```\n!python3 -m spacy evaluate $SPACY_SMALL_MODEL_DIR_GPU/model-best $SPACY_DATA_DIR/test_data.spacy \\\n--output $SPACY_SMALL_MODEL_DIR_GPU/model-best/spacy_small_model_evaluation.json \\\n--gpu-id 0\n```\n\n**Output of Evaluate Command** :\n\n```\nℹ Using GPU: 0\n\n================================== Results ==================================\n\nTOK     -    \nNER P   88.61\nNER R   90.26\nNER F   89.43\nSPEED   12020\n\n\n=============================== NER (per type) ===============================\n\n              P       R       F\nDISEASE   88.61   90.26   89.43\n\n✔ Saved results to\n../data/model_weights/spacy_roberta_base_/model-best/spacy_roberta_base_evaluation.json\n```\n\n## 6. Evaluate the models\n\n### 6A. Comparing the entity-level F1-score of (1) Rules, (2) Spacy small and (3) Spacy Roberta-base model\n\n\n## 7. Conclusion\n\nIn this blog article, we have shown how to effectively build a NER model on unlabeled data. <br>\n- We have compared Rules NER, a Spacy Small NER and roberta-base NER models. \n- We found `roberta_base` model is having the highest F1 score of 89%.  \n- We can also ensemble results of `Spacy Small NER` and `Spacy Roberta Base NER` models.  \n- A digression from the scope of this article: There are umpteen good tools (paid mostly) aiding the annotation. Sometimes, for simple NER problem (like tagging only one entity like this `Disease NER`), even excel is good for annotation\n\nReferences:\n- https://spacy.io/api/cli#evaluate\n- If you would like to replicate the above results, refer to the `DiseaseNER` notebooks in [this link](https://github.com/senthilkumarm1901/myCodingProjects/tree/main/SpacyNER/notebooks)\n","srcMarkdownNoYaml":"\n\n\n\n<hr>\n\n## 1. Introduction\n\n### TL;DR Summary of the Blog\n\n**How do we create ML NER Model from no labeled data?**: \n\n- Prepare **rules-bootstrapped training data** from unlabeled corpus\n   - If it is possible/easy  to annotate directly, one can do that. \n   - However, if rules taggging is possible \n       - In the **Disease NER dataset** example here\n       - there is an opportunity to use a huge list of words to tag via rules first\n       - then labeling becomes easier than labeling from scratch\n- Rules-boostrapped data is then **reviewed/edited by human annotators** \n- **Stratify Split** the human-reviewed data into train-dev-test at sentences level\n- Optimize and **Train** one or more Spacy ML NER Models \n- **Compare** and **Evaluate** the accuracy of the models \n\n![](spacy_model_ner/spacy_ML_model_training_on_unlabeld_data.png)\n\n**Now, we can list the above steps with a DISEASE NER example ...**\n\n- We have a `ncbi_disease` dataset of `7295` sentences speaking of various entities of which `disease` entity is of focus for us. \n> E.g.: \"Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\" <br>\n> `adenomatous polyposis coli tumour` is a `DISEASE` entity\n\n- Source of this dataset: [link](https://huggingface.co/datasets/lewtun/autoevaluate__ncbi_disease)\n\n- For the sake of the argument of this blog, we assume this dataset does not have labels. \n- In most real world datasets, we are  not likely to have labeled data\n\n<br>\n\n- Hence the below pipeline helps in building an ML model\n> Unlabeled Sentences speaking of various diseases <br>\n> Tag `DISEASE` NER via Rules using a huge list of [disease words](https://raw.githubusercontent.com/Shivanshu-Gupta/web-scrapers/master/medical_ner/medicinenet-diseases.json) <br>\n> Review/Edit the Rules-bootstrapped NER (tagging NER from scratch is a lot tougher) <br>\n> Split the Data into train-dev-test<br>\n> Train an ML model on train and dev datasets and evaluate on unseen test dataset<br>\n> Evaluate & Compare the Rules Model (baseline) and Spacy ML NER models (built from spacy-small and roberta base)\n\n## 2. Prepare Rules-bootstrapped Data from unlabeled data\n\n### 2A. Loading the disease words from an external file \n\n### 2B. Convert Disease Words into Spacy Patterns\n\n### 2C. Create `Disease NER` out of spacy patterns\n\n- Want to know more about creating rules NER? <br> \nRefer the below blog article <br>[learn_by_blogging/How_to_Leverage_Spacy_Rules_NER](https://senthilkumarm1901.github.io/learn_by_blogging/spacy/ner/2021/05/09/Spacy_Rules_NER.html)\n\n### 2D. Infer Spacy Rules NER as token-level results\n\n- Ofcourse, there are mistakes in this `rules_ner output` like in row #11 where `tumour` is not tagged `I-DISEASE`\n- We rectify the mistakes of rules by human annotion\n\n## 3. Human Review of the Rules Output\n\n### 3A. Edit token-level results in csv by human annotation/review\n\n## 4. Stratify Split the Data based on human annotations\n\nFrom the above table, we can infer that <br>\n- there are more multi-token diseases than single-token diseases\n- there are 3.3K sentences with only `O` as the token\n<br>\n<br>\nWe have to ensure all three splits - train, dev and test - have the same percentage of `O`, `B-DISEASE|I-DISEASE|O` and `B-DISEASE|O`\n\nAfter spliting into train-dev-test in a 80-10-10 split ...\n\n## 5. Train ML Model\n\n### 5A. Convert token-level Results 2 Spacy Model-acceptable Conll Data\n\n### 5B. Train a Spacy Small ML Model\n\n**CLI command for Spacy Model Training**:\n\n```\n!python3 -m spacy train $CONFIG_DIR/original_spacy_small_ner_config.cfg \\\n--output $SPACY_SMALL_MODEL_DIR_GPU \\\n--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n--verbose \\\n-g 0\n```\n\n**The output from the Spacy Model Training**:\n\n```\n[2022-05-24 07:09:39,410] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\nℹ Saving to output directory:\n../data/model_weights/spacy_small\nℹ Using GPU: 0\n\n=========================== Initializing pipeline ===========================\n[2022-05-24 07:09:41,789] [INFO] Set up nlp object from config\n[2022-05-24 07:09:41,797] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:09:41,798] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n[2022-05-24 07:09:41,798] [INFO] Pipeline: ['tok2vec', 'ner']\n[2022-05-24 07:09:41,803] [INFO] Created vocabulary\n[2022-05-24 07:09:41,804] [INFO] Finished initializing nlp object\n\n[2022-05-24 07:09:51,839] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n✔ Initialized pipeline\n\n============================= Training pipeline =============================\n[2022-05-24 07:09:51,852] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:09:51,853] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\nℹ Pipeline: ['tok2vec', 'ner']\nℹ Initial learn rate: 0.001\nE    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  ------------  --------  ------  ------  ------  ------\n  0       0          0.00     41.00    0.31    0.33    0.29    0.00\n  0     200        195.83   1820.96   41.44   55.56   33.05    0.41             \n  0     400        106.20   1131.87   47.27   66.58   36.64    0.47             \n  0     600         64.99    969.69   74.94   77.17   72.84    0.75             \n  0     800         87.66   1096.80   74.39   76.96   71.98    0.74             \n  1    1000         82.51   1134.93   77.53   79.28   75.86    0.78             \n  1    1200        113.61   1122.73   80.73   82.36   79.17    0.81             \n  1    1400        128.84   1178.08   84.40   86.10   82.76    0.84             \n...   \n 26    5400        287.07    591.02   85.96   87.04   84.91    0.86             \n 27    5600        382.16    540.78   86.21   87.56   84.91    0.86             \n 28    5800        406.03    615.57   86.29   86.67   85.92    0.86             \nEpoch 29:   0%|                                         | 0/200 [00:00<?, ?it/s]✔ Saved pipeline to output directory\n../data/model_weights/spacy_small/model-last\n```\n\n**Command for Evaluating the Model Results**:\n\n```\n!python3 -m spacy evaluate $SPACY_SMALL_MODEL_DIR_GPU/model-best $SPACY_DATA_DIR/test_data.spacy \\\n--output $SPACY_SMALL_MODEL_DIR_GPU/model-best/spacy_small_model_evaluation.json \\\n--gpu-id 0\n```\n\n**Output of Evaluate Command** :\n\n```\nℹ Using GPU: 0\n\n================================== Results ==================================\n\nTOK     -    \nNER P   89.75\nNER R   82.81\nNER F   86.14\nSPEED   20752\n\n\n=============================== NER (per type) ===============================\n\n              P       R       F\nDISEASE   89.75   82.81   86.14\n\n✔ Saved results to\n../data/model_weights/spacy_small/model-best/spacy_small_model_evaluation.json\n```\n\n### 5C. Train a Spacy Roberta Base ML Model\n\n**CLI command for Spacy Model Training**:\n\n```\n!python3 -m spacy train $CONFIG_DIR/original_trf_config.cfg \\\n--output $SPACY_ROBERTA_MODEL_DIR_GPU \\\n--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n--verbose \\\n-g 0\n```\n\n**The output from the Spacy Model Training**:\n\n```\n\n[2022-05-24 07:44:08,351] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n✔ Created output directory:\n../data/model_weights/spacy_roberta_base_\nℹ Saving to output directory:\n../data/model_weights/spacy_roberta_base_\nℹ Using GPU: 0\n\n=========================== Initializing pipeline ===========================\n[2022-05-24 07:44:11,169] [INFO] Set up nlp object from config\n[2022-05-24 07:44:11,178] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:44:11,180] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n[2022-05-24 07:44:11,180] [INFO] Pipeline: ['transformer', 'ner']\n[2022-05-24 07:44:11,184] [INFO] Created vocabulary\n[2022-05-24 07:44:11,185] [INFO] Finished initializing nlp object\n\n[2022-05-24 07:44:22,286] [INFO] Initialized pipeline components: ['transformer', 'ner']\n✔ Initialized pipeline\n\n============================= Training pipeline =============================\n[2022-05-24 07:44:22,298] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:44:22,299] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\nℹ Pipeline: ['transformer', 'ner']\nℹ Initial learn rate: 0.0\nE    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  -------------  --------  ------  ------  ------  ------\n  0       0        4392.55    285.04    0.21    0.17    0.29    0.00\n  1     200      126102.97  33471.51   84.94   81.78   88.36    0.85\n  3     400        1782.34   2642.55   89.50   90.83   88.22    0.90\n  5     600        1123.23   1596.50   90.69   89.06   92.39    0.91\n...\n 27    2800         153.06    176.41   90.94   90.35   91.52    0.91\n 29    3000         103.80    128.69   90.86   88.98   92.82    0.91\n 30    3200         121.04    141.70   91.47   90.56   92.39    0.91\n 32    3400          90.05    116.95   90.51   89.93   91.09    0.91\n 34    3600         111.25    131.25   91.12   90.15   92.10    0.91\n 36    3800          79.87     82.69   90.30   89.66   90.95    0.90\n 38    4000          82.07     82.97   90.97   90.01   91.95    0.91\n✔ Saved pipeline to output directory\n../data/model_weights/spacy_roberta_base_/model-last\nCPU times: user 11.1 s, sys: 2.78 s, total: 13.9 s\nWall time: 22min 37s\n\n```\n\n**Command for Evaluating the Model Results**:\n\n```\n!python3 -m spacy evaluate $SPACY_SMALL_MODEL_DIR_GPU/model-best $SPACY_DATA_DIR/test_data.spacy \\\n--output $SPACY_SMALL_MODEL_DIR_GPU/model-best/spacy_small_model_evaluation.json \\\n--gpu-id 0\n```\n\n**Output of Evaluate Command** :\n\n```\nℹ Using GPU: 0\n\n================================== Results ==================================\n\nTOK     -    \nNER P   88.61\nNER R   90.26\nNER F   89.43\nSPEED   12020\n\n\n=============================== NER (per type) ===============================\n\n              P       R       F\nDISEASE   88.61   90.26   89.43\n\n✔ Saved results to\n../data/model_weights/spacy_roberta_base_/model-best/spacy_roberta_base_evaluation.json\n```\n\n## 6. Evaluate the models\n\n### 6A. Comparing the entity-level F1-score of (1) Rules, (2) Spacy small and (3) Spacy Roberta-base model\n\n\n## 7. Conclusion\n\nIn this blog article, we have shown how to effectively build a NER model on unlabeled data. <br>\n- We have compared Rules NER, a Spacy Small NER and roberta-base NER models. \n- We found `roberta_base` model is having the highest F1 score of 89%.  \n- We can also ensemble results of `Spacy Small NER` and `Spacy Roberta Base NER` models.  \n- A digression from the scope of this article: There are umpteen good tools (paid mostly) aiding the annotation. Sometimes, for simple NER problem (like tagging only one entity like this `Disease NER`), even excel is good for annotation\n\nReferences:\n- https://spacy.io/api/cli#evaluate\n- If you would like to replicate the above results, refer to the `DiseaseNER` notebooks in [this link](https://github.com/senthilkumarm1901/myCodingProjects/tree/main/SpacyNER/notebooks)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2021-06-25-spacy_ml_ner.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"aliases":["/spacy/NER/2021/06/25/Spacy_ML_NER"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP","DL","Coding","Python"],"date":"2021-06-25","description":"In this blog post, I cover the process of creating trained ML NER model from Unlabeled data","hide":false,"image":"images/spacy/spacy_ML_model_training_on_unlabeld_data.png","title":"How to train a Spacy NER Model"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}