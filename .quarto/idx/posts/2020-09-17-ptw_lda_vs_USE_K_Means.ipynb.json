{"title":"Comparing Two Unsupervised Clustering Algorithms for Text Data","markdown":{"yaml":{"aliases":["/Topic_Modeling/LDA/Seeded_LDA/Universal_Sentence_Encoder/KMeans/2020/09/17/ptw_lda_vs_USE_K_Means"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["ML","NLP"],"date":"2020-09-17","description":"A Short Study comparing PTW_LDA and Transfer Learning powered KMeans on Text Data","hide":false,"image":"images/LDA_vs_KMeans/data1_evaluation_complete.PNG","output-file":"2020-09-17-ptw_lda_vs_use_k_means.html","title":"Comparing Two Unsupervised Clustering Algorithms for Text Data","toc":true},"headingText":"I. Introduction","containsRefs":false,"markdown":"\n\n\n\n\n### What algorithms are covered here?\n\n- **Prior-topic words** guided *Latent Dirichlet Allocation* and \n- **Universal Sentence Encoder** (USE) powered *K-Means*\n\n### What questions am I answering here?\n\n- Given that USE can encode complex semantic information, **is LDA worthwhile?** \n\n- **Can LDA be used for at least some type of data**, where it can produce better/on-par results compared to USE-K-Means? \n\n\nBefore diving into the study, let us understand how **USE-KMeans** and **PTW-guided LDA** works\n\n<hr>\n\n## II. About USE\n\n### IIA. How USE works?\n\n**USE** converts sentences into 512 embeddings\n![](https://www.learnopencv.com/wp-content/uploads/2018/11/Universal-Sentence-Encoder.png)\n\n<hr>\n\n**Semantic Similarity** Correlation Matrix\n![](https://2.bp.blogspot.com/-9Qk1fubLpzg/Wv2QGgKVVmI/AAAAAAAACvs/Gm-XF3prXVIIvaIkrTmkcIcYz-4qSxLKwCLcBGAs/s400/image2.png)\n\n<hr>\n\nDespite many common words, semantically different sentences will have **dissimilar embeddings** \n![](https://1.bp.blogspot.com/-w2kAi39zPrE/Wv2OPHTwDgI/AAAAAAAACvY/aQzvBcaIqYkw8McCBcXlTx0pj9FbILH0ACLcBGAs/s400/image4.png)\n\n### IIB. USE Architecture\n\nThere are two variants\n\n**Variant 1: Transformer Encoder**: <br>\n![](https://amitness.com/images/use-transformer-variant.png)\n\n<small>source: https://amitness.com/2020/06/universal-sentence-encoder/ </small>\n\nWhat does a **Transformer Encoder** Layer comprise?\n- Self Attention Layer \n- Feed Forward Network Layer\n![](https://amitness.com/images/use-transformer-one-layer.png)\n\n**Variant 2: Deep Averaging Network**:<br>\n![](https://amitness.com/images/use-deep-averaging-network-variant.png)\n\n### IIC. Pre-trained Tasks in USE\n\nOverall Pipeline of how **USE** is trained: <br>\n![](https://amitness.com/images/use-overall-pipeline.png)\n\n<hr>\n\nA. Skip-thought prediction: <br>\n- Use `Central Sentence` to predict both the `Previous Sentence` and `Next Sentence`\n\n![](https://amitness.com/images/nlp-ssl-neighbor-sentence.gif)\n\nB. Response Prediction: <br>\n- Train the USE architecture to do `smart reply` prediction\n\n![](https://3.bp.blogspot.com/-qcqYQcxfLS0/Wv2Pxmm945I/AAAAAAAACvk/decC5VtlRGUdD4NqCui3HgNd3LXdjEvlgCLcBGAs/s640/image3.gif)\n\nC. Natural Language Inference: <br>\n- Do `NLI` task prediction, where given a premise and a hypothesis, the model is trained to predict whether the hypothesis is an `entailment`, `contradition` or `neutral` to the premise\n\n![](images/LDA_vs_KMeans/markdown_table1.PNG)\n\n## III. About LDA\n\n- **Latent**: <br>\n    - Topic structures in a document are **latent/hidden** in the text\n\n\n- **Dirichlet**: <br>\n    - The **Dirichlet** distribution determines the mixture proportions of <br> \n            - the topics in the documents and\n            - the words in each topic.\n- **Allocation**: <br>\n    - **Allocation** of words to a given topic and allocation of topics to a document\n\n### IIIA. Intuitive understanding of Dirichlet Distribution\n\n- A **Normal/Gaussian** distribution is a continuous probability distribution over all the real numbers\n    - It is described by a mean and a variance.\n\n![](https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg)\n\n- A **Poisson** distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval \n\n![](https://upload.wikimedia.org/wikipedia/commons/1/16/Poisson_pmf.svg)\n\n> The Poisson distribution is specified by one parameter: lambda (λ). As lambda increases to sufficiently large values, the normal distribution (λ, λ) may be used to approximate the Poisson distribution\n> Source:https://en.wikipedia.org/wiki/Poisson_distribution \n\n<hr>\n\n- Now, what is **Dirichlet Distribution**? <br>\n    - The dirichlet distribution is a probability distribution as well\n    - but it is not sampling from the space of real numbers. Instead it is sampling over a probability simplex <br>\n\n```\n (0.6, 0.4)\n (0.1, 0.1, 0.8)\n (0.05, 0.2, 0.15, 0.1, 0.3, 0.2)\n```\n\n![](images/LDA_vs_KMeans/dirichlet_distribution_of_words_topics.PNG)\n\n- How Dirichlet Distribution varies w.r.t dirichlet prior: <br>\n    - The below image shows Dir(α) <br>\n    - As α increases from 0.05 (1/20) to 0.1, 0.2, 0.4 respectively in plots from left to right & top to down, you can see the distribution becoming more uniform. <br>\n![](images/LDA_vs_KMeans/dirichlet_distribution_with_prior.png)    \n\n### IIIB. How LDA Works?\n\n- LDA is a generative model <br>\n- LDA processes documents as 'bag of words' -- ordering of words is not important\n\n![](images/LDA_vs_KMeans/generative_process_1.png)\n\nIn principle, LDA generates a document based on **dirichlet distribution (dd) of topics over documents** and **dd of words over topics**\n\n![](images/LDA_vs_KMeans/generative_process_2.png)\n\n<br>\n<br>\n\nBut we inverse the generative process for statistical inference \n\n![](images/LDA_vs_KMeans/generative_process_3.png)\n\n### IIIC. Hyperparameters of LDA\n\n![](LDA_vs_KMeans/plate_notation_LDA.png)\n\nD = Total No. of Documents <br>\nN = No. of Words = Vocab Size <br>\nT = No. of Topics <br>\n<br>\nθd = Topic Distribution for a particular document d\n\n![](images/LDA_vs_KMeans/plate_notation_2.png)\n\n<br>\nΦt= Word Distribution for a topic t. Here for topic 1 and 2. \n\n![](images/LDA_vs_KMeans/plate_notation_3.png)\n\n(colored books represent words/tokens)\n\n```\nZd,n = Topic Distribution for n th word in document d\nWd,n = nth word in dth document\n```\n\n```\nα= parameter that sets the dircihlet prior on the per-document topic distribution (θ)\n= parameter that represents the doc-topic density\n= determines the no. of topics in each doc\n= (Default) = 1/num_of_topics (in sklearn and gensim)\ndecreasing alpha results in less number of topics per document\n \nβ= parameter that sets the dirichlet prior on the per-topic word distribution(ϕ)\n= parameter that represents the topic-word density\n= determines the no. of words per each topic\n= (Default) = 1/num_of_topics (in sklearn and gensim)\ndecreasing beta results in just a few major words in topics\n\nm = base measure for per-document topic distribution; a simplex vector/array (m1, m2, …, mT) ; sums to one\nα = Concentration parameter α (positive scalar) \n \n α * m = (α1, α2, …., αT)\n \n \nn = base measure for per-topic word distribution; a simplex vector/array (n1, n2, …, nN); sums to one\nβ = Concentration parameter β (positive scalar) \n \n β * n = (β1, β2, …., βT)\n \n \nThere is also another hyper parameter η - topic coherence or perplexity - which can be used to determine the number of topics \n```\n\n### IIID. Now, How does PTW-LDA work? \n\n- Nudge the regular LDA to converge faster and better with human-reviewed words list for each topic\n\n![](images/LDA_vs_KMeans/seededlda_1.png)\n\n- How the topics are seeded with some seed words\n![](images/LDA_vs_KMeans/seededlda_3.png)\nSource:Freecodecamp.org\n<br>\n\n- Chaotic LDA and Clear PTW_LDA outputs... <br>\n- LDA might need several hyperparameter tuning attempts to get to the desired splits\n\n- Default initialization with uniform word topic distribution <br>\n\n![](https://cdn-media-1.freecodecamp.org/images/1*RojQi7m5yBGHSD0Q0HGGYg.png)\n\n- Seeded Initialization\n![](https://cdn-media-1.freecodecamp.org/images/1*kMxQ47DFkpxIDBCyXeff6w.png)\n\n- The seeded words are guided towards seeded topics for converging faster\n![](images/LDA_vs_KMeans/simplistic_guided_LDA_notation.PNG)\n\n### IIID. Pre-processing and Hyper-parameters\n\n**Preprocessing**: <br>\n\n| PTW-LDA                                                                                                                                                                                                     | USE +   Clustering                            |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n| 1. Stop words removed                                                                                                                                                                                       | No   pre-processing; Comments were used as is |\n| 2.   Lemmatized                                                                                                                                                                                             |\n| 3. Top   20 words per (ground truth) label was extracted                                                                                                                                                    |\n| 4.   Human-reviewed list of 20 word lists (each corresponding to 1 topic) were   chosen as prior topic words input (SAT is useful when there are prior topic   words fed; otherwise it works as normal LDA) |\n\n\n**Hyperparameter Tuning**\n\n| PTW-LDA   <br>(best possible based on heuristics and <br>limited # of experimentations)                      | USE +   Clustering           |\n|------------------------------------------------------------------------------------------------------|------------------------------|\n| No. Of Topics                                                                                        | No. Of clusters              |\n| Max Iterations                                                                                       | Max Iterations (for K-Means) |\n| (default) Doc_topic_prior =   alpha = 1/ no_of_topics                                                |\n| (default) topic_word_prior =   beta = 1/ no_of_topics                                                |\n| Learning_method:   \"batch\" (whole dataset is used)                                                   |\n| (alternative is 'online'   which uses only batch size no. Of comments; similar to mini_batch_kmeans) |\n| Seeded_words_list                                                                                    |\n| Seed_coefficent/seed   confidence (how much to nudge the seeded words)                               |\n\n\n## IV. Comparison Study\n\n- Data I - `20 Newsgroups` - Supervised Evaluation <br>\n\n![](images/LDA_vs_KMeans/data1_evaluation_complete.PNG)\n\n[Source for Adjusted Mutual Information Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)\n\n- Data II -`ABC (Australian Broadcast Corporation) Corpus` - Unsupervised Evaluation\n\n|     Metric    |     PTW-LDA    |     USE-Clustering    |\n|-|-|-|\n|     Word Embedding   based Coherence Score <br> (more the coherence score, better is the clustering output)          |     Coherence   Score for 20 topics: 0.091 <br>       <br>    |     Coherence   Score for 20 clusters : 0.159          |\n|     Methodology of   computing the above metric    |                Take the top 10         words that constitute each of the 20 topics          (each topic    comprises of a probability simplex of the words; select the top 10 highly    probable words in that topic)      <br>      <br>      For our case, the    top 10 words used for coherence computation in the 10 topics are:       <br>      <br>      [['police',  'baghdad',     'war',  'probe',  'protest',     'anti',  'missing',  'man',     'fight',  'coalition'],    <br>      ['report',  'pm',     'death',  'korea',  'claim',     'war',  'north',  'nt',     'toll',  'protesters'],    <br>      ['win',  'govt',     'set',  'community',  'end',     'wins',  'vic',  'indigenous',  'road',     'help'], <br>      ['world',  'cup',     'australia',  'found',  'ban',     'plans',  'lead',  'gets',     'expected',  'match'],    <br>      ['un',  'coast',     'title',  'takes',  'peace',     'iraq',  'gold',  'defence',     'residents',  'play'],    <br>      ['iraq',  'iraqi',     'war',  'says',  'troops',     'killed',  'dead',  'hospital',  'clash',     'forces'], <br>      ['council',  'boost',     'mp',  'fire',  'group',     'qld',  'minister',  'defends',     'land',  'welcomes'],    <br>      ['man',  'court',     'charged',  'face',  'plan',     'open',  'murder',  'urged',     'case',  'charges'], <br>      ['new',  'oil',     'dies',  'security',  'crash',     'sars',  'high',  'year',     'house',  'car'], <br>      ['water',  'rain',     'claims',  'wa',  'nsw',     'farmers',  'drought',  'howard',     'centre',  'union']] <br>      <br>                   For each topic,         taking 2 out of the top 10 words at a time,  compute cosine similarity using         pre-trained W2V embedding          <br>            Overall         Coherence is sum of similarity scores of all possible pairs of words         for each topic, normalized by the          no. of topics                  |                Take the top 10         words that constitute each of the 20 clusters          (top 10 words    (from stop-words removed set) are computed based on their TF-IDF weighted    scores in that cluster)      <br>      <br>            For each         cluster, taking 2 out of the top 10 words at a time,  compute similarity using pre-trained         W2V embedding          <br>      <br>      For our case, the    top 10 words used for coherence computation in the 10 clusters are:   <br>      <br>     [['win',  'cup',     'final',  'wins',  'world',     'afl',  'coach',  'england',     'season',  'day'], <br>       ['council',     'plan',  'market',  'funding',     'boost',  'housing',  'water',     'funds',  'budget',  'rise'],     <br>       ['crash',     'man',  'killed',  'death',     'dies',  'dead',  'injured',     'woman',  'car',  'sydney'], <br>       ['interview',  'michael',     'business',  'abc',  'news',     'market',  'analysis',  'david',     'extended',  'andrew'],  <br>       ['australia',  'australian',  'aussie',     'sydney',  'australians',  'day',     'aussies',  'australias',  'melbourne',  'south'], <br>       ['abc',     'country',  'hour',  'news',     'weather',  'grandstand',  'friday',     'nsw',  'drum',  'monday'], <br>       ['govt',     'election',  'council',  'government',  'minister',  'pm',     'parliament',  'nsw',  'anti',     'trump'], <br>       ['police',     'man',  'court',  'murder',     'charged',  'accused',  'death',     'guilty',  'charges',  'assault'], <br>       ['farmers',     'water',  'drought',  'industry',  'farm',     'coal',  'green',  'cattle',     'mining',  'nsw'], <br>       ['health',     'hospital',  'flu',  'mental',     'doctors',  'treatment',  'cancer',     'drug',  'service',  'care']] <br>      <br>      <br>            Overall         Coherence is sum of similarity scores of all possible pairs of words         for each cluster, normalized by the          no. of clusters          <br>       |\n\n## Conclusion\n\n- We have used two News corpus of varying text length. One dataset has ground truth labels and the other doesn't have labels\n\n\n- In our comparison of **PTW-LDA vs USE-Clustering** \n    - using both 'Supervised' (Adjusted Mutual Information Score) and 'Unsupervised' evaluation metrics, USE-clustering performs far superior to PTW-LDA despite repeated attempts at different set of hyper-parameters for PTW-LDA\n\n**Important References**:<br>\n**USE**: <br> \n- https://amitness.com/2020/06/universal-sentence-encoder/ <br>\n- USE Paper: https://arxiv.org/abs/1803.11175    \n\n**LDA**: <br>\n- The original paper of LDA by David Blei \n- https://www.slideshare.net/hustwj/nicolas-loeff-lda\n- http://videolectures.net/mlss09uk_blei_tm/\n- http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf\n- https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution (it compares \nnormal distribution with Dirichlet Distribution)\n- https://ldabook.com/lda-as-a-generative-model.html\n- https://ldabook.com/index.html\n","srcMarkdownNoYaml":"\n\n\n\n## I. Introduction\n\n### What algorithms are covered here?\n\n- **Prior-topic words** guided *Latent Dirichlet Allocation* and \n- **Universal Sentence Encoder** (USE) powered *K-Means*\n\n### What questions am I answering here?\n\n- Given that USE can encode complex semantic information, **is LDA worthwhile?** \n\n- **Can LDA be used for at least some type of data**, where it can produce better/on-par results compared to USE-K-Means? \n\n\nBefore diving into the study, let us understand how **USE-KMeans** and **PTW-guided LDA** works\n\n<hr>\n\n## II. About USE\n\n### IIA. How USE works?\n\n**USE** converts sentences into 512 embeddings\n![](https://www.learnopencv.com/wp-content/uploads/2018/11/Universal-Sentence-Encoder.png)\n\n<hr>\n\n**Semantic Similarity** Correlation Matrix\n![](https://2.bp.blogspot.com/-9Qk1fubLpzg/Wv2QGgKVVmI/AAAAAAAACvs/Gm-XF3prXVIIvaIkrTmkcIcYz-4qSxLKwCLcBGAs/s400/image2.png)\n\n<hr>\n\nDespite many common words, semantically different sentences will have **dissimilar embeddings** \n![](https://1.bp.blogspot.com/-w2kAi39zPrE/Wv2OPHTwDgI/AAAAAAAACvY/aQzvBcaIqYkw8McCBcXlTx0pj9FbILH0ACLcBGAs/s400/image4.png)\n\n### IIB. USE Architecture\n\nThere are two variants\n\n**Variant 1: Transformer Encoder**: <br>\n![](https://amitness.com/images/use-transformer-variant.png)\n\n<small>source: https://amitness.com/2020/06/universal-sentence-encoder/ </small>\n\nWhat does a **Transformer Encoder** Layer comprise?\n- Self Attention Layer \n- Feed Forward Network Layer\n![](https://amitness.com/images/use-transformer-one-layer.png)\n\n**Variant 2: Deep Averaging Network**:<br>\n![](https://amitness.com/images/use-deep-averaging-network-variant.png)\n\n### IIC. Pre-trained Tasks in USE\n\nOverall Pipeline of how **USE** is trained: <br>\n![](https://amitness.com/images/use-overall-pipeline.png)\n\n<hr>\n\nA. Skip-thought prediction: <br>\n- Use `Central Sentence` to predict both the `Previous Sentence` and `Next Sentence`\n\n![](https://amitness.com/images/nlp-ssl-neighbor-sentence.gif)\n\nB. Response Prediction: <br>\n- Train the USE architecture to do `smart reply` prediction\n\n![](https://3.bp.blogspot.com/-qcqYQcxfLS0/Wv2Pxmm945I/AAAAAAAACvk/decC5VtlRGUdD4NqCui3HgNd3LXdjEvlgCLcBGAs/s640/image3.gif)\n\nC. Natural Language Inference: <br>\n- Do `NLI` task prediction, where given a premise and a hypothesis, the model is trained to predict whether the hypothesis is an `entailment`, `contradition` or `neutral` to the premise\n\n![](images/LDA_vs_KMeans/markdown_table1.PNG)\n\n## III. About LDA\n\n- **Latent**: <br>\n    - Topic structures in a document are **latent/hidden** in the text\n\n\n- **Dirichlet**: <br>\n    - The **Dirichlet** distribution determines the mixture proportions of <br> \n            - the topics in the documents and\n            - the words in each topic.\n- **Allocation**: <br>\n    - **Allocation** of words to a given topic and allocation of topics to a document\n\n### IIIA. Intuitive understanding of Dirichlet Distribution\n\n- A **Normal/Gaussian** distribution is a continuous probability distribution over all the real numbers\n    - It is described by a mean and a variance.\n\n![](https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg)\n\n- A **Poisson** distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval \n\n![](https://upload.wikimedia.org/wikipedia/commons/1/16/Poisson_pmf.svg)\n\n> The Poisson distribution is specified by one parameter: lambda (λ). As lambda increases to sufficiently large values, the normal distribution (λ, λ) may be used to approximate the Poisson distribution\n> Source:https://en.wikipedia.org/wiki/Poisson_distribution \n\n<hr>\n\n- Now, what is **Dirichlet Distribution**? <br>\n    - The dirichlet distribution is a probability distribution as well\n    - but it is not sampling from the space of real numbers. Instead it is sampling over a probability simplex <br>\n\n```\n (0.6, 0.4)\n (0.1, 0.1, 0.8)\n (0.05, 0.2, 0.15, 0.1, 0.3, 0.2)\n```\n\n![](images/LDA_vs_KMeans/dirichlet_distribution_of_words_topics.PNG)\n\n- How Dirichlet Distribution varies w.r.t dirichlet prior: <br>\n    - The below image shows Dir(α) <br>\n    - As α increases from 0.05 (1/20) to 0.1, 0.2, 0.4 respectively in plots from left to right & top to down, you can see the distribution becoming more uniform. <br>\n![](images/LDA_vs_KMeans/dirichlet_distribution_with_prior.png)    \n\n### IIIB. How LDA Works?\n\n- LDA is a generative model <br>\n- LDA processes documents as 'bag of words' -- ordering of words is not important\n\n![](images/LDA_vs_KMeans/generative_process_1.png)\n\nIn principle, LDA generates a document based on **dirichlet distribution (dd) of topics over documents** and **dd of words over topics**\n\n![](images/LDA_vs_KMeans/generative_process_2.png)\n\n<br>\n<br>\n\nBut we inverse the generative process for statistical inference \n\n![](images/LDA_vs_KMeans/generative_process_3.png)\n\n### IIIC. Hyperparameters of LDA\n\n![](LDA_vs_KMeans/plate_notation_LDA.png)\n\nD = Total No. of Documents <br>\nN = No. of Words = Vocab Size <br>\nT = No. of Topics <br>\n<br>\nθd = Topic Distribution for a particular document d\n\n![](images/LDA_vs_KMeans/plate_notation_2.png)\n\n<br>\nΦt= Word Distribution for a topic t. Here for topic 1 and 2. \n\n![](images/LDA_vs_KMeans/plate_notation_3.png)\n\n(colored books represent words/tokens)\n\n```\nZd,n = Topic Distribution for n th word in document d\nWd,n = nth word in dth document\n```\n\n```\nα= parameter that sets the dircihlet prior on the per-document topic distribution (θ)\n= parameter that represents the doc-topic density\n= determines the no. of topics in each doc\n= (Default) = 1/num_of_topics (in sklearn and gensim)\ndecreasing alpha results in less number of topics per document\n \nβ= parameter that sets the dirichlet prior on the per-topic word distribution(ϕ)\n= parameter that represents the topic-word density\n= determines the no. of words per each topic\n= (Default) = 1/num_of_topics (in sklearn and gensim)\ndecreasing beta results in just a few major words in topics\n\nm = base measure for per-document topic distribution; a simplex vector/array (m1, m2, …, mT) ; sums to one\nα = Concentration parameter α (positive scalar) \n \n α * m = (α1, α2, …., αT)\n \n \nn = base measure for per-topic word distribution; a simplex vector/array (n1, n2, …, nN); sums to one\nβ = Concentration parameter β (positive scalar) \n \n β * n = (β1, β2, …., βT)\n \n \nThere is also another hyper parameter η - topic coherence or perplexity - which can be used to determine the number of topics \n```\n\n### IIID. Now, How does PTW-LDA work? \n\n- Nudge the regular LDA to converge faster and better with human-reviewed words list for each topic\n\n![](images/LDA_vs_KMeans/seededlda_1.png)\n\n- How the topics are seeded with some seed words\n![](images/LDA_vs_KMeans/seededlda_3.png)\nSource:Freecodecamp.org\n<br>\n\n- Chaotic LDA and Clear PTW_LDA outputs... <br>\n- LDA might need several hyperparameter tuning attempts to get to the desired splits\n\n- Default initialization with uniform word topic distribution <br>\n\n![](https://cdn-media-1.freecodecamp.org/images/1*RojQi7m5yBGHSD0Q0HGGYg.png)\n\n- Seeded Initialization\n![](https://cdn-media-1.freecodecamp.org/images/1*kMxQ47DFkpxIDBCyXeff6w.png)\n\n- The seeded words are guided towards seeded topics for converging faster\n![](images/LDA_vs_KMeans/simplistic_guided_LDA_notation.PNG)\n\n### IIID. Pre-processing and Hyper-parameters\n\n**Preprocessing**: <br>\n\n| PTW-LDA                                                                                                                                                                                                     | USE +   Clustering                            |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n| 1. Stop words removed                                                                                                                                                                                       | No   pre-processing; Comments were used as is |\n| 2.   Lemmatized                                                                                                                                                                                             |\n| 3. Top   20 words per (ground truth) label was extracted                                                                                                                                                    |\n| 4.   Human-reviewed list of 20 word lists (each corresponding to 1 topic) were   chosen as prior topic words input (SAT is useful when there are prior topic   words fed; otherwise it works as normal LDA) |\n\n\n**Hyperparameter Tuning**\n\n| PTW-LDA   <br>(best possible based on heuristics and <br>limited # of experimentations)                      | USE +   Clustering           |\n|------------------------------------------------------------------------------------------------------|------------------------------|\n| No. Of Topics                                                                                        | No. Of clusters              |\n| Max Iterations                                                                                       | Max Iterations (for K-Means) |\n| (default) Doc_topic_prior =   alpha = 1/ no_of_topics                                                |\n| (default) topic_word_prior =   beta = 1/ no_of_topics                                                |\n| Learning_method:   \"batch\" (whole dataset is used)                                                   |\n| (alternative is 'online'   which uses only batch size no. Of comments; similar to mini_batch_kmeans) |\n| Seeded_words_list                                                                                    |\n| Seed_coefficent/seed   confidence (how much to nudge the seeded words)                               |\n\n\n## IV. Comparison Study\n\n- Data I - `20 Newsgroups` - Supervised Evaluation <br>\n\n![](images/LDA_vs_KMeans/data1_evaluation_complete.PNG)\n\n[Source for Adjusted Mutual Information Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)\n\n- Data II -`ABC (Australian Broadcast Corporation) Corpus` - Unsupervised Evaluation\n\n|     Metric    |     PTW-LDA    |     USE-Clustering    |\n|-|-|-|\n|     Word Embedding   based Coherence Score <br> (more the coherence score, better is the clustering output)          |     Coherence   Score for 20 topics: 0.091 <br>       <br>    |     Coherence   Score for 20 clusters : 0.159          |\n|     Methodology of   computing the above metric    |                Take the top 10         words that constitute each of the 20 topics          (each topic    comprises of a probability simplex of the words; select the top 10 highly    probable words in that topic)      <br>      <br>      For our case, the    top 10 words used for coherence computation in the 10 topics are:       <br>      <br>      [['police',  'baghdad',     'war',  'probe',  'protest',     'anti',  'missing',  'man',     'fight',  'coalition'],    <br>      ['report',  'pm',     'death',  'korea',  'claim',     'war',  'north',  'nt',     'toll',  'protesters'],    <br>      ['win',  'govt',     'set',  'community',  'end',     'wins',  'vic',  'indigenous',  'road',     'help'], <br>      ['world',  'cup',     'australia',  'found',  'ban',     'plans',  'lead',  'gets',     'expected',  'match'],    <br>      ['un',  'coast',     'title',  'takes',  'peace',     'iraq',  'gold',  'defence',     'residents',  'play'],    <br>      ['iraq',  'iraqi',     'war',  'says',  'troops',     'killed',  'dead',  'hospital',  'clash',     'forces'], <br>      ['council',  'boost',     'mp',  'fire',  'group',     'qld',  'minister',  'defends',     'land',  'welcomes'],    <br>      ['man',  'court',     'charged',  'face',  'plan',     'open',  'murder',  'urged',     'case',  'charges'], <br>      ['new',  'oil',     'dies',  'security',  'crash',     'sars',  'high',  'year',     'house',  'car'], <br>      ['water',  'rain',     'claims',  'wa',  'nsw',     'farmers',  'drought',  'howard',     'centre',  'union']] <br>      <br>                   For each topic,         taking 2 out of the top 10 words at a time,  compute cosine similarity using         pre-trained W2V embedding          <br>            Overall         Coherence is sum of similarity scores of all possible pairs of words         for each topic, normalized by the          no. of topics                  |                Take the top 10         words that constitute each of the 20 clusters          (top 10 words    (from stop-words removed set) are computed based on their TF-IDF weighted    scores in that cluster)      <br>      <br>            For each         cluster, taking 2 out of the top 10 words at a time,  compute similarity using pre-trained         W2V embedding          <br>      <br>      For our case, the    top 10 words used for coherence computation in the 10 clusters are:   <br>      <br>     [['win',  'cup',     'final',  'wins',  'world',     'afl',  'coach',  'england',     'season',  'day'], <br>       ['council',     'plan',  'market',  'funding',     'boost',  'housing',  'water',     'funds',  'budget',  'rise'],     <br>       ['crash',     'man',  'killed',  'death',     'dies',  'dead',  'injured',     'woman',  'car',  'sydney'], <br>       ['interview',  'michael',     'business',  'abc',  'news',     'market',  'analysis',  'david',     'extended',  'andrew'],  <br>       ['australia',  'australian',  'aussie',     'sydney',  'australians',  'day',     'aussies',  'australias',  'melbourne',  'south'], <br>       ['abc',     'country',  'hour',  'news',     'weather',  'grandstand',  'friday',     'nsw',  'drum',  'monday'], <br>       ['govt',     'election',  'council',  'government',  'minister',  'pm',     'parliament',  'nsw',  'anti',     'trump'], <br>       ['police',     'man',  'court',  'murder',     'charged',  'accused',  'death',     'guilty',  'charges',  'assault'], <br>       ['farmers',     'water',  'drought',  'industry',  'farm',     'coal',  'green',  'cattle',     'mining',  'nsw'], <br>       ['health',     'hospital',  'flu',  'mental',     'doctors',  'treatment',  'cancer',     'drug',  'service',  'care']] <br>      <br>      <br>            Overall         Coherence is sum of similarity scores of all possible pairs of words         for each cluster, normalized by the          no. of clusters          <br>       |\n\n## Conclusion\n\n- We have used two News corpus of varying text length. One dataset has ground truth labels and the other doesn't have labels\n\n\n- In our comparison of **PTW-LDA vs USE-Clustering** \n    - using both 'Supervised' (Adjusted Mutual Information Score) and 'Unsupervised' evaluation metrics, USE-clustering performs far superior to PTW-LDA despite repeated attempts at different set of hyper-parameters for PTW-LDA\n\n**Important References**:<br>\n**USE**: <br> \n- https://amitness.com/2020/06/universal-sentence-encoder/ <br>\n- USE Paper: https://arxiv.org/abs/1803.11175    \n\n**LDA**: <br>\n- The original paper of LDA by David Blei \n- https://www.slideshare.net/hustwj/nicolas-loeff-lda\n- http://videolectures.net/mlss09uk_blei_tm/\n- http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf\n- https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution (it compares \nnormal distribution with Dirichlet Distribution)\n- https://ldabook.com/lda-as-a-generative-model.html\n- https://ldabook.com/index.html\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2020-09-17-ptw_lda_vs_use_k_means.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"aliases":["/Topic_Modeling/LDA/Seeded_LDA/Universal_Sentence_Encoder/KMeans/2020/09/17/ptw_lda_vs_USE_K_Means"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["ML","NLP"],"date":"2020-09-17","description":"A Short Study comparing PTW_LDA and Transfer Learning powered KMeans on Text Data","hide":false,"image":"images/LDA_vs_KMeans/data1_evaluation_complete.PNG","title":"Comparing Two Unsupervised Clustering Algorithms for Text Data"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}