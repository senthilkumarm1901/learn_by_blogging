{"title":"The Theory of Word2Vec Algorithm","markdown":{"yaml":{"aliases":["/Word2Vec/2020/05/03/Pretrained_Word_Embeddings"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP"],"date":"2020-05-03","description":"This blog post attempts to explain one of the seminal algorithms in NLP for embedding creation","hide":false,"image":"images/word_embeddings/cbow_vs_skip_gram.png","output-file":"2020-05-03-pretrained_word_embeddings.html","title":"The Theory of Word2Vec Algorithm","toc":true},"headingText":"1. The Precursor Language Models to Word2Vec","containsRefs":false,"markdown":"\n\n\n\n\n### 1A. The Counting based Bigram Language Model \n\n**Sample Corpus**: <br>\n> This is **the house** that Jack built.<br>\n> This is **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** cat,<br>\n> That killed **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n\nWhat is the probability of **house** occurring given **the** before it?\n\nIntuitively, we count,\n\n$$ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } $$\n\nMathematically, the above formula can be writen as $$ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } $$\n\nWhat we have computed above is a **Bigram Language Model**:\n\n$$ Bigram \\ Model : { p ( W_t|W_{t-1} ) } $$\n\nIf you want to read more about Count-based Language Models, refer [here](https://senthilkumarm1901.github.io/learn_by_blogging/markov_langugage_model/log_probability/perplexity/2020/03/17/Introduction_to_Statistical_Language_Modeling.html)\n\n<hr>\n\n### 1B. Evolution of Word2Vec from Context Predicting based Language Models\n\n#### How Word2Vec Evolved from Logistic Bigram Language Model\n\n![](images/transfer_learning_evolution/context_prediction_based_language_models.png)\n\n<hr>\n\n#### A. Simpler (Linear) Logistic Bigram Model\n\n![](images/word_embeddings/simpler_LR_bigram_model.png)\n\n**Forward Propagation**:\n- If x is the current word vector and y is the vector of next word, <br>\n    - Prob of y being the next word given the current word is  is given by \n    $$ y' = p(y \\ | x) \\ = \\ softmax(W^Tx + B) $$\n    \n    $$ y' = p(y=j |x) \\ = {{e^{(w_j^T+ b_j)}} \\over \\sum \\limits_{k=1}^K {e^{(w_k^T x + b_k)}}} $$\n    $$ Where\\ j\\ is\\ one\\ of\\ the\\ output\\ words\\ from\\ a\\ vocabulary\\ size\\ of\\ K\\ words $$\n    \n**Cost Function Equation**:\n- How do we optimize W by finding gradient descent on the cost function $J$ $ \\delta J $ \n\n- The cost function in our case is\n\n$$ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {y'_{k1,k2}}} $$\n\n$$ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {p(y_{k1,k2} | x_{k1})}} $$\n(since number of output class is same as the number of input words in a bigram language model)\n\n**Computing Gradient Descent on the Cost Function**:\n- Step 1: **Initalizing parameters to learn with Random Weights**:  Initialize W to random weights of size K x K (where K is the vocab size) and B to random weights of size K x 1\n- Step 2: **Compute gradient** (a.k.a partial derivative) on $ J $ w.r.t W and B \n- Step 3: **Perform Gradient Descent** Update W based on $ \\delta J $ as \n    - $ W = W - \\eta \\ \\delta_W J $ \n    - $ B = B - \\eta \\ \\delta_B J $\n    where $\\eta$ is the learning rate\n- Repeat steps 1, 2, and 3 for an entire round of data, then one `epoch` of training is over. For every minibatch there is only a small shift in the weights. The size of the shifting of weights is determined by learning_rate parameter\n\n**Model Training Process**:\n- While loss_curve not stagnating or `X` number of epochs are not over:\n    - Repeat steps 1, 2, and 3 \n- Final `W` and `B` weights are used for model inferencing in the $ y' = p(y \\ | x) \\ = \\ softmax(W^Tx + B) $\n\n<hr>\n\n#### B. Feed Forward Neural Network Bigram Model\n\nA Typical Logistic Regression based Model: <br>\n    $$ p(y|x) = softmax(W^T\\ x) $$\n    \nA Neural Network based Model: <br>\n    $$ h = tanh(W_1^T\\ x)$$ \n    $$ p(y|x) = softmax(W_2^T\\ h) $$\n    \nA Neural Network Bigram Model: <br>\n    $$ p(x_{t+1} | x_t) = softmax(W_2^T tanh(W_1^T x_t)) $$\n    <br>\n    $$ Where\\ x_t\\ and\\ x_{t+1} are words at positions t and t+1 $$\n    <br>\n    $$ Where\\ W_1\\ is\\ of\\ dimension\\ K\\ X\\ D\\ and\\ W_2\\ is\\ of\\ dimension\\ D\\ X\\ K\\ $$\n    <br>\n    $$ Where\\ K\\ is\\ the\\ vocabulary\\ size$$\n    $$ Where D\\ is\\ the\\ number\\ of\\ neurons\\ in\\ the\\ hidden\\ layer\\$$\n    \nThe Cost Function and Model Training are similar to **Logistic Bigram Model**\n\n<hr>\n\n## 2. Theory of Word2Vec\n\n### 2A. What are the two types of Word2Vec Models?\n\n##### 1. Continuous Bag of Words:\n- Predict focus word given bag-of-words context\n    - E.g. The quick brown fox jumps over the lazy dog\n        - current_slide_position = 4\n        - focus_word = sentence.split()[4] = \"jumps\"\n        - context_window_size = 2 (one of the hyperparameters)\n        - context_words = sentence.split()[4-2:4] + sentence.split()[4:4+2] = [brown, fox, over, the] \n \n![](images/word_embeddings/continuous_bow.png)\n\n- Training Data: p(jumps | brown), p(jumps | fox), p(jumps | over), p(jumps|the)\n\n##### How CBOW works?\n\n- Context size is one of the hyper paramaters of Word2Vec\n- Usual size: 5-10 words. We can widen the context words by dropping stop words like \"the\"\n\n**Forward Propagation**: <br>\nStep 1: Find the mean of the input word vectors\n- Compute the dot product of  Weight matrix $W_1$ with each of the context word vector $c$ and then average the resulting word embeddings\n- Since we compute mean, the order of the context words does not influence the prediction (hence the bag-of-words name)\n\n$$ h = {{1 \\over \\vert{C}\\vert} {\\sum \\limits_{c \\epsilon C} W_1^T c}}$$\n$$ Where\\ h\\ is\\ the\\ mean\\ of\\ the\\ input\\ context\\ word\\ embeddings $$\n\n$$ Where\\ C\\ is\\ the\\ no.\\ of\\ Context\\ Words $$\n\n$$ Where\\ c\\ is\\ the\\ one-hot\\ encoded\\ vector\\ of\\ every\\ input\\ context\\ word$$\n<br>\n\nStep 2: Get the output prediction\n- Multiply the mean word embeding $h$ by the second weight matrix $W_2$ and then do `softmax` on the product \n- The above operations gives the predicted probability of output word given all the context words\n\n$$ p(y | C) = softmax(W_2^T h) $$\n\n**Key learnings from the above CBoW Equations**:\n- Word2Vec is a linear model (there is no activation function like tanh)\n- To be specific, Word2Vec is a log-linear model\n- Since Word2Vec is linear, the training is faster than a typical NN because backpropagation through linear layer is faster than through a non-linear layer\n      \n<hr>\n\n##### 2. Skipgram:\n- Predict context words given the focus word\n\n    - E.g. The quick brown fox jumps over the lazy dog\n        - current_slide_position = 4\n        - focus_word = sentence.split()[4] = \"jumps\"\n        - context_window_size = 2 (one of the hyperparameters)\n        - context_words = sentence.split()[4-2:4] + sentence.split()[4:4+2] = [brown, fox, over, the] \n\n**Skip Gram is a like a bigram where we skip a few word positions**\n![](word_embeddings/skip_gram.png)\nImage Source: NLP Udemy Course by Lazy Programmer  \n\n- Training Data: p(brown | jumps), p(fox|jumps), p(over|jumps), p(the|jumps)\n\n##### How Skip Gram works?\n\n**Forward Propagation:**\n    \n$$ h = W_1^T\\ input\\_focus\\_word $$\n\n$$ p(y|input\\_focus\\_word) = softmax(W_2^T\\ h) $$\n\n*Where input_focus_word = one-hot encoding of the input focus word*\n\n<hr>\n\n### 2B. How is Word2Vec training optimized (objective function and optimization methods)?\n\n\n#### Why ordinary softmax could be problematic?\nK = Vocab Size = No. of Output Classes\n\nSuppose a large dataset has 100,000 unique tokens as K\n- Large number of output classes - affect accuracy\n- P(output_word|focus_word) = 1/ 100,000 = 99.99999% chance of failure\n- Order of complexity of softmax calculation is $O(K)$\n\n$$ p(y=j |x) \\ = {{e^{(w_j^T+ b_j)}} \\over \\sum \\limits_{k=1}^K {e^{(w_k^T x + b_k)}}} $$\n\n<hr>\n\n#### Hierarchical Softmax\n\n- An approximation to the normal softmax.\n- It uses a binary tree method to find the probability of a word being the output word\n- Reduces the number of classes in the denominator\n- Reduces the complexity from $O(K)$ to $O(\\log_2 K)$\n\n![](images/word_embeddings/hierarchical_softmax_binary_tree.png)\n\nSource: [Oreilly Link](https://www.oreilly.com/library/view/python-natural-language/9781787121423/a63263c0-bd79-4c15-88d0-8898186e03a5.xhtml)\n\n**Key points**: \n\n\nThe error is propagated backwards to only those nodes that are activated at the time of prediction \n\n- To train the model, our goal is still to minimize the negative log Likelihood (since this also a softmax). \n- But instead of updating all output vectors per word, we update the vectors of the nodes in the binary tree that are in the path from root to leaf node.\n- \"Huffman coding\" is used to construct this binary tree. \n    - Frequent words are closer to the top\n    - Infrequent words are closer to the bottom\n\n<hr>\n\n#### Negative Sampling\n\n- The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. \n\n- That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression is used instead\n\n**Usual Softmax**\n\n![](images/word_embeddings/usual_softmax.png)\n\n**Negative Sampling Softmax**\n\n![](images/word_embeddings/negative_sampling_softmax.png)\n\nFor each training sample, the classifier is fed \n- a true pair (a center word and another word that appears in its context) and \n- a number of k randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary)\n\nBy learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.\n\n- This is important: instead of predicting the next word (the \"standard\" training technique), the optimized classifier simply predicts whether a pair of words is good or bad (which makes this a binary classification problem)\n\nA model is trained to predict whether the output word is a correct or corrupt one. ?\n<br>\n- Hyperparameter in Word2Vec w.r.t Negative Sampling: Number of Negative samples\n     - Original Word2Vec paper suggests using 5-20 negative sampling words for smaller datasets, and 2-5 for large datasets \n\nSource: Advanced NLP Udemy Course by Lazy Programmer\n\n<hr>\n\n### 2C. How Word2vec was implemented?\n\n**1. There is a twist in the way negative sampling logic was implemented**\n\n**Expected Method** <br> \nEx. \"The quick brown fox jumps over the lazy dog.\" <br>\n- Word2Vec Model Type: Skipgram\n- Focus Word (input): jumps\n- Context Words (correct output/positive samples): brown, fox, over, the\n- Negative Samples (corrupted outputs/ negative samples): apple, orange, london, ship\n\n**Actual Method** <br>\n\nEx. \"The quick brown fox jumps over the lazy dog.\" <br>\nNegative sentence = \"The quick brown fox lighthouse over the lazy dog.\"\n- Word2Vec Model Type: Skipgram\n- Focus Word (positive input): jumps\n- Negative input: lighthouse\n- Context Words (positive samples): Predicting `brown`, `fox`, `over` or `the` given `jumps`\n- Context Words (corrupted outputs or negative samples): Predicting `brown`, `fox`, `over` or `the` given `lighthouse`\n\n<hr>\n\n**2. Dropping high frequency words increases context window**\n\nEx. \"The quick brown fox jumps over the lazy dog.\" <br>\nNew sentence = \"quick brown fox jumps lazy dog.\"\n\n<hr>\n\n**3. Decaying Learning Rate**\n- Decay the learning rate from max to min\n\n<hr>\n\n**4. Typical Hyper Parameters used for Word2Vec model Training**\n- no_of_epochs = 20\n- context_window_size = 5\n- starting_learning_rate = 0.025 \n- final_learning_rate = 0.0001\n- (automatic calculation of learning rate decay using above two values)\n- Hidden layer dimension size = Word2Vec Embedding Size = 300\n\n<hr>\n\n## 3. Applications of Word2Vec\n\n- Word2Vec is used to **measure similarity** between 2 documents\n- Pre-trained Word2Vec or Custom-trained Word2Vec bettered **Text Classification** results (compared to ones with bag-of-words apporach)\n- Word2Vec as a way to augment text data | [link](https://www.kaggle.com/code/theoviel/using-word-embeddings-for-data-augmentation/notebook)\n- Word2Vec Coherence Score Computation for Topic Modeling\n\n```python\noverall_coherence_score = 0\nFor each_topic in topics:\n  Coherence_score_for_this_topic = 0\n  do(Each of the 20 topics consists of  probability simplex of words)\n  do(Select the top 10 highly problems in each topic)\n  do(Take 2 out of the top 10 words):\n    Coherence_score_for_this_topic = Coherence_score_for_this_topic + \n                                      pre-trained_W2V_embedding_based_similarity(word1,word2)\n  overall_coherence_score = Overall_coherence_score + Coherence_score_for_this_topic\n```\n\n<hr>\n\n## 4. Conclusion\n\n- Your choice of Word2Vec model depends on the below rationale\n    - **Skip-gram**: Works well with small amount of the training data, represents well even rare words or phrases.\n    - **CBOW**: Several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n\n<hr>\n\n**What made `Word2Vec` popular?**\n- `Word2Vec` was the first scalable model that generated word embeddings for large corpus (millions of unique words). \n- One of the first useful Transfer Learning embedding that improved accuracy of Text Classification (compared to bag of words approaches)\n\n**What are the competing pre-trained embeddings?** \n- `GloVe` - a Word Embedding algorithm trained using word co-occurrences - achieved similar results as Word2Vec but with less training time. However, memory foot-print needd to store co-occurrence matrix made it disadvantageous for large corpuses\n- Both `Word2Vec` and `GloVe` did not handle **out of vocabulary (OOV)** cases. All OOV words were given the same embedding. \n- `FastText` - an algorithm very similar to `Word2Vec` but at character level - solved the OOV issue to some extent. This is possible because FastText aggregates embeddings of individual ngrams of the word. There are more chances of presence of ngrams of a word than the whole word. \n\n<hr>\n\n**The need for more contextualized embeddings:**\n- Polysemy is not captured. E.g.: \"cell\" could be \"biological cell\", \"prison cell\" or \"phone\"\n- All above 3 algorithms sufferred with respect to polysemy. The contextualized embeddings from ELMo, ULMFiT and transformer models since BERT solved the polysemy problem better\n      \n<hr>\n\n## 5. References and Useful Links\n\n- Advanced NLP and Deep Learning Course in Udemy by Lazy Programmer | [link](https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/)\n- Mathjax Cheatsheet | [link](https://jojozhuang.github.io/tutorial/mathjax-cheat-sheet-for-mathematical-notation/)\n- Efficient Estimation of Word Representations in Vector Space | [Paper](https://arxiv.org/pdf/1301.3781.pdf)\n\nGlove (though not discussed here, in case you are interested to read theory of Glove): \n- http://www.foldl.me/2014/glove-python/\n- http://nbviewer.jupyter.org/github/hans/glove.py/blob/master/demo/glove.py%20exploration.ipynb\n- https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8\n- Good glove numpy implementation (implemented using adaptive gradient descent method): https://github.com/hans/glove.py\n\thttps://github.com/hans/glove.py/blob/master/glove.py\n\thttp://nbviewer.jupyter.org/github/hans/glove.py/blob/master/demo/glove.py%20exploration.ipynb\n- Glove implementation using Alternating Least Squares method:\n\thttps://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/glove.py\n- https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970\n- https://towardsdatascience.com/emnlp-what-is-glove-part-ii-9e5ad227ee0\n\nWord2Vec: \n- https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281 (part 1)\n- http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ (part 2)\n- https://machinelearningmastery.com/develop-word-embeddings-python-gensim/ (practical)\n\nW2V implementations in Python\n- https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/\n- https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/\n- https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py\n- https://www.tensorflow.org/tutorials/representation/word2vec\n\thttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n\thttps://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\n\nComparison: \n- https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n- https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/\n- https://github.com/parulsethi/gensim/blob/wordrank_wrapper/docs/notebooks/Wordrank_comparisons.ipynb (use brown corpus from nltk easy to download and compare)\n\n","srcMarkdownNoYaml":"\n\n\n\n## 1. The Precursor Language Models to Word2Vec\n\n### 1A. The Counting based Bigram Language Model \n\n**Sample Corpus**: <br>\n> This is **the house** that Jack built.<br>\n> This is **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n> This is **the** cat,<br>\n> That killed **the** rat,<br>\n> That ate **the** malt<br>\n> That lay in **the house** that Jack built.<br>\n\nWhat is the probability of **house** occurring given **the** before it?\n\nIntuitively, we count,\n\n$$ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } $$\n\nMathematically, the above formula can be writen as $$ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } $$\n\nWhat we have computed above is a **Bigram Language Model**:\n\n$$ Bigram \\ Model : { p ( W_t|W_{t-1} ) } $$\n\nIf you want to read more about Count-based Language Models, refer [here](https://senthilkumarm1901.github.io/learn_by_blogging/markov_langugage_model/log_probability/perplexity/2020/03/17/Introduction_to_Statistical_Language_Modeling.html)\n\n<hr>\n\n### 1B. Evolution of Word2Vec from Context Predicting based Language Models\n\n#### How Word2Vec Evolved from Logistic Bigram Language Model\n\n![](images/transfer_learning_evolution/context_prediction_based_language_models.png)\n\n<hr>\n\n#### A. Simpler (Linear) Logistic Bigram Model\n\n![](images/word_embeddings/simpler_LR_bigram_model.png)\n\n**Forward Propagation**:\n- If x is the current word vector and y is the vector of next word, <br>\n    - Prob of y being the next word given the current word is  is given by \n    $$ y' = p(y \\ | x) \\ = \\ softmax(W^Tx + B) $$\n    \n    $$ y' = p(y=j |x) \\ = {{e^{(w_j^T+ b_j)}} \\over \\sum \\limits_{k=1}^K {e^{(w_k^T x + b_k)}}} $$\n    $$ Where\\ j\\ is\\ one\\ of\\ the\\ output\\ words\\ from\\ a\\ vocabulary\\ size\\ of\\ K\\ words $$\n    \n**Cost Function Equation**:\n- How do we optimize W by finding gradient descent on the cost function $J$ $ \\delta J $ \n\n- The cost function in our case is\n\n$$ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {y'_{k1,k2}}} $$\n\n$$ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {p(y_{k1,k2} | x_{k1})}} $$\n(since number of output class is same as the number of input words in a bigram language model)\n\n**Computing Gradient Descent on the Cost Function**:\n- Step 1: **Initalizing parameters to learn with Random Weights**:  Initialize W to random weights of size K x K (where K is the vocab size) and B to random weights of size K x 1\n- Step 2: **Compute gradient** (a.k.a partial derivative) on $ J $ w.r.t W and B \n- Step 3: **Perform Gradient Descent** Update W based on $ \\delta J $ as \n    - $ W = W - \\eta \\ \\delta_W J $ \n    - $ B = B - \\eta \\ \\delta_B J $\n    where $\\eta$ is the learning rate\n- Repeat steps 1, 2, and 3 for an entire round of data, then one `epoch` of training is over. For every minibatch there is only a small shift in the weights. The size of the shifting of weights is determined by learning_rate parameter\n\n**Model Training Process**:\n- While loss_curve not stagnating or `X` number of epochs are not over:\n    - Repeat steps 1, 2, and 3 \n- Final `W` and `B` weights are used for model inferencing in the $ y' = p(y \\ | x) \\ = \\ softmax(W^Tx + B) $\n\n<hr>\n\n#### B. Feed Forward Neural Network Bigram Model\n\nA Typical Logistic Regression based Model: <br>\n    $$ p(y|x) = softmax(W^T\\ x) $$\n    \nA Neural Network based Model: <br>\n    $$ h = tanh(W_1^T\\ x)$$ \n    $$ p(y|x) = softmax(W_2^T\\ h) $$\n    \nA Neural Network Bigram Model: <br>\n    $$ p(x_{t+1} | x_t) = softmax(W_2^T tanh(W_1^T x_t)) $$\n    <br>\n    $$ Where\\ x_t\\ and\\ x_{t+1} are words at positions t and t+1 $$\n    <br>\n    $$ Where\\ W_1\\ is\\ of\\ dimension\\ K\\ X\\ D\\ and\\ W_2\\ is\\ of\\ dimension\\ D\\ X\\ K\\ $$\n    <br>\n    $$ Where\\ K\\ is\\ the\\ vocabulary\\ size$$\n    $$ Where D\\ is\\ the\\ number\\ of\\ neurons\\ in\\ the\\ hidden\\ layer\\$$\n    \nThe Cost Function and Model Training are similar to **Logistic Bigram Model**\n\n<hr>\n\n## 2. Theory of Word2Vec\n\n### 2A. What are the two types of Word2Vec Models?\n\n##### 1. Continuous Bag of Words:\n- Predict focus word given bag-of-words context\n    - E.g. The quick brown fox jumps over the lazy dog\n        - current_slide_position = 4\n        - focus_word = sentence.split()[4] = \"jumps\"\n        - context_window_size = 2 (one of the hyperparameters)\n        - context_words = sentence.split()[4-2:4] + sentence.split()[4:4+2] = [brown, fox, over, the] \n \n![](images/word_embeddings/continuous_bow.png)\n\n- Training Data: p(jumps | brown), p(jumps | fox), p(jumps | over), p(jumps|the)\n\n##### How CBOW works?\n\n- Context size is one of the hyper paramaters of Word2Vec\n- Usual size: 5-10 words. We can widen the context words by dropping stop words like \"the\"\n\n**Forward Propagation**: <br>\nStep 1: Find the mean of the input word vectors\n- Compute the dot product of  Weight matrix $W_1$ with each of the context word vector $c$ and then average the resulting word embeddings\n- Since we compute mean, the order of the context words does not influence the prediction (hence the bag-of-words name)\n\n$$ h = {{1 \\over \\vert{C}\\vert} {\\sum \\limits_{c \\epsilon C} W_1^T c}}$$\n$$ Where\\ h\\ is\\ the\\ mean\\ of\\ the\\ input\\ context\\ word\\ embeddings $$\n\n$$ Where\\ C\\ is\\ the\\ no.\\ of\\ Context\\ Words $$\n\n$$ Where\\ c\\ is\\ the\\ one-hot\\ encoded\\ vector\\ of\\ every\\ input\\ context\\ word$$\n<br>\n\nStep 2: Get the output prediction\n- Multiply the mean word embeding $h$ by the second weight matrix $W_2$ and then do `softmax` on the product \n- The above operations gives the predicted probability of output word given all the context words\n\n$$ p(y | C) = softmax(W_2^T h) $$\n\n**Key learnings from the above CBoW Equations**:\n- Word2Vec is a linear model (there is no activation function like tanh)\n- To be specific, Word2Vec is a log-linear model\n- Since Word2Vec is linear, the training is faster than a typical NN because backpropagation through linear layer is faster than through a non-linear layer\n      \n<hr>\n\n##### 2. Skipgram:\n- Predict context words given the focus word\n\n    - E.g. The quick brown fox jumps over the lazy dog\n        - current_slide_position = 4\n        - focus_word = sentence.split()[4] = \"jumps\"\n        - context_window_size = 2 (one of the hyperparameters)\n        - context_words = sentence.split()[4-2:4] + sentence.split()[4:4+2] = [brown, fox, over, the] \n\n**Skip Gram is a like a bigram where we skip a few word positions**\n![](word_embeddings/skip_gram.png)\nImage Source: NLP Udemy Course by Lazy Programmer  \n\n- Training Data: p(brown | jumps), p(fox|jumps), p(over|jumps), p(the|jumps)\n\n##### How Skip Gram works?\n\n**Forward Propagation:**\n    \n$$ h = W_1^T\\ input\\_focus\\_word $$\n\n$$ p(y|input\\_focus\\_word) = softmax(W_2^T\\ h) $$\n\n*Where input_focus_word = one-hot encoding of the input focus word*\n\n<hr>\n\n### 2B. How is Word2Vec training optimized (objective function and optimization methods)?\n\n\n#### Why ordinary softmax could be problematic?\nK = Vocab Size = No. of Output Classes\n\nSuppose a large dataset has 100,000 unique tokens as K\n- Large number of output classes - affect accuracy\n- P(output_word|focus_word) = 1/ 100,000 = 99.99999% chance of failure\n- Order of complexity of softmax calculation is $O(K)$\n\n$$ p(y=j |x) \\ = {{e^{(w_j^T+ b_j)}} \\over \\sum \\limits_{k=1}^K {e^{(w_k^T x + b_k)}}} $$\n\n<hr>\n\n#### Hierarchical Softmax\n\n- An approximation to the normal softmax.\n- It uses a binary tree method to find the probability of a word being the output word\n- Reduces the number of classes in the denominator\n- Reduces the complexity from $O(K)$ to $O(\\log_2 K)$\n\n![](images/word_embeddings/hierarchical_softmax_binary_tree.png)\n\nSource: [Oreilly Link](https://www.oreilly.com/library/view/python-natural-language/9781787121423/a63263c0-bd79-4c15-88d0-8898186e03a5.xhtml)\n\n**Key points**: \n\n\nThe error is propagated backwards to only those nodes that are activated at the time of prediction \n\n- To train the model, our goal is still to minimize the negative log Likelihood (since this also a softmax). \n- But instead of updating all output vectors per word, we update the vectors of the nodes in the binary tree that are in the path from root to leaf node.\n- \"Huffman coding\" is used to construct this binary tree. \n    - Frequent words are closer to the top\n    - Infrequent words are closer to the bottom\n\n<hr>\n\n#### Negative Sampling\n\n- The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. \n\n- That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression is used instead\n\n**Usual Softmax**\n\n![](images/word_embeddings/usual_softmax.png)\n\n**Negative Sampling Softmax**\n\n![](images/word_embeddings/negative_sampling_softmax.png)\n\nFor each training sample, the classifier is fed \n- a true pair (a center word and another word that appears in its context) and \n- a number of k randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary)\n\nBy learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.\n\n- This is important: instead of predicting the next word (the \"standard\" training technique), the optimized classifier simply predicts whether a pair of words is good or bad (which makes this a binary classification problem)\n\nA model is trained to predict whether the output word is a correct or corrupt one. ?\n<br>\n- Hyperparameter in Word2Vec w.r.t Negative Sampling: Number of Negative samples\n     - Original Word2Vec paper suggests using 5-20 negative sampling words for smaller datasets, and 2-5 for large datasets \n\nSource: Advanced NLP Udemy Course by Lazy Programmer\n\n<hr>\n\n### 2C. How Word2vec was implemented?\n\n**1. There is a twist in the way negative sampling logic was implemented**\n\n**Expected Method** <br> \nEx. \"The quick brown fox jumps over the lazy dog.\" <br>\n- Word2Vec Model Type: Skipgram\n- Focus Word (input): jumps\n- Context Words (correct output/positive samples): brown, fox, over, the\n- Negative Samples (corrupted outputs/ negative samples): apple, orange, london, ship\n\n**Actual Method** <br>\n\nEx. \"The quick brown fox jumps over the lazy dog.\" <br>\nNegative sentence = \"The quick brown fox lighthouse over the lazy dog.\"\n- Word2Vec Model Type: Skipgram\n- Focus Word (positive input): jumps\n- Negative input: lighthouse\n- Context Words (positive samples): Predicting `brown`, `fox`, `over` or `the` given `jumps`\n- Context Words (corrupted outputs or negative samples): Predicting `brown`, `fox`, `over` or `the` given `lighthouse`\n\n<hr>\n\n**2. Dropping high frequency words increases context window**\n\nEx. \"The quick brown fox jumps over the lazy dog.\" <br>\nNew sentence = \"quick brown fox jumps lazy dog.\"\n\n<hr>\n\n**3. Decaying Learning Rate**\n- Decay the learning rate from max to min\n\n<hr>\n\n**4. Typical Hyper Parameters used for Word2Vec model Training**\n- no_of_epochs = 20\n- context_window_size = 5\n- starting_learning_rate = 0.025 \n- final_learning_rate = 0.0001\n- (automatic calculation of learning rate decay using above two values)\n- Hidden layer dimension size = Word2Vec Embedding Size = 300\n\n<hr>\n\n## 3. Applications of Word2Vec\n\n- Word2Vec is used to **measure similarity** between 2 documents\n- Pre-trained Word2Vec or Custom-trained Word2Vec bettered **Text Classification** results (compared to ones with bag-of-words apporach)\n- Word2Vec as a way to augment text data | [link](https://www.kaggle.com/code/theoviel/using-word-embeddings-for-data-augmentation/notebook)\n- Word2Vec Coherence Score Computation for Topic Modeling\n\n```python\noverall_coherence_score = 0\nFor each_topic in topics:\n  Coherence_score_for_this_topic = 0\n  do(Each of the 20 topics consists of  probability simplex of words)\n  do(Select the top 10 highly problems in each topic)\n  do(Take 2 out of the top 10 words):\n    Coherence_score_for_this_topic = Coherence_score_for_this_topic + \n                                      pre-trained_W2V_embedding_based_similarity(word1,word2)\n  overall_coherence_score = Overall_coherence_score + Coherence_score_for_this_topic\n```\n\n<hr>\n\n## 4. Conclusion\n\n- Your choice of Word2Vec model depends on the below rationale\n    - **Skip-gram**: Works well with small amount of the training data, represents well even rare words or phrases.\n    - **CBOW**: Several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n\n<hr>\n\n**What made `Word2Vec` popular?**\n- `Word2Vec` was the first scalable model that generated word embeddings for large corpus (millions of unique words). \n- One of the first useful Transfer Learning embedding that improved accuracy of Text Classification (compared to bag of words approaches)\n\n**What are the competing pre-trained embeddings?** \n- `GloVe` - a Word Embedding algorithm trained using word co-occurrences - achieved similar results as Word2Vec but with less training time. However, memory foot-print needd to store co-occurrence matrix made it disadvantageous for large corpuses\n- Both `Word2Vec` and `GloVe` did not handle **out of vocabulary (OOV)** cases. All OOV words were given the same embedding. \n- `FastText` - an algorithm very similar to `Word2Vec` but at character level - solved the OOV issue to some extent. This is possible because FastText aggregates embeddings of individual ngrams of the word. There are more chances of presence of ngrams of a word than the whole word. \n\n<hr>\n\n**The need for more contextualized embeddings:**\n- Polysemy is not captured. E.g.: \"cell\" could be \"biological cell\", \"prison cell\" or \"phone\"\n- All above 3 algorithms sufferred with respect to polysemy. The contextualized embeddings from ELMo, ULMFiT and transformer models since BERT solved the polysemy problem better\n      \n<hr>\n\n## 5. References and Useful Links\n\n- Advanced NLP and Deep Learning Course in Udemy by Lazy Programmer | [link](https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/)\n- Mathjax Cheatsheet | [link](https://jojozhuang.github.io/tutorial/mathjax-cheat-sheet-for-mathematical-notation/)\n- Efficient Estimation of Word Representations in Vector Space | [Paper](https://arxiv.org/pdf/1301.3781.pdf)\n\nGlove (though not discussed here, in case you are interested to read theory of Glove): \n- http://www.foldl.me/2014/glove-python/\n- http://nbviewer.jupyter.org/github/hans/glove.py/blob/master/demo/glove.py%20exploration.ipynb\n- https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8\n- Good glove numpy implementation (implemented using adaptive gradient descent method): https://github.com/hans/glove.py\n\thttps://github.com/hans/glove.py/blob/master/glove.py\n\thttp://nbviewer.jupyter.org/github/hans/glove.py/blob/master/demo/glove.py%20exploration.ipynb\n- Glove implementation using Alternating Least Squares method:\n\thttps://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/glove.py\n- https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970\n- https://towardsdatascience.com/emnlp-what-is-glove-part-ii-9e5ad227ee0\n\nWord2Vec: \n- https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281 (part 1)\n- http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ (part 2)\n- https://machinelearningmastery.com/develop-word-embeddings-python-gensim/ (practical)\n\nW2V implementations in Python\n- https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/\n- https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/\n- https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py\n- https://www.tensorflow.org/tutorials/representation/word2vec\n\thttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n\thttps://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\n\nComparison: \n- https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n- https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/\n- https://github.com/parulsethi/gensim/blob/wordrank_wrapper/docs/notebooks/Wordrank_comparisons.ipynb (use brown corpus from nltk easy to download and compare)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2020-05-03-pretrained_word_embeddings.html","toc":true},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.336","theme":"cosmo","title-block-banner":true,"comments":{"utterances":{"repo":"senthilkumarm1901/QuartoBlogComments"}},"aliases":["/Word2Vec/2020/05/03/Pretrained_Word_Embeddings"],"author":"Senthil Kumar","badges":true,"branch":"master","categories":["NLP"],"date":"2020-05-03","description":"This blog post attempts to explain one of the seminal algorithms in NLP for embedding creation","hide":false,"image":"images/word_embeddings/cbow_vs_skip_gram.png","title":"The Theory of Word2Vec Algorithm"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}