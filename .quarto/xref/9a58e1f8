{"headings":["the-precursor-language-models-to-word2vec","a.-the-counting-based-bigram-language-model","b.-evolution-of-word2vec-from-context-predicting-based-language-models","how-word2vec-evolved-from-logistic-bigram-language-model","a.-simpler-linear-logistic-bigram-model","b.-feed-forward-neural-network-bigram-model","theory-of-word2vec","a.-what-are-the-two-types-of-word2vec-models","continuous-bag-of-words","how-cbow-works","skipgram","how-skip-gram-works","b.-how-is-word2vec-training-optimized-objective-function-and-optimization-methods","why-ordinary-softmax-could-be-problematic","hierarchical-softmax","negative-sampling","c.-how-word2vec-was-implemented","applications-of-word2vec","conclusion","references-and-useful-links"],"entries":[]}