[
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html",
    "title": "Introduction to Statistical Language Modeling",
    "section": "",
    "text": "A model that generates a probability distribution over sequences of words\nIn simpler words, LM is a model to predict the next word in a sequence of words"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#what-is-a-language-model",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#what-is-a-language-model",
    "title": "Introduction to Statistical Language Modeling",
    "section": "",
    "text": "A model that generates a probability distribution over sequences of words\nIn simpler words, LM is a model to predict the next word in a sequence of words"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#applications-of-lms",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#applications-of-lms",
    "title": "Introduction to Statistical Language Modeling",
    "section": "2. Applications of LMs",
    "text": "2. Applications of LMs\n\nPredicting upcoming words or estimating probability of a phrase or sentence is useful in noisy, ambiguous text data sources\n\nSpeech Recognition: E.g.: P(‘recognize speech’) &gt;&gt; P(‘wreck a nice beach’)\nSpelling Correction: E.g.: P(‘I have a gub’) &lt;&lt; P(‘I have a gun’)\nMachine Translation: E.g.: P(‘strong winds’) &gt; P(‘large winds’)\nOptical Character Recognition/ Handwriting Recognition\nAutoreply Suggestions\nText Classification (discussed with python implementation of a simple N-gram model)\nText Generation (discussed this with Char-level and Word-level language models)"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#n-gram-language-modelling",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#n-gram-language-modelling",
    "title": "Introduction to Statistical Language Modeling",
    "section": "3. N-gram Language Modelling",
    "text": "3. N-gram Language Modelling\nSample Corpus:  &gt; This is the house that Jack built. &gt; This is the malt &gt; That lay in the house that Jack built. &gt; This is the rat, &gt; That ate the malt &gt; That lay in the house that Jack built. &gt; This is the cat, &gt; That killed the rat, &gt; That ate the malt &gt; That lay in the house that Jack built.\nSource: “The house that jack built” Nursery Rhyme\nWhat is the probability of house occurring given the before it?\nIntuitively, we count,\n\\[ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } \\]\nMathematically, the above formula can be writen as \\[ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } \\]\nWhat we have computed above is a Bigram Language Model:\n\\[ Bigram\\:Model : { p\\,( W_t|W_{t-1} ) } \\]\nHow do ngrams help us calculate the prob of a sentence?: &gt; Sentence, S = ‘A B’ &gt; We know that, probability of this sentence ‘A B’ as, &gt; P (S) = Prob (A before B) = P(A , B) = Joint Probability of A and B = P(B|A)* P(A) &gt; &gt; Let us assume a three word sentence, S = ‘A B C’ &gt; P(S) = Prob (A before B before C ) = P (A , B , C) &gt; = P(C | A , B) * P (A n B) &gt; = P (C| A , B) * P(B | A) * P (A)\nEven if there are more words, we could keep applying Conditional Probability and compute the prob of a sentence. The above rule is called Chain Rule of Probability \\[ Prob \\ ({ A_n,\\ A_{n-1},\\ ....,A_1}) = Prob\\ ( {A_n \\ |\\ {A_{n-1},\\ ....,A_1}} ) \\  \\times \\  Prob\\ (A_{n-1},\\ ....,A_1) \\]\nWith four vairables the chain rule of probability is: \\[ Prob \\ ({ A_4,\\ A_3,\\ A_2,\\ A_1}) = Prob\\ ( {A_4 \\ |\\ {A_{3},\\ A_2,\\ A_1}} ) \\  \\times \\  Prob\\ ( {A_3 \\ |\\ {A_{2},\\ A_1}})\\ \\times \\ Prob\\ ({A_2 \\ |\\ A_1})\\ \\times \\ Prob\\ (A_1) \\]"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#out-of-vocabulary",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#out-of-vocabulary",
    "title": "Introduction to Statistical Language Modeling",
    "section": "4. Out of Vocabulary",
    "text": "4. Out of Vocabulary\nSuppose we have a new sentence like the below one:  &gt; This is the dog, &gt; That scared the cat, &gt; That killed the rat, &gt; That ate the malt, &gt; That lay in the house that Jack built.\nIf we calculate the Probability of the above sentence, based on the language model trained on the previous sample corpus (The house that Jack built Nursery Rhyme), it would be zero (because “the dog” has not occurred in the LM training corpus.\nBy our chain rule of probabilities where we keep multiplying probabilities, we would encounter $ P({dog the}) = 0 $ , hence the overall probability of the above example sentence would be zero"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#one-way-to-avoid-smoothing",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#one-way-to-avoid-smoothing",
    "title": "Introduction to Statistical Language Modeling",
    "section": "One way to avoid, Smoothing!",
    "text": "One way to avoid, Smoothing!\n\\[ MLE: P({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ ) \\over Count\\ (\\ B\\ )} \\]\nWhy is it called MLE? * Suppose a bigram “the dog” occurs 5 times in 100 documents. * Given that we have this 100 document corpus (which the language model represents), the maximum likelihood of the bigram parameter “the dog” appearing in the text is 0.05\nAdd 1 Smoothing (Laplace Smoothing) \\[ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ 1 \\over Count\\ (\\ B\\ )\\ +\\ |V|} \\] * We pretend that each unique bigram occurs once more than it actually did! * Since we have added 1 to each bigram, we have added |V| bigrams in total. Hence normalizing by adding |V| to the denominator * Disadvantage: It reduces the probability of high frequency words\nAdd- $ $ Smoothing! \\[ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ \\delta \\over Count\\ (\\ B\\ )\\ +\\ \\delta * |V|} \\] * \\(\\delta\\) is any fraction such as 0.1, 0.05, etc., * Unlike Add one smoothing, this solves the zero prob issue and avoids reducing the prob of high frequency words"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#what-is-markov-assumption",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#what-is-markov-assumption",
    "title": "Introduction to Statistical Language Modeling",
    "section": "5. What is Markov Assumption?",
    "text": "5. What is Markov Assumption?\nSo the probability of occurrence of a 4-word sentence, by markov assumption is: \\[ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B,\\ C,\\ D}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C,\\ D}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) \\]\n\n“What I see NOW depends only on what I saw in the PREVIOUS step”\n\n\\[ P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1}) = P(w_i,\\ |\\ {w_{i-1}}) \\]\nHence the probability of occurrence of a 4-word sentence with Markov Assumption is: \\[ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) \\]\n\nWhat are its advantages?\nProbability of an entire sentence could be very low but individual phrases could be more probable.  For e.g.: \n\nActual Data: “The quick fox jumps over the lazy dog”\n\nProbability of a new sentence: Prob(“The quick fox jumps over the lazy cat”) = 0 (though probable to occur,‘cat’ is not there in vocab, hence zero)\nIn Markov assumption (with additive smoothing), the above sentence will have a realistic probability"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#evaluation",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#evaluation",
    "title": "Introduction to Statistical Language Modeling",
    "section": "6. Evaluation",
    "text": "6. Evaluation\n\nPerplexity\n\nPerplexity is a measurement of how well a probability model predicts a sample.\nIt is used to compare probability models.\nA low perplexity indicates the probability distribution is good at predicting the sample\n\nDefinition: - Perplexity is the inverse probability of the test set, normalized by the number of words.  Perplexity of test data = PP(test data) = \\[ P(w_1,\\ w_2,\\ ....,\\ w_N)^{1 \\over N}  \\] \\[ \\] \\[ {\\sqrt[N]{1 \\over P(w_1,\\ w_2,\\ ....,\\ w_N)}} \\] \\[ \\] \\[ {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1})}}}\\]\nthe lower the perplexity of a LM for predicting a sample, higher is the confidence for that sample to occur in the distribution\n\n\n\nLog Probability\n\nBut multiplying probabilities like these could end up giving you the result closer to zero.\nFor e.g.: P(D|C) = 0.1, P(C|B) = 0.07, P(B|A) = 0.05, P(A) = 0.2\nP(A before B before C before D before E) = 0.00007 \\(--&gt;\\) too low or almost zero !\nLogarithm to the rescue !\nLogarithm is a monotonically increasing function !\nMeaning: If P(E|D) &gt; P(D|C), then log(P(E|D)) &gt; log (P(D|C))\nAlso, log (A *B) = log (A) + log (B)\n\n\n\n\nLogarithmChart\n\n\n\\[ \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\] - where N is the number of words in the sentence. - Since log (probabilities) are always negative (see graph: log of values less than 1 is negative), shorter sentences will have higher probability of occurrence.\nTo normalize for length of sentences, \\[ {1 \\over N} \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] \\]\nHigher the log probability value of a LM in predicting a sample, higher is the confidence for that sample to occur in the distribution\n\nPerplexity and Log-Probability metrics for a Bigram Markov Language Model\n\\[ Perplexity\\ for\\ a\\ bigram\\ model = {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1}})}}}\\]\nComparing with normalized log probability for a bigram model:\n\\[ LogProbability\\ for\\ a\\ bigram\\ model = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] \\]"
  },
  {
    "objectID": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#application",
    "href": "posts/2020-03-17-introduction_to_statistical_language_modeling.html#application",
    "title": "Introduction to Statistical Language Modeling",
    "section": "7. Application",
    "text": "7. Application\n\nText Classification\nIn this notebook, Markov Bigram model is implemented on a text classification problem. We can build a deterministic classifier where class is attributed to the class with highest log probability score.\nprint(bigram_markov(\"economic growth slows down\"))\n# log probability score of different classes\n[('business', -3.9692388514802905),\n ('politics', -6.7677569438805945),\n ('tech', -8.341093396600021),\n ('sport', -9.000286606679415),\n ('entertainment', -9.09610496174359)]\n\nClass attributed: 'business'\nprint(bigram_markov(\"prime minister has gone to the US on an official trip\"))\n# log probability score of different classes\n[('politics', -3.5746871267381852),\n ('business', -7.440089097987748),\n ('tech', -10.252850263607193),\n ('sport', -11.981405120960808),\n ('entertainment', -12.124273481509913)]\n \n Class attributed: 'politics'\n\n\nText Generation\nIn this notebook, a char-level LM, using trigrams and bigrams of characters, was used to generate dianosour names.\n&gt;&gt; print(names.iloc[:,0].head())\n\n# Names of Dianosours used as input\n0     Aachenosaurus\n1          Aardonyx\n2    Abdallahsaurus\n3       Abelisaurus\n4     Abrictosaurus\n&gt;&gt; char_level_trigram_markov_model(names)\n\n# Names of Dianosours generated as output\nbroplisaurus\nprorkhorpsaurus\njintagasaurus\ncarsaurus\ngapophongasaurus\nteglangsaurus\nborudomachsaurus\nzheisaurus\nhorachillisaurus\naveiancsaurus\nReferences:\n\nWikipedia\nUdemy Course on Deep NLP by Lazy Programmer \nNLP Course on Coursera by National Research University Higher School of Economics"
  },
  {
    "objectID": "posts/2023-09-30-pycon-india.html",
    "href": "posts/2023-09-30-pycon-india.html",
    "title": "Build a Replicable Serverless Python NLP App from Scratch",
    "section": "",
    "text": "PyCon’23 OpenSpaces Talk"
  },
  {
    "objectID": "posts/2023-09-30-pycon-india.html#the-presentation-is-hosted-in-toyota-connected-india-github-organization",
    "href": "posts/2023-09-30-pycon-india.html#the-presentation-is-hosted-in-toyota-connected-india-github-organization",
    "title": "Build a Replicable Serverless Python NLP App from Scratch",
    "section": "",
    "text": "PyCon’23 OpenSpaces Talk"
  },
  {
    "objectID": "posts/2023-09-30-pycon-india.html#the-repo-is-hosted-here",
    "href": "posts/2023-09-30-pycon-india.html#the-repo-is-hosted-here",
    "title": "Build a Replicable Serverless Python NLP App from Scratch",
    "section": "The repo is hosted here:",
    "text": "The repo is hosted here:\ngithub.com/Toyota-Connected-India/serverless_nlp_app"
  },
  {
    "objectID": "posts/2022-10-12-sql-fundamentals.html",
    "href": "posts/2022-10-12-sql-fundamentals.html",
    "title": "SQL Fundamentals - Learnings from Kaggle Learn Google BigQuery SQL Course",
    "section": "",
    "text": "Key points about SQL\n\nStructured Query Language to access data from Relational databases\nCase Insensitive: \n\nSQL keywords are by default set to case insensitive\nThe names of the tables and columns specification are set to case insensitive on the SQL database server,\nHowever, it can be enabled and disabled by configuring the settings in SQL.\n\n\n\n\nCommon data types in SQL (sqlite) and most important in other SQL tools\n\nNULL. The value is a NULL value.\nINTEGER. The value is a signed integer, stored in 0, 1, 2, 3, 4, 6, or 8 bytes depending on the magnitude of the value.\nREAL. The value is a floating point value, stored as an 8-byte IEEE floating point number.\nTEXT. The value is a text string, stored using the database encoding (UTF-8, UTF-16BE or UTF-16LE).\nBLOB. The value is a blob of data, stored exactly as it was input (image, audio, etc.,)  (source)\n\n\n\nTypes of Queries\n\nQuery Type: \n\n\nSELECT ... FROM ... WHERE\n\n    SELECT DISTINCT country\n    FROM `bigquery-public-data.openaq.global_air_quality`\n    WHERE unit = 'ppm'\n\nQuery Type: \n\n\nSELECT ... FROM ... WHERE ... LIKE\n\n    SELECT DISTINCT country\n    FROM `bigquery-public-data.some_db.some_table`\n    WHERE unit LIKE '%bigquery%'\n\nQuery Type: \n\n\nSELECT ... FROM ... GROUP BY ... HAVING ...\n\nSELECT parent, COUNT(1) AS NumPosts\nFROM `bigquery-public-data.hacker_news.comments`\nGROUP BY parent\nHAVING COUNT(1) &gt; 10\n\nQuery Type: \n\n\nSELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY DESC|ASC\n\n    SELECT indicator_code, indicator_name, COUNT(indicator_code) as num_rows\n    FROM `bigquery-public-data.world_bank_intl_education.international_education`\n    WHERE year = 2016\n    GROUP BY indicator_code, indicator_name\n    HAVING num_rows &gt;= 175\n    ORDER BY num_rows DESC\n\nQuery Type: (exploring SQL EXTRACT FROM datetime variables) \n\n\nSELECT (EXTRACT DAYOFWEEK|MONTH|YEAR|DAYOFYEAR FROM time_stamp_column AS some_name FROM ... GROUP BY ... ORDER BY ...\n\nSELECT COUNT(consecutive_number) AS num_accidents, \n       EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\nFROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\nGROUP BY day_of_week\nORDER BY num_accidents DESC\nSource: Bigquery docyumentation of available time_stamp related keywords\n\nQuery Type: (CTE - Common Table Expression) \n\n\nWITH TEMP AS (CTE) SELECT some_column FROM TEMP GROUP BY ... ORDER BY ...\n\n WITH time AS \n (\n     SELECT DATE(block_timestamp) AS trans_date\n     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n )\n SELECT COUNT(1) AS transactions,\n        trans_date\n FROM time\n GROUP BY trans_date\n ORDER BY trans_date\n\nQuery Type: (JOIN): \n\n\nSELECT table1.col1, table1.col2, table2.col1 FROM table1 INNER JOIN table2 ON table1.PRIMARY_KEY = table2.FOREIGN_KEY \nA primary key is a column or a set of columns in a table whose values uniquely identify a row in the table\nA foreign key is a column or a set of columns in a table whose values correspond to the values of the primary key in another table\n\nSELECT a.owner_user_id AS user_id, COUNT(a.id) AS number_of_answers\nFROM `bigquery-public-data.stackoverflow.posts_questions` AS q\nINNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n    ON q.id = a.parent_id\nWHERE q.tags LIKE '%bigquery%'\nGROUP BY user_id\n\nQuery Type (CREATE, DROP, INSERT)\n\nDROP TABLE IF EXISTS marks_data;\n\nCREATE TABLE marks_data (\n                        grade_class integer,\n                        marks integer integer,\n                        student_id integer PRIMARY_KEY,\n                        names text\n                        );\n                        \n-- If importing from a CSV                        \n-- SELECT IMPORT(\"path/to/grade_marks.csv\", \"CSV\", \"marks_data\");\n\nINSERT INTO marks_data VALUES(‘12’,78,'S56','Senthil')\n\nQuery Type (sub_query): \n\n\nSub querying in FROM: SELECT A, B FROM (select  tabl2.col1 AS A, tabl2.col2 AS B FROM table2)\nSub quering in SELECT: SELECT account, level, (SELECT AVG(level) FROM Players) AS avg_level FROM Players\nso many other varieties …\n\nSELECT grade_class, student_id, marks\nFROM \n(\nSELECT grade_class, marks, student_id, RANK() OVER(\n    PARTITION BY grade_class\n    ORDER BY marks\n    ) marks_rank\nFROM marks_data_2\n)\nWHERE marks_rank=3"
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html",
    "href": "posts/2024-05-16-serverless-llm.html",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "",
    "text": "Want to build LLM applications?\nWondering what is the most cost effective way to learn and build them in cloud?\n\n\nThink OpenSource LLM.  Think Serverless"
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html#lambda-to-anonymize-text",
    "href": "posts/2024-05-16-serverless-llm.html#lambda-to-anonymize-text",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "1. Lambda to Anonymize Text",
    "text": "1. Lambda to Anonymize Text\n\nA Lambda to run inference on a purpose-built ML Model\n\nThis lambda can Anonymize Text\nusing a Huggingface BERT Transformer-based Fine-tuned Model\n\n\n\n1.A. Architecture\n\n\n\n1.B. How to invoke the API Gateway Endpoint\n\n\n\n1.C. How the output looks in a Streamlit App\n\n\n\n1.D. AWS CLI commands to create the Architecture\n\n\n\n\nhttps://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/"
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html#small-language-model",
    "href": "posts/2024-05-16-serverless-llm.html#small-language-model",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "2. Small Language Model",
    "text": "2. Small Language Model\n\nA Lambda to run a Small Language Model like Microsoft’s Phi3\n\n\n2.A. Architecture\n\n\n\n2.B. How to invoke the API Gateway Endpoint\n\n\n\n2.C. How the output looks in a Streamlit App\n\n\n\n2.D. AWS CLI commands to create the Architecture\n\n\n\n\nhttps://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/"
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html#small-language-model-with-rag",
    "href": "posts/2024-05-16-serverless-llm.html#small-language-model-with-rag",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "3. Small Language Model with RAG",
    "text": "3. Small Language Model with RAG\n\nA Lambda to run a RAG Implementation on a Small Language Model like Phi3, that gives better context\n\n\n3.A. A Brief Overview on RAG\nWhat is RAG, How does RAG improve LLM Accuracy?\n\nRetrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data.\n\nSource: Databricks\nHow does LLM work?\n\nSource: AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications\nHow does RAG in LLM work?\n\nSource: RealPython Blog: chromadb-vector-database\nHow is a Vector DB created\n\nSource: AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications\nDetour: If you wish to use other Vector databases\n\nSource: Data Quarry Blog: Vector databases - What makes each one different?\n\n\n3.B. Architecture\n\n\nURL we are testing on is from my favorite DL/NLP Researcher.\n\nhttps://magazine.sebastianraschka.com/p/understanding-large-language-models\n\n\n\n\n\n3.C. How to invoke the API Gateway Endpoint\n\n\n\n3.D. How the output looks in a Streamlit App\n\n\n\n3.E. AWS CLI commands to create the Architecture\n\n\n\n\nhttps://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/"
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html#large-language-model-a-partial-serverless",
    "href": "posts/2024-05-16-serverless-llm.html#large-language-model-a-partial-serverless",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "4. Large Language Model (A Partial Serverless)",
    "text": "4. Large Language Model (A Partial Serverless)\n\nA Lambda to invoke a LLM like Mistral 7B Instruct that is running in SageMaker Endpoint\n\n\n4.A. Architecture\n\n\n\n4.B. How to invoke the API Gateway Endpoint\n\n\n\n4.C. AWS CLI commands to create the Architecture\n\n\n\n\nhttps://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/"
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html#key-challenges-faced",
    "href": "posts/2024-05-16-serverless-llm.html#key-challenges-faced",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "Key Challenges Faced",
    "text": "Key Challenges Faced\n\nServerless could mean we end up with low end cpu architecture. Hence, latency high for RAG LLM implementations\nRAG could mean any big context. But converting the RAG context into a vector store will take time. Hence size of the context needs to be lower for “AWS Lambda” implementations\nMaximum timelimit in Lambda is 15 min. API Gateway times out in 30 seconds. Hence could not be used in RAG LLM implementation"
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html#key-learnings",
    "href": "posts/2024-05-16-serverless-llm.html#key-learnings",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "Key Learnings",
    "text": "Key Learnings\nMLOps Concepts:\n\nDockerizing ML Applications. What works in your machine works everywhere. More than 70% of the time building these LLM Apps is in perfecting the dockerfile.\nThe art of storing ML Models in AWS Lambda Containers. Use cache_dir well. Otherwise, models get downloaded everytime docker container is created\n\nos.environ['HF_HOME'] = '/tmp/model' #the only `write-able` dir in AWS lambda = `/tmp`\n...\n...\nyour_model=\"ab-ai/pii_model\"\ntokenizer = AutoTokenizer.from_pretrained(your_model,cache_dir='/tmp/model')\nner_model = AutoModelForTokenClassification.from_pretrained(your_model,cache_dir='/tmp/model')\nAWS Concepts:\n\naws cli is your friend for shorterning deployment time, especially for Serverless\nAPI Gateway is a frustratingly beautiful service. But a combination of aws cli and OpenAPI spec makes it replicable\nAWS Lambda Costing is cheap for PoCs\n\nFinally, the LLM Concepts:\n\nFrameworks: Llama cpp, LangChain, LlamaIndex, Huggingface (and so many more!)\nSLMs work well with Reasoning but are too slow/bad for general knowledge questions\n\n\nModels are like wines and these LLM frameworks are like bottles. The important thing is the wine more than the bottle. But getting used to how the wines are stored in the bottles help."
  },
  {
    "objectID": "posts/2024-05-16-serverless-llm.html#next-steps-for-the-author",
    "href": "posts/2024-05-16-serverless-llm.html#next-steps-for-the-author",
    "title": "Building Frugal Open Source LLM Applications using Serverless Cloud",
    "section": "Next Steps for the Author",
    "text": "Next Steps for the Author\n\nCodes discussed in recipes may not be fully efficient! We can further reduce cost if run time is reduced\n\nFor Phi3-Mini-RAG:\n\nTry leveraging a better embedding model (apart from the ancient Sentence Transformers)\nWhat about other vector databases? - Like Pinecone Milvus (we have used opensource Chromodb) here\nIdeas to explore: Rust for LLMs. Rust for Lambda.\n\nSources: - Rust ML Minimalist framework - Candle: https://github.com/huggingface/candle - Rust for LLM - https://github.com/rustformers/llm - Rust for AWS Lambda - https://www.youtube.com/watch?v=He4inXmMZZI\n\n\n\n\ngithub.com/senthilkumarm1901/serverless_nlp_app\n\n\nThank You"
  },
  {
    "objectID": "posts/2020-07-17-evolution_of_RNN_architectures.html",
    "href": "posts/2020-07-17-evolution_of_RNN_architectures.html",
    "title": "Evolution of RNN Architectures for Transfer Learning",
    "section": "",
    "text": "Why Language Modeling?\nA short introduction to Language Modeling\nHow Transfer Learning Evolved\nEvolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\nThe RNN-based Transfer Learning Architectures - ULMFiT & ELMo"
  },
  {
    "objectID": "posts/2020-07-17-evolution_of_RNN_architectures.html#agenda",
    "href": "posts/2020-07-17-evolution_of_RNN_architectures.html#agenda",
    "title": "Evolution of RNN Architectures for Transfer Learning",
    "section": "",
    "text": "Why Language Modeling?\nA short introduction to Language Modeling\nHow Transfer Learning Evolved\nEvolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\nThe RNN-based Transfer Learning Architectures - ULMFiT & ELMo"
  },
  {
    "objectID": "posts/2020-07-17-evolution_of_RNN_architectures.html#why-language-modeling",
    "href": "posts/2020-07-17-evolution_of_RNN_architectures.html#why-language-modeling",
    "title": "Evolution of RNN Architectures for Transfer Learning",
    "section": "Why Language Modeling?",
    "text": "Why Language Modeling?\n\nThe crux of Transfer Learning in 2 steps: \n\nBuild a Language Model* that understands the underlying features of the text\n\n\nFine-tune the Language Model with additional layers for downstream tasks\n\n\n\nWhy Language Model for pre-training? Language modeling can be seen as the ideal source task as it captures many facets of language relevant for downstream tasks, such as long-term dependencies, hierarchical relations and sentiment (also being self-supervised) \nRuder et al in the ULMFiT paper _______________________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/2020-07-17-evolution_of_RNN_architectures.html#introduction-to-language-modeling",
    "href": "posts/2020-07-17-evolution_of_RNN_architectures.html#introduction-to-language-modeling",
    "title": "Evolution of RNN Architectures for Transfer Learning",
    "section": "Introduction to Language Modeling",
    "text": "Introduction to Language Modeling\nLanguage Model: A model of the probability of a sequence of words\n\nA language model can assign probability to each possible next word. And also, help in assigning a probability to an entire sentence.\n\n\nApplications of Language Model\n\nSpeech Recognition: E.g.: P(‘recognize speech’) &gt;&gt; P(‘wreck a nice beach’)\nSpelling Correction: E.g.: P(‘I have a gub’) &lt;&lt; P(‘I have a gun’)\nMachine Translation: E.g.: P(‘strong winds’) &gt; P(‘large winds’)\nOptical Character Recognition/ Handwriting Recognition\nAutoreply Suggestions\nText Classification (discussed with python implementation of a simple N-gram model)\nText Generation (discussed this with Char-level and Word-level language models) _______________________________________________________________________________________________________________\n\n\n\nEvaluation Metrics for LM\n\n(1) Perplexity\n\nA low perplexity indicates a better Language Model\n\n\n\n(2) Log Probability\n\nHigher the log probability value of a LM in predicting a sample, higher is the confidence for that sample to occur in the distribution\n\nmy detailed notes on these 2 evaluation metrics of Language model is here"
  },
  {
    "objectID": "posts/2020-07-17-evolution_of_RNN_architectures.html#how-transfer-learning-evolved",
    "href": "posts/2020-07-17-evolution_of_RNN_architectures.html#how-transfer-learning-evolved",
    "title": "Evolution of RNN Architectures for Transfer Learning",
    "section": "How Transfer Learning Evolved",
    "text": "How Transfer Learning Evolved\n\nStage1: NLP started with rule-based and statistical methodologies\nStage2: ML algos such as Naive Bayes, SVM, Trees coupled with bag-of-words word representations\nStage3: Recurrent Neural Networks such as LSTM\nStage4: RNN based Seq2Seq Transfer Learning Architectures (ULMFit, ELMo, etc.,)\nStage 5: Transformers –&gt; ‘ImageNet’ moment in NLP\n\n Source: Evolution of TL in NLP https://arxiv.org/pdf/1910.07370v1.pdf"
  },
  {
    "objectID": "posts/2020-07-17-evolution_of_RNN_architectures.html#evolution-of-rnn-units---rnn-lstm-gru-awd-lstm",
    "href": "posts/2020-07-17-evolution_of_RNN_architectures.html#evolution-of-rnn-units---rnn-lstm-gru-awd-lstm",
    "title": "Evolution of RNN Architectures for Transfer Learning",
    "section": "Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM",
    "text": "Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\nWhy RNNs came into existence? - Models such as the Multi-layer Perceptron Network, vector machines and logistic regression did not perform well on sequence modelling tasks (e.g.: text_sequence2sentiment_classification) - Why? Lack of memory element ; No information retention\n\nCometh the RNNs:\n\nRNNs attempted to redress this shortcoming by introducing loops within the network, thus allowing the retention of information.\n\nAn unrolled RNN \n\nAdvantage of a vanilla RNN:\n\nBetter than traditional ML algos in retaining information\n\n\n\nLimitations of a vanilla RNN:\n\nRNNs fail to model long term dependencies.\nthe information was often “forgotten” after the unit activations were multiplied several times by small numbers\nVanishing gradient and exploding gradient problems\n\n\n\n\nLong Short Term Memory (LSTM):\n\na special type of RNN architecture\ndesigned to keep information retained for extended number of timesteps\neach LSTM cell consists of 4 layers (3 sigmoid functioins or gates and 1 tanh function)\nThe 3 sigmoid functions are called forget, update and output gates\n\n\n\nGated Recurrent Unit (GRU)\n\na curtailed version of LSTM\nretains the resisting vanishing gradient properties of LSTM but GRUs are internally simpler and faster than LSTMs. &gt; 1/ forget and update gates from LSTM are merged into a single update gate  &gt; 2/ The update gate decides how much of previous memory to keep around.  &gt; 3/ There is a reset gate which defines how to combine new input with previous value.\n\nIf interested in the math behind the RNN architectures, refer this notebook I wrote in 2019\n\n\nComparison of performance between GRU and LSTM:\n\nGRUs are almost on par with LSTM but with efficient computation.\nHowever, with large data LSTMs with higher expressiveness may lead to better results"
  },
  {
    "objectID": "posts/2020-07-17-evolution_of_RNN_architectures.html#the-rnn-based-transfer-learning-architectures---ulmfit-elmo",
    "href": "posts/2020-07-17-evolution_of_RNN_architectures.html#the-rnn-based-transfer-learning-architectures---ulmfit-elmo",
    "title": "Evolution of RNN Architectures for Transfer Learning",
    "section": "The RNN-based Transfer Learning Architectures - ULMFiT & ELMo",
    "text": "The RNN-based Transfer Learning Architectures - ULMFiT & ELMo\n\nSome history and comparison with CV\nHistorically (before the Transformer era), - Fine-tuning a LM required millions of in-domain corpus (in other words, transfer learning was not possible) - LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier\nSource: - Evolution of TL in NLP: https://arxiv.org/pdf/1910.07370v1.pdf - ULMFiT paper: https://arxiv.org/pdf/1801.06146.pdf\n\n\nULMFit\n\nUniversal Language Model Fine-tuning (ULMFiT) for Text Classification\nThis paper introduces techniques that are essential to fine-tune an LSTM-based Language Model\nThis paper specifically the superior performance of ULMFiT approach in 6 text classification datasets\n\n\n\nWhat does ULMFiT propose?\n\nPretrain a LM on a large general-domain corpus and fine-tune it on the target task (here, text classification) using novel* techniques\nWhy called Universal (the following have become synonymous with what a TL model is):\n\nIt works across tasks varying in document size, number, and label type\n\n\nit uses a single architecture and training process;\n\n\nit requires no custom feature engineering or preprocessing; and\n\n\nit does not require additional in-domain documents or labels\n\nWhat are the novel techniques:\ndiscriminative fine-tuning,\nslanted triangular learning rates, and\ngradual unfreezing\n\n\n\nThe Fine-tuning Differences in Computer Vision vs NLP\n\nCompared to CV models (which are several layers deep), NLP models are typically more shallow and thus require different fine-tuning methods\nFeatures in deep neural networks in CV have been observed to transition from general to task-specific from the first to the last layer.\nFor this reason, most work in CV focuses on transferring the first layers of the model and fine-tuning the last or several of the last layers and leaving the remaining layers frozen\n\n\nULMFiT uses AWD-LSTM cell based Language Model\n\n\n\nAbout AWD LSTM\n\nAverage SGD Weight Dropped (AWD) LSTM\nIt uses DropConnect and a variant of Average-SGD (NT-ASGD) along with several other well-known regularization strategies\n\nWhy dropout won’t work? - Dropout, an algorithm that randomly(with a probability p) ignore units’ activations during the training phase allows for the regularization of a neural network. - By diminishing the probability of neurons developing inter-dependencies, it increases the individual power of a neuron and thus reduces overfitting. - However, dropout inhibits the RNNs capability of developing long term dependencies as there is loss of information caused due to randomly ignoring units activations.\nHence drop connect - the drop connect algorithm randomly drops weights instead of neuron activations. It does so by randomly(with probability 1-p) setting weights of the neural network to zero during the training phase. - Thus redressing the issue of information loss in the Recurrent Neural Network while still performing regularization.\n Source: Yashu Seth on AWD LSTM - https://yashuseth.blog/2018/09/12/awd-lstm-explanation-understanding-language-model/\nIf interested in understanding the architecture of ULMFit in-depth, checkout my notebook from 2019 here\n\n\nELMo\n\nELMo comes up with better word representations/embeddings using Language Models that learn the context of the word in focus  Ignore the hidden vectors predicting the padding tokens and only focus on the vectors that predict on the words source: https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27\n\nELMo uses the Bi-directional Language Model to get a new embedding that will be concatenated with the initialized word embedding. The word “are” in the above figure will have a representation formed with the following embedding vectors\n\nOriginal embedding, GloVe, Word2Vec or FastText for example\nForward pass hidden layer representation vector\nBackward pass hidden layer representation vector\n\n\n\nAbout ELMo Word Vectors:\nELMo models both - (1) complex characteristics of word use (e.g., syntax and semantics) - (2) how these uses vary across linguistic contexts (i.e., to model polysemy)   - ELMo word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretraind on a large text corpus   - ELMo assigns each token/word a representation that is function of the entire input sentence   - ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM - In other words, ELMo doesn’t just use the top LSTM layer, but all the internal layers   - higher-level LSTM states capture context-dependent aspects of word meaning - lower-level states model aspects of syntax\nELMo does well in 6 diverse NLP tasks\n\n\n\n\n\n\n\n\n\n\n\nTask\nDescription\nComments about Dataset\nEvaluation Parameter\nPrevious SOTA\nELMo SOTA\n\n\n\n\nSQuAD\nStanford Question Answering Dataset\na reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable\nF1 score (harmonic mean of precision and recall)\n84.4\n85.8\n\n\nSNLI\nStanford Natural Language Inference\nSNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE)\nAccuracy\n88.6\n88.7\n\n\nSRL\nSemantic Role Labeling\nSemantic Role Labeling (SRL) recovers the latent predicate argument structure of a sentence, providing representations that answer basic questions about sentence meaning, including “who” did “what” to “whom,” etc\nF1 Score\n81.7\n84.6\n\n\nCoref\nCoreference resolution\nCoreference resolution is the task of finding all expressions that refer to the same entity in a text.\nAverage F1\n67.2\n70.4\n\n\nNER\nNamed Entity Recognition\nThe named entity recognition model identifies named entities (people, locations, organizations, and miscellaneous) in the input text\nF1\n91.93\n92.22\n\n\nSST-5\n5-class Stanford Sentiment Treebank Dataset\nfine-grained sentiment classification task uses 5 discrete classes: Strongly positive, Weakly positive, Neutral, Weakly negative, Strongly negative\nAccuracy\n53.7\n54.7\n\n\n\n\nsources for the Task Description: - https://rajpurkar.github.io/SQuAD-explorer/ - https://nlp.stanford.edu/projects/snli/ - https://demo.allennlp.org/semantic-role-labeling/MTIzODQzNg== - https://demo.allennlp.org/coreference-resolution/MTIzODQzNA== - https://demo.allennlp.org/named-entity-recognition/MTIzODQzOA== - https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4\nPre-trained Bidirectional LM Architecture of ELMo:\n\nAdvantages of ELMo: - high-quality deep context-dependent representations are learned from biLMs - the biLM layers efficiently encode different types of syntactic and semantic information about words-in-context\n\n\nConclusion:\n\nI hope this blog gives a good understanding of the pre-transformer era history of Transfer Learning architectures in NLP\nI will cover more about BERT and Transformers in the upcoming articles"
  },
  {
    "objectID": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html",
    "href": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html",
    "title": "The Mental Model for Leveraging LLMs in Cloud",
    "section": "",
    "text": "I. The Tasks, Models and Compute Thought Process  II. Tasks: Predictive vs Generative Tasks  III. Models: Evolution of LLMs - from a size-centric view  IV. Compute: Architecting LLM Applications in Cloud  - The Mental Model for Hosting in Cloud  - The Compute Environments in AWS  - Marriage between Models and Compute  V. Conclusion \n\n\n\nThe below thought process diagram is a simplified first step to a Model Selection.\nI have kept the following thoughts out of scope, for now. But for a reader, they may be super important\n\nDo you need domain-specific models (relevant if your application is run in healthcare/finance/law etc.,)?\nWhy does it have to be just one of the 2 options - Purpose-built/Customized LLM and Prompt-based General Purpose LLM?\nWhy not a Prompt-based General Purpose LLM customized with RAG or Fine-tuning\n\n\n\n  Source: Author’s take | Open for debate  \n\n\n\n\n  Source: My fav NLP Researcher / SpaCy Founder - Ines Motwani in a PyCon’24  \n\n\n\n\n  Source: Author’s take | Open for debate  \n\n\n  Source: Author’s take | Open for debate   \n  For a better read on Quantizations: Refer Introduction to Post Training Quantization Medium Article  \n\n\n\n\n\n  Source: Author’s take | Open for debate  \n\n\n\n  Source: Author’s take | Open for debate  \n\n\n\n\n  Source: Author’s take | Open for debate  \n\n\n\n\n\nIn this blog, we have seen the different LLM sizes out in the market and their compute capacity needed above.\nToDo: In a future blog, I could focus also on the right sized EC2/ SageMaker instances for different LLMs discussed above.\nToDo: It would also be a good continuation of this blog to focus on Efficient LLM Inferencing options like below\n\nllama.cpp\nollama\nmistral.rs\nvLLM\nPyTorch Mobile\nTensorflow Lite\nApple Core ML\nWindows DirectML"
  },
  {
    "objectID": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#i.the-initial-thought-process",
    "href": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#i.the-initial-thought-process",
    "title": "The Mental Model for Leveraging LLMs in Cloud",
    "section": "",
    "text": "The below thought process diagram is a simplified first step to a Model Selection.\nI have kept the following thoughts out of scope, for now. But for a reader, they may be super important\n\nDo you need domain-specific models (relevant if your application is run in healthcare/finance/law etc.,)?\nWhy does it have to be just one of the 2 options - Purpose-built/Customized LLM and Prompt-based General Purpose LLM?\nWhy not a Prompt-based General Purpose LLM customized with RAG or Fine-tuning\n\n\n\n  Source: Author’s take | Open for debate"
  },
  {
    "objectID": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#ii.-the-debate-on-tasks",
    "href": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#ii.-the-debate-on-tasks",
    "title": "The Mental Model for Leveraging LLMs in Cloud",
    "section": "",
    "text": "Source: My fav NLP Researcher / SpaCy Founder - Ines Motwani in a PyCon’24"
  },
  {
    "objectID": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#iii.-the-debate-on-models-evolution-of-llms---from-the-pov-of-their-memory-footprint",
    "href": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#iii.-the-debate-on-models-evolution-of-llms---from-the-pov-of-their-memory-footprint",
    "title": "The Mental Model for Leveraging LLMs in Cloud",
    "section": "",
    "text": "Source: Author’s take | Open for debate  \n\n\n  Source: Author’s take | Open for debate   \n  For a better read on Quantizations: Refer Introduction to Post Training Quantization Medium Article"
  },
  {
    "objectID": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#iv.-the-debate-on-compute-architecting-llm-applications-in-cloud",
    "href": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#iv.-the-debate-on-compute-architecting-llm-applications-in-cloud",
    "title": "The Mental Model for Leveraging LLMs in Cloud",
    "section": "",
    "text": "Source: Author’s take | Open for debate  \n\n\n\n  Source: Author’s take | Open for debate  \n\n\n\n\n  Source: Author’s take | Open for debate"
  },
  {
    "objectID": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#v.-conclusion",
    "href": "posts/2024-06-17-how-to-host-open-source-llms-in-aws.html#v.-conclusion",
    "title": "The Mental Model for Leveraging LLMs in Cloud",
    "section": "",
    "text": "In this blog, we have seen the different LLM sizes out in the market and their compute capacity needed above.\nToDo: In a future blog, I could focus also on the right sized EC2/ SageMaker instances for different LLMs discussed above.\nToDo: It would also be a good continuation of this blog to focus on Efficient LLM Inferencing options like below\n\nllama.cpp\nollama\nmistral.rs\nvLLM\nPyTorch Mobile\nTensorflow Lite\nApple Core ML\nWindows DirectML"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html",
    "href": "posts/2022-11-08-ubiquitous-regex.html",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "",
    "text": "Regular Expressions are a language of their own for matching patterns\nThey are highly useful in text data processing\n\nThe official Python source defines the Regex in the following way:\n\nAn expression containing ‘meta’ characters and literals to identify and/or replace a pattern matching that expression Meta Characters: these characters have a special meaning Literals: these characters are the actual characters that are to be matched\n\nUse Cases - To search a string pattern - To split a string based on a pattern - To replace a part of the string"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#case-1-extract-username-and-domain-from-email",
    "href": "posts/2022-11-08-ubiquitous-regex.html#case-1-extract-username-and-domain-from-email",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Case 1: Extract Username and Domain from Email",
    "text": "Case 1: Extract Username and Domain from Email\n\nKey Concepts: Use of group() attribute in re.search and numbered captures using proper paranthesis\npattern: “(+).(+)@(+).+”\n\nemail = \"senthil.kumar@gutszen.com\"\npattern = \"(\\w+)\\.(\\w+)@(\\w+)\\.\\w+\"\nmatch = re.search(pattern, email)\nif match:\n    first_name = match.group(1)\n    last_name = match.group(2)\n    company = match.group(3)\n    print(f\"first_name: {first_name}\")\n    print(f\"last_name: {last_name}\")\n    print(f\"company: {company}\")\nfirst_name: senthil\nlast_name: kumar\ncompany: gutszen"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#case-2-a-regex-gotcha---an-example-where-raw_string_literal-is-needed",
    "href": "posts/2022-11-08-ubiquitous-regex.html#case-2-a-regex-gotcha---an-example-where-raw_string_literal-is-needed",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Case 2: A Regex Gotcha - An example where raw_string_literal is needed",
    "text": "Case 2: A Regex Gotcha - An example where raw_string_literal is needed\n\nIn most cases without or without a raw literal, the python pattern works fine. stackoverflow comment\nBut for the followiing example where text is a raw literal string with a  in it\n\ntext = r\"Can you capture? this\\that\"\npattern = r\"\\w+\\\\\\w+\"\n\nmatches = re.findall(pattern, text)\nfor match in matches:\n    print(f\"Matched String: {match}\")\nMatched String: this\\that\n\nWhat happens if I try below example where both text and pattern are devoid of raw literal?\nDo notice the hat word in the end of the matches\n\ntext = \"Can you capture? this\\that\"\npattern = \"\\w+\\\\w+\"\n\nmatches = re.findall(pattern, text)\nfor match in matches:\n    print(f\"Matched String: {match}\")\nMatched String: Can\nMatched String: you\nMatched String: capture\nMatched String: this\nMatched String: hat\n\nWhat if I try below example?\nDo notice the capture of this&lt;tab_space&gt;hat\n\ntext = \"Can you capture? this\\that\"\npattern = r\"\\w+\\t\\w+\"\n\nmatches = re.findall(pattern, text)\nfor match in matches:\n    print(f\"Matched String: {match}\")\nMatched String: this    hat"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#case-3a-importance-of-greedy-operator",
    "href": "posts/2022-11-08-ubiquitous-regex.html#case-3a-importance-of-greedy-operator",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Case 3A: Importance of Greedy Operator !!",
    "text": "Case 3A: Importance of Greedy Operator !!\n\nUse of ? as a greedy operator\n\ntext = \"She said, 'Hello', and he replied, 'Hi'\"\npattern = \"'(.+?)'\"\nmatches = re.findall(pattern, text)\nfor match in matches:\n    print(f\"Matched String: {match}\")\nMatched String: Hello\nMatched String: Hi\ntext = \"She said, 'Hello', and he replied, 'Hi'\"\npattern = \"'(.+)'\"\nmatches = re.findall(pattern, text)\nfor match in matches:\n    print(f\"Matched String: {match}\")\nMatched String: Hello', and he replied, 'Hi"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#case-3b-importance-of-escaping-paranthesis",
    "href": "posts/2022-11-08-ubiquitous-regex.html#case-3b-importance-of-escaping-paranthesis",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Case 3B: Importance of Escaping Paranthesis!!",
    "text": "Case 3B: Importance of Escaping Paranthesis!!\n\nWhat if you want to capture text within paranthesis?\n\ntext = \"She said, (Hello), and he replied, (Hi)\"\npattern = \"\\((.+?)\\)\"\nmatches = re.findall(pattern, text)\nfor match in matches:\n    print(f\"Matched String: {match}\")\nMatched String: Hello\nMatched String: Hi"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#case-4-splitting-sentences-using-regex",
    "href": "posts/2022-11-08-ubiquitous-regex.html#case-4-splitting-sentences-using-regex",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Case 4: Splitting Sentences using Regex",
    "text": "Case 4: Splitting Sentences using Regex\n\nUse of [^&lt;patterns&gt;] to look for Negative matches until we meet any of the &lt;patterns&gt;\n\ntext = \"Hello! My Name is Senthil. How are you doing?\"\npattern = r\"([^.?!]+[.?!])\"\nsentences = re.findall(pattern, text)\nfor sentence in sentences:\n    print(f\"Sentence: {sentence.strip()}\")\nSentence: Hello!\nSentence: My Name is Senthil.\nSentence: How are you doing?"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#case-5-extraction-of-different-url-formats",
    "href": "posts/2022-11-08-ubiquitous-regex.html#case-5-extraction-of-different-url-formats",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Case 5: Extraction of different URL Formats",
    "text": "Case 5: Extraction of different URL Formats\n\nMultiple Concepts: Operator OR |; ? for 0 or 1 match; [^/\\s]+ means anything but a / or a space\n\ntext = \"Visit my website at https://www.example.com and check out www.blog.example.com or http://blogspot.com\"\npattern = r\"https?://[^/\\s]+|www.[^/\\s]+\"\nmatches = re.findall(pattern, text)\nfor match in matches:\n    print(f\"URL: {match}\")\nURL: https://www.example.com\nURL: www.blog.example.com\nURL: http://blogspot.com"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#bonus-case---the-repeating-patterns---extracting-html-tags",
    "href": "posts/2022-11-08-ubiquitous-regex.html#bonus-case---the-repeating-patterns---extracting-html-tags",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Bonus Case - The Repeating Patterns - Extracting html tags",
    "text": "Bonus Case - The Repeating Patterns - Extracting html tags\n\nThe expectation for below python code is to capture all tags and and their contents.\nBut regex will capture only the outermost &lt;div&gt; tag\n\nhtml = \"&lt;div&gt;&lt;p&gt;This is a paragraph&lt;/p&gt;&lt;span&gt;This is a span&lt;/span&gt;&lt;/div&gt;\"\npattern = r\"&lt;(.+?)&gt;(.+?)&lt;/\\1&gt;\"\nmatches = re.findall(pattern, html)\nfor match in matches:\n    tag = match[0]\n    content = match[1]\n    print(f\"Tag: {tag}, Content: {content}\")\nTag: div, Content: &lt;p&gt;This is a paragraph&lt;/p&gt;&lt;span&gt;This is a span&lt;/span&gt;"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#best-solution---use-specific-modules-avoid-regex",
    "href": "posts/2022-11-08-ubiquitous-regex.html#best-solution---use-specific-modules-avoid-regex",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Best Solution - Use specific modules (avoid regex)",
    "text": "Best Solution - Use specific modules (avoid regex)\n\nIn this html parsing case, use BeautifulSoup\n\nfrom bs4 import BeautifulSoup\n\nhtml = \"&lt;div&gt;&lt;p&gt;This is a paragraph&lt;/p&gt;&lt;span&gt;This is a span&lt;/span&gt;&lt;/div&gt;\"\nsoup = BeautifulSoup(html, 'html.parser')\n\ndef process_tags(element):\n    if not element.name.startswith(r\"[\"):\n        print(f\"Tag: {element.name}, Content: {element.get_text()}\")\n    for child in element.children:\n        if child.name:\n            process_tags(child)\n\nprocess_tags(soup)\nTag: div, Content: This is a paragraphThis is a span\nTag: p, Content: This is a paragraph\nTag: span, Content: This is a span"
  },
  {
    "objectID": "posts/2022-11-08-ubiquitous-regex.html#insisting-on-a-regex-solution",
    "href": "posts/2022-11-08-ubiquitous-regex.html#insisting-on-a-regex-solution",
    "title": "An Ode to the Ubiquitous Regex",
    "section": "Insisting on a Regex Solution?",
    "text": "Insisting on a Regex Solution?\nfetch_tags_pattern = r\"\\&lt;(\\w+)\\&gt;\"\ntag_matches = re.findall(fetch_tags_pattern, html)\n\nfor tag in tag_matches:\n    tag_pattern = f\"&lt;({tag})&gt;(.*?)&lt;/{tag}&gt;\"\n    matches = re.findall(tag_pattern, html)\n    for match in matches:\n        tag = match[0]\n        content = re.sub('(&lt;.*?&gt;)',' ',match[1])\n        print(f\"Tag: {tag}, Content: {content}\")\nTag: div, Content:  This is a paragraph  This is a span \nTag: p, Content: This is a paragraph\nTag: span, Content: This is a span"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "",
    "text": "We have taken FashionMNIST dataset and prepared a simple 2-layer NN model to uncover the fundamental concepts of PyTorch\nBefore going into the DL portions, let us look at Tensors first"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#introduction",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#introduction",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "",
    "text": "We have taken FashionMNIST dataset and prepared a simple 2-layer NN model to uncover the fundamental concepts of PyTorch\nBefore going into the DL portions, let us look at Tensors first"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#what-are-tensors",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#what-are-tensors",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "0. What are Tensors",
    "text": "0. What are Tensors\n\nTensors are like numerical arrays that encode the input, output and weights/parameters of a model in the form of arrays and matrices.\nTypical 1D and 2D arrays:\n\n Source: docs.microsoft.com/en-US/learn\n\nHow to imagine a 3D array:\n\n Source: docs.microsoft.com/en-US/learn\n\nTensors work better on GPUs. They are optimized for automatic differentiation\nTensors and numpy often have the same memory address. For example, review the code below \n\nimport numpy as np\nimport torch\n\ndata = [[1,2],[3,4]]\nnp_array = np.array(data)\ntensor_array = torch.from_numpy(np_array)\n\n# doing multiplication opearation on `np_array`\nnp.multiply(np_array,2,out=np_array)\n\nprint(f\"Numpy array:{np_array}\")\nprint(f\"Tensor array:{tensor_array}\")\nNumpy array:[[2 4]\n [6 8]]\nTensor array:tensor([[2, 4],\n        [6, 8]])\nHow to initialize a tensor?: \n# directly from a python datastructure element\ndata = [[1,2],[3,4]]\nx_tensor_from_data = torch.tensor(data)\n\n# from numpy_array\nnp_array = np.array(data)\nx_tensor_from_numpy = torch.from_numpy(np_array)\n\n# from other tensors\nx_new_tensor = torch.rand_like(x_tensor_from_data, dtype=torch.float) # dtype overrides the dtype of z_tensor_from_data\n    \n# random or new tensor of given shape\nshape = (2,3,) # or just (2,3)\nx_new_tensor_2 = torch.ones(shape)\nWhat are the attributes of a tensor?:\nprint(f\"{x_new_tensor_2.shape}\")\nprint(f\"{x_new_tensor_2.dtype}\")\nprint(f\"{x_new_tensor_2.device}\") # whether stored in CPU or GPU\nWhen to use CPU and and when to use GPU while operating tensors?: \n\nSome common tensor operations include: Any arithmetic operation, linear algebra, matrix manipulation (transposing, indexing, slicing)\nTypical GPUs have 1000s of cores. GPUs can handle parallel processing.\n\n Source: docs.microsoft.com/en-US/learn\n\nTypical CPUs have 4 cores. Modern CPUs can have upto 16 cores. Cores are units that do the actual computation. Each core processes tasks in sequential order\n\n Source: docs.microsoft.com/en-US/learn\n\nCaveat: Copying large tensors across devices can be expensive w.r.t time and memory\nPyTorch uses Nvidia CUDA library in the backend to operate on GPU cards\n\nif torch.cuda._is_available():\n    gpu_tensor = original_tensor.to('cuda') \nWhat are the common tensor operations?:  - Joining or ConCATenate\nnew_tensor = torch.cat([tensor, tensor],dim=1) # join along column if dim=1\n\nMatrix Multiplication\n\n\n# you would have to do the transpose\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\ny3 = torch.rand_like(tensor)\ntorch.matmul(tensor, tensor.T, out=y3)\nassert y1 = y2 = y3\n\nElement-wise Multiplication\n\n\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\nSingle element tensor into python numerical value\n\n\nsum_of_values = tensor.sum()\nsum_of_values_python_variable = sum_of_values.item()\nprint(sum_of_values.dtype, type(sum_of_values_python_variable))\n# &gt;&gt; torch.int64, &lt;class 'int'&gt;\n\nIn-place Operations\n\n# add in_place\ntensor.add_(5)\n# transpose  in place\ntensor.t_()\n\nSummary of the key operations\n\ntorch.cuda.is_available() gives a boolean output\ntorch.tensor(x) \nx could be a 1D or 2D iterable (list or tuple) \ntorch.ones_like(tensor_variable), torch.rand_like(tensor_variable) \ntorch.ones(shape_in_a_tuple_or_list) , torch.zeros(shape_in_a_tuple_or_list) and torch.rand(shape_in_a_tuple_or_list) \ntorch_tensor_variable[start_index:end_index:step_value] (similar to a numpy indexing)\nnumpy to torch tensor: torch.from_numpy(np_array)\ntorch_tensor to numpy: torch_tensor_variable.numpy()\nConcatenate across rows torch.cat((an_iterable_of_tensors),dim=0)\nConcatenate across columns torch.cat((an_iterable_of_tensors),dim=1) \ntensor multiplication tensor1 * tensor2 == torch.mul(tensor1,tensor2,out=tensor3) == tensor1.mul(tensor2) \nconvert single_element_tensor into a python datatype using .item() –&gt; single_element_tensor = tensor1.sum(); python_variable = single_element_tensor.item() \nIn-place Operations in torch using _: x.add_(5) will add 5 to each element of x \ntensor n = t.numpy() & np.add(n,2,out=n) –&gt; A change in n will automatically change t (vice versa is true too)\n\n\nImporting relevant modules\n\n\nCode\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# torchvision.datasets module contains `Dataset` objects for many real-world vision data\nfrom torchvision import datasets # other domain-specific libraries TorchAudio, TorchText\nfrom torchvision.transforms import (\n    ToTensor, # for normalizing the pixel values to the range [0,1]\n    Lambda, # to make user-defined functions as one of the transformations \n    )\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#dataset-and-dataloaders",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#dataset-and-dataloaders",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "1. Dataset and DataLoaders",
    "text": "1. Dataset and DataLoaders\nTwo data primitives to handle data efficiently:  - torch.utils.data.Dataset - torch.utils.data.DataLoader\nHow should the data be preprocessed before training in DL?:  - Pass samples of data in minibatches - reshuffle the data at every epoch to overfitting - leverage Python’s multiprocessing to speed up data retrieval\ntorch.utils.data.DataLoader abstracts all the above steps\nWhat does Dataset do? - Dataset: Stores data samples and their corresponding labels - DataLoader: Wraps an iterable around Dataset to enable easy access to the samples. DataLoader can also be used along with torch.multiprocessing - torchvision.datasets and torchtext.datasets are both subclasses of torch.utils.data.Dataset (they have getitem and len methods implemented) and also they can be passed to a torch.utils.data.DataLoader\nWhat does normalization do?:  - Changes the range of the data - When one pixel value is 15 and another pixel is 190, the higher pixel value will deviate the learning\nWhy do we do normalization of data before training a DL: - Prediction accuracy is better for normalized data - Model can learn faster if data is normalized\nMore details on PyTorch Primitives\n\ntorchvision.datasets –&gt; to use pre-existing datasets like FashionMNIST, coco, cifar, etc.,\ntorchvision.datasets have arguments/parameters to transform featuers (aka inputs) and target_transform to transform labels (like one hot encoding of labels\nCustomDatasetClass must overwrite the magic methods of python such as - __init__, __getitem__ and __len__ methods inherited from Dataset\ntorchvision.transforms.ToTensor (to transform/modify the features) and torchvision.transforms.Lambda (to transform the target/labels) - torchvision.transforms.ToTensor() converts features to normalized tensors - torchvision.transforms.Lambda could be used to transform labels - Lambda(lambda y: torch.zeros(number_of_classes,dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)                 ) - Tensor.scatter_ is used to change values of a tensor variable at specified indices\n\n\n1A. Converting Data into Model Suitable Iterables\n\nDownloading and transforming the datasets\nPreparing train, validation and test datasets\n\n\n# help(datasets.FashionMNIST)\nhelp(datasets.MNIST)\n\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n\n\n\nprint(test_data.test_labels[0:5])\n\ntensor([9, 2, 1, 1, 6])\n\n\n\ntraining_data.class_to_idx\n\n{'T-shirt/top': 0,\n 'Trouser': 1,\n 'Pullover': 2,\n 'Dress': 3,\n 'Coat': 4,\n 'Sandal': 5,\n 'Shirt': 6,\n 'Sneaker': 7,\n 'Bag': 8,\n 'Ankle boot': 9}\n\n\n# If you have a custom dataset in your location\n\nclass CustomImageDataset(Dataset):\n    \"\"\"FashionMNIST like Image Dataset Class\"\"\"\n    def __init__(self, \n                 annotations_file,\n                 img_dir,\n                 transform=None,\n                 target_transform=None):\n        \"\"\"\n        Args:\n            transform (Optional): dataset will take an optional argument transform \n                so that any required processing can be applied on the sample\n        \"\"\"\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n    \n    def __len__(self):\n        return len(self.img)\n    \n    def __getitem__(self, idx):\n        # format of data \n        # image_location, label_type\n        # tshirt1.jpg, T-shirt/top # class needs to be convered into numerical format\n        # pant4.jpg, Trouser # class needs to be convered into numerical format\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n        image = tvio.read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        sample = {\"image\": image, \"label\": label}\n        return sample\n\n\n# target_transform\n# turn the integer y values into a `one_hot_encoded` vector \n# 1. create a zero tensor of size 10 torch.zeros(10, dtype=torch.float)\n# 2. `scatter_` assigns a value =1\nthe_target_lambda_function = Lambda(lambda y: torch.zeros(10,\n                                    dtype=troch.float).scatter_(dim=0,\n                                                    index=torch.tensor(y), value=1))\n\n\ntraining_data = CustomImageDataset(\n    root=\"data\", # the path where the train/test data is stored\n    train=True, # False if it is a test dataset \n    download=False, # downloads the data from Web if not available at root\n    transform=ToTensor(), # transform the features; converts PIL image or numpy array into a FloatTensor and scaled the image's pixel intensity to the range [0,1]\n    target_transform=the_target_lambda_function\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=False,\n    transform=ToTensor(),\n    target_transform=the_target_lambda_function\n    # target_transform=torch.nn.functional.one_hot(y, num_classes=10) # alternate way\n)\nPreparing Validation Data from Test Data\n\nindices = list(range(len(training_data)))\nnp.random.shuffle(indices)\n\nprint(indices[0:5])\n\n[7400, 11594, 9947, 24051, 56426]\n\n\n\nsplit = int(np.floor(0.2 * len(training_data)))\ntraining_data_sample = SubsetRandomSampler(indices[split:])\nvalidation_data_sample = SubsetRandomSampler(indices[:split])\n\nConvert into iterables\n\nbatchsize = 4\n\n# create iterables \ntrain_dataloader = DataLoader(training_data, sampler=training_data_sample, batch_size=batchsize)\nvalidation_dataloader = DataLoader(training_data, sampler=validation_data_sample, batch_size=batchsize)\ntest_dataloader = DataLoader(test_data, batch_size=batchsize)\n\nprint(len(train_dataloader))\nprint(len(validation_dataloader))\nprint(len(test_dataloader))\n\n# to understand the shape of input features and output\nfor X,y in test_dataloader:\n    print(\"Shape of Features:\",X.shape)\n    print(\"Shape of Labels:\",y.shape)\n    break\n\n12000\n3000\n2500\nShape of Features: torch.Size([4, 1, 28, 28])\nShape of Labels: torch.Size([4])\n\n\n\nlen(train_dataloader)\n\n12000\n\n\n\nThe above shape of training image is in the format NCHW\nbatchsize N, no. of channels C, height H, width W\n\n\n\n1B. Visualize sample data\n\ndataiter = iter(train_dataloader)\nimages, labels = dataiter.next()\n\nfig = plt.figure(figsize=(5,5))\n\nfor idx in np.arange(4):\n    ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(labels[idx].item())\n    fig.tight_layout()\n\n\n\n\n\n# looking into just one image, label\n\nfigure = plt.figure(figsize=(5,5))\nimg, label = test_data[0]\n\nplt.axis(\"off\")\nplt.imshow(img.squeeze(),cmap=\"gray\")\n\n&lt;matplotlib.image.AxesImage at 0x7f38e81aedd0&gt;\n\n\n\n\n\n\n# Helper function for inline image display\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n        \ndataiter = iter(train_dataloader)\nimages, labels = dataiter.next()\n\n# Create a grid from the images and show them\nimg_grid = torchvision.utils.make_grid(images)\nmatplotlib_imshow(img_grid, one_channel=True)\n\n\n\n\n\n\n1C. Initiating the Tensorboard Logs and Visualizing Sample Images\n\n# specifying the log directory\nwriter = SummaryWriter('runs/fashion_mnist_2_layer_NN_experiment_1')\n\n# writing the grid of 4 images to Tensorboard log dir\nwriter.add_image('Four Sample Fashion-MNIST Images', img_grid)\nwriter.flush()\n\nHow to load the tensorboard\nTo view, start TensorBoard on the command line with: - tensorboard --logdir=runs - and open a browser tab to http://localhost:6006/ - Can view the sample images in images tab\n\nLoad the TensorBoard notebook extension for jupyter notebook\n\n%load_ext tensorboard\n\nRun the tensorboard from jupyter notebook\n\n%tensorboard --logdir runs/fashion_mnist_2_layer_NN_experiment_1"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#build-the-model-layers",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#build-the-model-layers",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "2. Build the Model Layers",
    "text": "2. Build the Model Layers\nBuild a NN with 2 hidden layers and 1 output layer\nComponents of a Neural Network:\n\nTypical Neural Network: \n\n\n\n\nimage\n\n\n\nActivation Function, Weight and Bias\n\n\n\n\nimage\n\n\n\nLinear weighted sum of inputs: x = ∑(weights * inputs) + bias\n\nf(x) = activation_func(x)\nActivation Functions add non-linearity to the model\n\nDifferent Activation Functions: \n\nSigmoid: 1/(1 + exp(-x))\nSoftmax: exp(x) / (sum(exp(x)))\nReLU: max(0,x)\nTanh: (exp(x) - exp(-x))/(exp(x) + exp(-x))\n\n\nBuilding a neural network in PyTorch - torch.nn class provides all the building block needed to build a NN - Every module/layer in PyTorch subclases the torch.nn.Module - A NN is a composite module consisting of other modules (layers)\n\nInitialize all layers in __init__ module\nBuild a 3-layer NN with\n\nflattened 28*28 image as input,\n2 hidden layers will have 512 neurons each and\nthe third layer (which also has relu activation function) will have 10 neurons each corresponding to the number of classes\n\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device}\")\n\nUsing cuda\n\n\n\n# defining the model architecture\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        # initialize the layers in __init__ constructor\n        super(NeuralNetwork,self).__init__()\n        # supercharge your sub-class by inheriting the defaults from Parent class\n        self.flatten = nn.Flatten()\n        # one can also use Functional API in PyTorch \n        # but below codes use Sequential API\n        # the below stack of layers generates scores or logits\n        self.linear_relu_stack = nn.Sequential(\n            # hidden layer 1 consisting of 512 neurons\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            # hidden layer 2 consisting of 512 neurons too\n            nn.Linear(512,512),\n            nn.ReLU(),\n            # output layer consisting of 10 neurons \n            nn.Linear(512,10),\n            # we can also build a NN without this final layer ReLU\n            # instead can also run the log_softmax directly\n            nn.ReLU(), \n        )\n        \n    def forward(self,x): # need to pass the input argument x\n        # function where the input is run through \n        # the initialized layers\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n    \n# create a instance of the class NeuralNetwork \n# move it to the device (CPU or GPU)\nmodel = NeuralNetwork().to(device)\n\n# print model structure\nprint(model)\n\n# is nn.ReLU in the final layer?\n# https://ai.stackexchange.com/questions/8491/does-it-make-sense-to-apply-softmax-on-top-of-relu\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n    (5): ReLU()\n  )\n)\n\n\n\nWhy model(X) instead of model.forward(X)?  Source\n\nDissecting the steps using Functional API\n\nStep 1:Convert 28*28 into a contiguous array of 784 pixel values\n\ninput_image = torch.rand(3, 28, 28)\nprint(input_image.size())\n# step 1: Flatten the input image\nflatten = nn.Flatten() # instantitate\nflat_image = flatten(input_image)  # pass the prev layer (input) into the instance\nprint(flat_image.size())\n\nStep 2: Dense or linear layer in PyTorch weight * input + bias\n\n# step 2: apply linear transformation `weight * input + bias`\nlayer1 = nn.Linear(in_features=28*28, out_features=512) # instantiate\nhidden1 = layer1(flat_image) # pass the prev layer (flattened image) into the instance\nprint(hidden1.size())\n\nStep 3: Apply Relu activation on the linear transformation\n\nrelu_activation = nn.ReLU() #instantiate\nhidden1 = relu_activation(hidden1)\nRepeat Step 2 and 3 for hidden2: \nlayer2 = nn.Linear(in_features=512, out_features=512)\nhidden2 = layer2(hidden1)\nhidden2 = relu_activation(hidden2)\n\nStep 4: Compute the logits\n\n# a simple 1 hidden layer NN with 20 neurons in the hidden layer\nnn_seq_modules = nn.Sequential(\n                    flatten,\n                    layer1,\n                    relu_activation,\n                    layer2,\n                    relu_activation,\n                    nn.Linear(512, 10), # the output                )\ninput_image = torch.rand(3, 28, 28)\nlogits =  nn_seq_modules(input_image)   \n\nStep 5: Apply Softmax function\n\n\nsoftmax = nn.Softmax(dim=1)\npredict_probab = softmax(logits)\n\nFull NN workflow:\n\n Source: docs.microsoft.com/en-US/learn\nHow to see internal layers of a NN in PyTorch:\nprint(\"Weights stored in first layer: {model.linear_relu_stack[0].weight} \\n\")\nprint(\"Bias stored in first layer: {model.linear_relu_stack[0].bias} \\n\") \n    \nfrom name, param in model.named_parameters():\n    print(f\"Layer: {name} | Size: {param.size()}\"\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784])\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512])\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512])\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512])\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512])\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10])"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#training-the-model",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#training-the-model",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "3. Training the Model",
    "text": "3. Training the Model\nTraining with training data and evaluating loss on Validation Data\n\n3A.Setting Hyperparameters\n\nnum_of_epochs: The number of times the entire training dataset is pass through the network\nbatch_size: The number of data samples seen by the model before updating its weights. (derived parameter steps = total_training_data/batch_size - the number of batches needed to complete an epoch)\nlearning_rate: How much to change the weights in the w = w - learning_rate * gradient. Smaller value means the model will take a longer time to find best weights. Larger value of learning_rate might make the NN miss the optimal weights because we might step over the best values\nChoice of loss_fn  Common Loss Functions for classification problems :\n\nnn.NLLLoss #Negative Log Likelihood\n\nnn.CrossEntropyLoss # combination of nn.LogSoftmax and nn.NLLLoss\n\n\nChoice of optimizers \n\ntorch.optim.SGD\ntorch.optim.Adam\ntorch.optim.RMSProp and many more …\n\n\n\nnum_of_epochs = 40\nbatchsize = 4 # already mentioned in the DataLoader arguments\nlearning_rate = 1e-3\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(),\n                            lr=learning_rate\n                           )\n# SGD optimizer in PyTorch actually is Mini-batch Gradient Descent with momentum\n# it updates one mini-batch at a time (batchsize)\n# Source: https://discuss.pytorch.org/t/how-sgd-works-in-pytorch/8060\n\n\n\n3B. Writing Core Training and Evaluation Loop Functions\n\nloss_fn and optimizer are passed to train_loop and just loss_fn to test_loop\n\nfor i in range(epochs):\n    print(f\"Epoch {i+1}\\n ----------------------------\")\n    train_loop(train_dataloader, validation_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader,model, loss_fn)\nprint(\"Over!\")    \n\ndef train_loop(train_dataloader, validation_dataloader, model, loss_fn, optimizer, epoch):\n    train_size = len(train_dataloader.dataset)\n    validation_size = len(validation_dataloader.dataset)\n    training_loss_per_epoch = 0\n    validation_loss_per_epoch = 0\n    for batch_number, (X,y) in enumerate(train_dataloader):\n        X,y = X.to(device), y.to(device)\n        \n        # compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        \n        # Backpropagation steps\n        # key optimizer steps\n        # by default, gradients add up in PyTorch\n        # we zero out in every iteration\n        optimizer.zero_grad() \n        # performs the gradient computation steps (across the DAG)\n        loss.backward()\n        # adjust the weights\n        optimizer.step()\n        training_loss_per_epoch += loss.item()\n        \n#         if batch_number % 100 == 0:\n#             print(f\"After completing {batch_number * len(X)} samples, the loss is:\")\n#             print(loss.item()) \n            \n    for batch_number, (X,y) in enumerate(validation_dataloader):\n        X,y = X.to(device), y.to(device)\n        \n        # compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        \n        validation_loss_per_epoch += loss.item()\n    avg_training_loss = training_loss_per_epoch/train_size\n    avg_validation_loss = validation_loss_per_epoch/validation_size\n    print(f\"Average Training Loss of {epoch}: {avg_training_loss}\")\n    print(f\"Average Validation Loss of {epoch}: {avg_validation_loss}\")\n    writer.add_scalars('Training vs. Validation Loss',\n                       {'Training': avg_training_loss, \n                        'Validation': avg_validation_loss\n                       },\n                       epoch\n                      )\n\n\ndef test_loop(test_dataloader,model, loss_fn, epoch):\n    test_size = len(test_dataloader.dataset)\n    # Failing to do eval can yield inconsistent inference results\n    model.eval()\n    test_loss_per_epoch, accuracy_per_epoch = 0, 0\n    # disabling gradient tracking while inference\n    with torch.no_grad():\n        for X,y in test_dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            test_loss_per_epoch += loss.item()\n            accuracy_per_epoch += (pred.argmax(1)==y).type(torch.float).sum().item()\n    print(f\"Average Test Loss of {epoch}: {test_loss_per_epoch/test_size}\")\n    print(f\"Average Accuracy of {epoch}: {accuracy_per_epoch/test_size}\")\n\n\n\n3C. Training the model for many epochs\n\n%%time\nfor epoch in range(num_of_epochs):\n    print(f\"Epoch Number: {epoch} \\n---------------------\")\n    train_loop(train_dataloader, validation_dataloader, model, loss_fn, optimizer, epoch)\n    test_loop(test_dataloader,model, loss_fn, epoch)\n\nEpoch Number: 0 \n---------------------\nAverage Training Loss of 0: 0.37492141907910503\nAverage Validation Loss of 0: 0.07822599628902972\nAverage Test Loss of 0: 0.3941003955438733\nAverage Accuracy of 0: 0.4513\nEpoch Number: 1 \n---------------------\nAverage Training Loss of 1: 0.29412952572156986\nAverage Validation Loss of 1: 0.06984573040464893\nAverage Test Loss of 1: 0.3524202892445028\nAverage Accuracy of 1: 0.5089\nEpoch Number: 37 \n---------------------\nAverage Training Loss of 37: 0.13975639427933614\nAverage Validation Loss of 37: 0.037423237568447926\nAverage Test Loss of 37: 0.19380079013922005\nAverage Accuracy of 37: 0.7052\nEpoch Number: 38 \n---------------------\nAverage Training Loss of 38: 0.13921849230745761\nAverage Validation Loss of 38: 0.038412615390023046\nAverage Test Loss of 38: 0.19745682889677718\nAverage Accuracy of 38: 0.7015\nEpoch Number: 39 \n---------------------\nAverage Training Loss of 39: 0.13862396091737622\nAverage Validation Loss of 39: 0.03721317019570803\nAverage Test Loss of 39: 0.1929354560287782\nAverage Accuracy of 39: 0.7063\nCPU times: user 12min 2s, sys: 5.22 s, total: 12min 7s\nWall time: 11min 39s\n\n\ntruncated the results for easy viewing\nPoints to ponder: - The accuracy for this 2-layer NN stands at 71%. - The Hyperparameters - batch_size, learning_rate, choice of optimizer - can be varied to see how results change. - Changing Architecture: Deepening the number of hidden layers can help in improving the accuracy or changing the architecture to use CNN or any pre-trained NN like LeNet-5 or others will improve further\n\n\n3D. Saving, Loading and Exporting the model\n\n!mkdir -p model_weights/\n\n/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.utf-8)\n\n\n\ntorch.save(model.state_dict(),\"model_weights/fmnist_2_layer_nn_model_batch_size_4.pth\")\n\nHow to save and load the model for inference?\n# pytorch models save the parameters in a internal state dictionary called `state_dict`\ntorch.save(model.state_dict(),\"data/modelname.pth\")\n    \n# infer from a saved model\n# instantiate the model architecture class\nmodel = NeuralNetwork()\nmodel.load_state_dict(torch.load(\"data/modelname.pth\"))\n# the eval method is called before inferencing so that the batch normalization dropout layers are set to `evaluation` mod\n# Failing to do this can yield inconsistent inference results\nmodel.eval()\nHow to export a pytorch model to run in any Programming Language/Platform: \n\nONNX: Open Neural Network Exchange\nConverting PyTorch model to onnx format aids in running the model in Java, Javascript, C# and ML.NET\n\n# while explorting pytorch model to onnx, \n# we'd have to pass a sample input of the right shape\n# this will help produce a `persisted` ONNX model    \nimport torch.onnx as onnx\ninput_image = torch.zeros((1,28,28))\nonnx_model_location = 'data/model.onnx'\nonnx.export(model, input_image, onnx_model)"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#predict-using-the-trained-model",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#predict-using-the-trained-model",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "4. Predict using the Trained Model",
    "text": "4. Predict using the Trained Model\nLoading the trained model and predicting for unseen data\n\n# construct the model structure\nmodel = NeuralNetwork()\n# load the state_dict\nmodel.load_state_dict(torch.load(\"model_weights/fmnist_2_layer_nn_model_batch_size_4.pth\"))\n\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n\nPredicted: \"Ankle boot\", Actual: \"Ankle boot\"\n\n\n\n# these are logit scores and not softmax outputs \n# yet they are enough for predicting the class \n# since the logits are finally coming out of a ReLU() unit\n# A ReLU outputs from (0,max)\npred[0]\n\ntensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4451, 0.0000, 0.0000, 0.0000,\n        5.6093])"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#leveraging-tensorboard",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#leveraging-tensorboard",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "5. Leveraging Tensorboard",
    "text": "5. Leveraging Tensorboard\nReiterating the steps we have already done using Tensorboard\n\n1.Specifying the Log directory and using add_images method\n\n# `torch.utils.data.tensorboard.SummaryWriter` class\n# specifying the log directory\nwriter = SummaryWriter('runs/fashion_mnist_2_layer_NN_experiment_1')\n\n# writing the grid of 4 images to Tensorboard log dir\n# we can look at `IMAGES` tab of Tensorboard for this\nwriter.add_image('Four Sample Fashion-MNIST Images', img_grid)\nwriter.flush()\n\n2.Tracking Epoch level Average Training and Validation Losses.\n\n# We can track in the `SCALARS` tab of the Tensorboard\nwriter.add_scalars('Training vs. Validation Loss',\n                   {'Training': avg_training_loss, \n                    'Validation': avg_validation_loss\n                   },\n                   epoch\n                  )\nThe Graph of Training Loss (blue line) and Validation Loss (green line) in Tensorboard\n\n\n3.After trained model is obtained, we can look at the graph to trace the sample input through your model\n\n# We can track in the `GRAPH` tab of the Tensorboard\ndataiter = iter(train_dataloader)\nimages, labels = dataiter.next()\n\n# add_graph() will trace the sample input through your model\nwriter.add_graph(model, images)\nwriter.flush()\nNN_graph in Tensorboard"
  },
  {
    "objectID": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#sources-and-github-links",
    "href": "posts/2021-08-15-pytorch_nn_model_on_fashionmnist_tensorboard.html#sources-and-github-links",
    "title": "An Introduction to PyTorch Fundamentals for Training DL Models",
    "section": "6. Sources and GitHub Links ",
    "text": "6. Sources and GitHub Links \nSources: - MSFT PyTorch Course | link - PyTorch Official Tutorial Explaining with FashionMNIST data | link - A useful Medium article on FashionMNIST dataset | link\nGithub Links: - Dockerfile to replicate the environment | link - To replicate the DL workflow described here | Notebook link"
  },
  {
    "objectID": "posts/2020-05-03-pretrained_word_embeddings.html",
    "href": "posts/2020-05-03-pretrained_word_embeddings.html",
    "title": "The Theory of Word2Vec Algorithm",
    "section": "",
    "text": "Sample Corpus:  &gt; This is the house that Jack built. &gt; This is the malt &gt; That lay in the house that Jack built. &gt; This is the rat, &gt; That ate the malt &gt; That lay in the house that Jack built. &gt; This is the cat, &gt; That killed the rat, &gt; That ate the malt &gt; That lay in the house that Jack built.\nWhat is the probability of house occurring given the before it?\nIntuitively, we count,\n\\[ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } \\]\nMathematically, the above formula can be writen as \\[ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } \\]\nWhat we have computed above is a Bigram Language Model:\n\\[ Bigram \\ Model : { p ( W_t|W_{t-1} ) } \\]\nIf you want to read more about Count-based Language Models, refer here\n\n\n\n\n\n\n\n\n\n\n\n\nForward Propagation: - If x is the current word vector and y is the vector of next word,  - Prob of y being the next word given the current word is is given by \\[ y' = p(y \\ | x) \\ = \\ softmax(W^Tx + B) \\]\n$$ y' = p(y=j |x) \\ = {{e^{(w_j^T+ b_j)}} \\over \\sum \\limits_{k=1}^K {e^{(w_k^T x + b_k)}}} $$\n$$ Where\\ j\\ is\\ one\\ of\\ the\\ output\\ words\\ from\\ a\\ vocabulary\\ size\\ of\\ K\\ words $$\nCost Function Equation: - How do we optimize W by finding gradient descent on the cost function \\(J\\) $ J $\n\nThe cost function in our case is\n\n\\[ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {y'_{k1,k2}}} \\]\n\\[ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {p(y_{k1,k2} | x_{k1})}} \\] (since number of output class is same as the number of input words in a bigram language model)\nComputing Gradient Descent on the Cost Function: - Step 1: Initalizing parameters to learn with Random Weights: Initialize W to random weights of size K x K (where K is the vocab size) and B to random weights of size K x 1 - Step 2: Compute gradient (a.k.a partial derivative) on $ J $ w.r.t W and B - Step 3: Perform Gradient Descent Update W based on $ J $ as - $ W = W -  _W J $ - $ B = B -  _B J $ where \\(\\eta\\) is the learning rate - Repeat steps 1, 2, and 3 for an entire round of data, then one epoch of training is over. For every minibatch there is only a small shift in the weights. The size of the shifting of weights is determined by learning_rate parameter\nModel Training Process: - While loss_curve not stagnating or X number of epochs are not over: - Repeat steps 1, 2, and 3 - Final W and B weights are used for model inferencing in the $ y’ = p(y  | x)  =  softmax(W^Tx + B) $\n\n\n\n\nA Typical Logistic Regression based Model:  \\[ p(y|x) = softmax(W^T\\ x) \\]\nA Neural Network based Model:  \\[ h = tanh(W_1^T\\ x)\\] \\[ p(y|x) = softmax(W_2^T\\ h) \\]\nA Neural Network Bigram Model:  \\[ p(x_{t+1} | x_t) = softmax(W_2^T tanh(W_1^T x_t)) \\]  \\[ Where\\ x_t\\ and\\ x_{t+1} are words at positions t and t+1 \\]  \\[ Where\\ W_1\\ is\\ of\\ dimension\\ K\\ X\\ D\\ and\\ W_2\\ is\\ of\\ dimension\\ D\\ X\\ K\\ \\]  \\[ Where\\ K\\ is\\ the\\ vocabulary\\ size\\] \\[ Where D\\ is\\ the\\ number\\ of\\ neurons\\ in\\ the\\ hidden\\ layer\\\\]\nThe Cost Function and Model Training are similar to Logistic Bigram Model"
  },
  {
    "objectID": "posts/2020-05-03-pretrained_word_embeddings.html#the-precursor-language-models-to-word2vec",
    "href": "posts/2020-05-03-pretrained_word_embeddings.html#the-precursor-language-models-to-word2vec",
    "title": "The Theory of Word2Vec Algorithm",
    "section": "",
    "text": "Sample Corpus:  &gt; This is the house that Jack built. &gt; This is the malt &gt; That lay in the house that Jack built. &gt; This is the rat, &gt; That ate the malt &gt; That lay in the house that Jack built. &gt; This is the cat, &gt; That killed the rat, &gt; That ate the malt &gt; That lay in the house that Jack built.\nWhat is the probability of house occurring given the before it?\nIntuitively, we count,\n\\[ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } \\]\nMathematically, the above formula can be writen as \\[ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } \\]\nWhat we have computed above is a Bigram Language Model:\n\\[ Bigram \\ Model : { p ( W_t|W_{t-1} ) } \\]\nIf you want to read more about Count-based Language Models, refer here\n\n\n\n\n\n\n\n\n\n\n\n\nForward Propagation: - If x is the current word vector and y is the vector of next word,  - Prob of y being the next word given the current word is is given by \\[ y' = p(y \\ | x) \\ = \\ softmax(W^Tx + B) \\]\n$$ y' = p(y=j |x) \\ = {{e^{(w_j^T+ b_j)}} \\over \\sum \\limits_{k=1}^K {e^{(w_k^T x + b_k)}}} $$\n$$ Where\\ j\\ is\\ one\\ of\\ the\\ output\\ words\\ from\\ a\\ vocabulary\\ size\\ of\\ K\\ words $$\nCost Function Equation: - How do we optimize W by finding gradient descent on the cost function \\(J\\) $ J $\n\nThe cost function in our case is\n\n\\[ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {y'_{k1,k2}}} \\]\n\\[ J = {{- 1 \\over K } \\sum \\limits_{k1=1}^K \\sum \\limits_{k2=1}^K y_{k1,k2} \\log {p(y_{k1,k2} | x_{k1})}} \\] (since number of output class is same as the number of input words in a bigram language model)\nComputing Gradient Descent on the Cost Function: - Step 1: Initalizing parameters to learn with Random Weights: Initialize W to random weights of size K x K (where K is the vocab size) and B to random weights of size K x 1 - Step 2: Compute gradient (a.k.a partial derivative) on $ J $ w.r.t W and B - Step 3: Perform Gradient Descent Update W based on $ J $ as - $ W = W -  _W J $ - $ B = B -  _B J $ where \\(\\eta\\) is the learning rate - Repeat steps 1, 2, and 3 for an entire round of data, then one epoch of training is over. For every minibatch there is only a small shift in the weights. The size of the shifting of weights is determined by learning_rate parameter\nModel Training Process: - While loss_curve not stagnating or X number of epochs are not over: - Repeat steps 1, 2, and 3 - Final W and B weights are used for model inferencing in the $ y’ = p(y  | x)  =  softmax(W^Tx + B) $\n\n\n\n\nA Typical Logistic Regression based Model:  \\[ p(y|x) = softmax(W^T\\ x) \\]\nA Neural Network based Model:  \\[ h = tanh(W_1^T\\ x)\\] \\[ p(y|x) = softmax(W_2^T\\ h) \\]\nA Neural Network Bigram Model:  \\[ p(x_{t+1} | x_t) = softmax(W_2^T tanh(W_1^T x_t)) \\]  \\[ Where\\ x_t\\ and\\ x_{t+1} are words at positions t and t+1 \\]  \\[ Where\\ W_1\\ is\\ of\\ dimension\\ K\\ X\\ D\\ and\\ W_2\\ is\\ of\\ dimension\\ D\\ X\\ K\\ \\]  \\[ Where\\ K\\ is\\ the\\ vocabulary\\ size\\] \\[ Where D\\ is\\ the\\ number\\ of\\ neurons\\ in\\ the\\ hidden\\ layer\\\\]\nThe Cost Function and Model Training are similar to Logistic Bigram Model"
  },
  {
    "objectID": "posts/2020-05-03-pretrained_word_embeddings.html#theory-of-word2vec",
    "href": "posts/2020-05-03-pretrained_word_embeddings.html#theory-of-word2vec",
    "title": "The Theory of Word2Vec Algorithm",
    "section": "2. Theory of Word2Vec",
    "text": "2. Theory of Word2Vec\n\n2A. What are the two types of Word2Vec Models?\n\n1. Continuous Bag of Words:\n\nPredict focus word given bag-of-words context\n\nE.g. The quick brown fox jumps over the lazy dog\n\ncurrent_slide_position = 4\nfocus_word = sentence.split()[4] = “jumps”\ncontext_window_size = 2 (one of the hyperparameters)\ncontext_words = sentence.split()[4-2:4] + sentence.split()[4:4+2] = [brown, fox, over, the]\n\n\n\n\n\nTraining Data: p(jumps | brown), p(jumps | fox), p(jumps | over), p(jumps|the)\n\n\n\nHow CBOW works?\n\nContext size is one of the hyper paramaters of Word2Vec\nUsual size: 5-10 words. We can widen the context words by dropping stop words like “the”\n\nForward Propagation:  Step 1: Find the mean of the input word vectors - Compute the dot product of Weight matrix \\(W_1\\) with each of the context word vector \\(c\\) and then average the resulting word embeddings - Since we compute mean, the order of the context words does not influence the prediction (hence the bag-of-words name)\n\\[ h = {{1 \\over \\vert{C}\\vert} {\\sum \\limits_{c \\epsilon C} W_1^T c}}\\] \\[ Where\\ h\\ is\\ the\\ mean\\ of\\ the\\ input\\ context\\ word\\ embeddings \\]\n\\[ Where\\ C\\ is\\ the\\ no.\\ of\\ Context\\ Words \\]\n\\[ Where\\ c\\ is\\ the\\ one-hot\\ encoded\\ vector\\ of\\ every\\ input\\ context\\ word\\] \nStep 2: Get the output prediction - Multiply the mean word embeding \\(h\\) by the second weight matrix \\(W_2\\) and then do softmax on the product - The above operations gives the predicted probability of output word given all the context words\n\\[ p(y | C) = softmax(W_2^T h) \\]\nKey learnings from the above CBoW Equations: - Word2Vec is a linear model (there is no activation function like tanh) - To be specific, Word2Vec is a log-linear model - Since Word2Vec is linear, the training is faster than a typical NN because backpropagation through linear layer is faster than through a non-linear layer\n\n\n\n2. Skipgram:\n\nPredict context words given the focus word\n\nE.g. The quick brown fox jumps over the lazy dog\n\ncurrent_slide_position = 4\nfocus_word = sentence.split()[4] = “jumps”\ncontext_window_size = 2 (one of the hyperparameters)\ncontext_words = sentence.split()[4-2:4] + sentence.split()[4:4+2] = [brown, fox, over, the]\n\n\n\nSkip Gram is a like a bigram where we skip a few word positions  Image Source: NLP Udemy Course by Lazy Programmer\n\nTraining Data: p(brown | jumps), p(fox|jumps), p(over|jumps), p(the|jumps)\n\n\n\nHow Skip Gram works?\nForward Propagation:\n\\[ h = W_1^T\\ input\\_focus\\_word \\]\n\\[ p(y|input\\_focus\\_word) = softmax(W_2^T\\ h) \\]\nWhere input_focus_word = one-hot encoding of the input focus word\n\n\n\n\n2B. How is Word2Vec training optimized (objective function and optimization methods)?\n\nWhy ordinary softmax could be problematic?\nK = Vocab Size = No. of Output Classes\nSuppose a large dataset has 100,000 unique tokens as K - Large number of output classes - affect accuracy - P(output_word|focus_word) = 1/ 100,000 = 99.99999% chance of failure - Order of complexity of softmax calculation is \\(O(K)\\)\n\\[ p(y=j |x) \\ = {{e^{(w_j^T+ b_j)}} \\over \\sum \\limits_{k=1}^K {e^{(w_k^T x + b_k)}}} \\]\n\n\n\nHierarchical Softmax\n\nAn approximation to the normal softmax.\nIt uses a binary tree method to find the probability of a word being the output word\nReduces the number of classes in the denominator\nReduces the complexity from \\(O(K)\\) to \\(O(\\log_2 K)\\)\n\n\nSource: Oreilly Link\nKey points:\nThe error is propagated backwards to only those nodes that are activated at the time of prediction\n\nTo train the model, our goal is still to minimize the negative log Likelihood (since this also a softmax).\nBut instead of updating all output vectors per word, we update the vectors of the nodes in the binary tree that are in the path from root to leaf node.\n“Huffman coding” is used to construct this binary tree.\n\nFrequent words are closer to the top\nInfrequent words are closer to the bottom\n\n\n\n\n\nNegative Sampling\n\nThe basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem.\nThat is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression is used instead\n\nUsual Softmax\n\nNegative Sampling Softmax\n\nFor each training sample, the classifier is fed - a true pair (a center word and another word that appears in its context) and - a number of k randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary)\nBy learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.\n\nThis is important: instead of predicting the next word (the “standard” training technique), the optimized classifier simply predicts whether a pair of words is good or bad (which makes this a binary classification problem)\n\nA model is trained to predict whether the output word is a correct or corrupt one. ?  - Hyperparameter in Word2Vec w.r.t Negative Sampling: Number of Negative samples - Original Word2Vec paper suggests using 5-20 negative sampling words for smaller datasets, and 2-5 for large datasets\nSource: Advanced NLP Udemy Course by Lazy Programmer\n\n\n\n\n2C. How Word2vec was implemented?\n1. There is a twist in the way negative sampling logic was implemented\nExpected Method  Ex. “The quick brown fox jumps over the lazy dog.”  - Word2Vec Model Type: Skipgram - Focus Word (input): jumps - Context Words (correct output/positive samples): brown, fox, over, the - Negative Samples (corrupted outputs/ negative samples): apple, orange, london, ship\nActual Method \nEx. “The quick brown fox jumps over the lazy dog.”  Negative sentence = “The quick brown fox lighthouse over the lazy dog.” - Word2Vec Model Type: Skipgram - Focus Word (positive input): jumps - Negative input: lighthouse - Context Words (positive samples): Predicting brown, fox, over or the given jumps - Context Words (corrupted outputs or negative samples): Predicting brown, fox, over or the given lighthouse\n\n2. Dropping high frequency words increases context window\nEx. “The quick brown fox jumps over the lazy dog.”  New sentence = “quick brown fox jumps lazy dog.”\n\n3. Decaying Learning Rate - Decay the learning rate from max to min\n\n4. Typical Hyper Parameters used for Word2Vec model Training - no_of_epochs = 20 - context_window_size = 5 - starting_learning_rate = 0.025 - final_learning_rate = 0.0001 - (automatic calculation of learning rate decay using above two values) - Hidden layer dimension size = Word2Vec Embedding Size = 300"
  },
  {
    "objectID": "posts/2020-05-03-pretrained_word_embeddings.html#applications-of-word2vec",
    "href": "posts/2020-05-03-pretrained_word_embeddings.html#applications-of-word2vec",
    "title": "The Theory of Word2Vec Algorithm",
    "section": "3. Applications of Word2Vec",
    "text": "3. Applications of Word2Vec\n\nWord2Vec is used to measure similarity between 2 documents\nPre-trained Word2Vec or Custom-trained Word2Vec bettered Text Classification results (compared to ones with bag-of-words apporach)\nWord2Vec as a way to augment text data | link\nWord2Vec Coherence Score Computation for Topic Modeling\n\noverall_coherence_score = 0\nFor each_topic in topics:\n  Coherence_score_for_this_topic = 0\n  do(Each of the 20 topics consists of  probability simplex of words)\n  do(Select the top 10 highly problems in each topic)\n  do(Take 2 out of the top 10 words):\n    Coherence_score_for_this_topic = Coherence_score_for_this_topic + \n                                      pre-trained_W2V_embedding_based_similarity(word1,word2)\n  overall_coherence_score = Overall_coherence_score + Coherence_score_for_this_topic"
  },
  {
    "objectID": "posts/2020-05-03-pretrained_word_embeddings.html#conclusion",
    "href": "posts/2020-05-03-pretrained_word_embeddings.html#conclusion",
    "title": "The Theory of Word2Vec Algorithm",
    "section": "4. Conclusion",
    "text": "4. Conclusion\n\nYour choice of Word2Vec model depends on the below rationale\n\nSkip-gram: Works well with small amount of the training data, represents well even rare words or phrases.\nCBOW: Several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n\n\n\nWhat made Word2Vec popular? - Word2Vec was the first scalable model that generated word embeddings for large corpus (millions of unique words). - One of the first useful Transfer Learning embedding that improved accuracy of Text Classification (compared to bag of words approaches)\nWhat are the competing pre-trained embeddings? - GloVe - a Word Embedding algorithm trained using word co-occurrences - achieved similar results as Word2Vec but with less training time. However, memory foot-print needd to store co-occurrence matrix made it disadvantageous for large corpuses - Both Word2Vec and GloVe did not handle out of vocabulary (OOV) cases. All OOV words were given the same embedding. - FastText - an algorithm very similar to Word2Vec but at character level - solved the OOV issue to some extent. This is possible because FastText aggregates embeddings of individual ngrams of the word. There are more chances of presence of ngrams of a word than the whole word.\n\nThe need for more contextualized embeddings: - Polysemy is not captured. E.g.: “cell” could be “biological cell”, “prison cell” or “phone” - All above 3 algorithms sufferred with respect to polysemy. The contextualized embeddings from ELMo, ULMFiT and transformer models since BERT solved the polysemy problem better"
  },
  {
    "objectID": "posts/2020-05-03-pretrained_word_embeddings.html#references-and-useful-links",
    "href": "posts/2020-05-03-pretrained_word_embeddings.html#references-and-useful-links",
    "title": "The Theory of Word2Vec Algorithm",
    "section": "5. References and Useful Links",
    "text": "5. References and Useful Links\n\nAdvanced NLP and Deep Learning Course in Udemy by Lazy Programmer | link\nMathjax Cheatsheet | link\nEfficient Estimation of Word Representations in Vector Space | Paper\n\nGlove (though not discussed here, in case you are interested to read theory of Glove): - http://www.foldl.me/2014/glove-python/ - http://nbviewer.jupyter.org/github/hans/glove.py/blob/master/demo/glove.py%20exploration.ipynb - https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8 - Good glove numpy implementation (implemented using adaptive gradient descent method): https://github.com/hans/glove.py https://github.com/hans/glove.py/blob/master/glove.py http://nbviewer.jupyter.org/github/hans/glove.py/blob/master/demo/glove.py%20exploration.ipynb - Glove implementation using Alternating Least Squares method: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/glove.py - https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970 - https://towardsdatascience.com/emnlp-what-is-glove-part-ii-9e5ad227ee0\nWord2Vec: - https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281 (part 1) - http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ (part 2) - https://machinelearningmastery.com/develop-word-embeddings-python-gensim/ (practical)\nW2V implementations in Python - https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/ - https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/ - https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py - https://www.tensorflow.org/tutorials/representation/word2vec https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\nComparison: - https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c - https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/ - https://github.com/parulsethi/gensim/blob/wordrank_wrapper/docs/notebooks/Wordrank_comparisons.ipynb (use brown corpus from nltk easy to download and compare)"
  },
  {
    "objectID": "posts/2023-08-16-II-Shell-Scripting.html",
    "href": "posts/2023-08-16-II-Shell-Scripting.html",
    "title": "Boost Your Productivity with Bash Script Recipes",
    "section": "",
    "text": "A better version of this blog piece is published in Toyota Connected India Medium Link\nBash Scripting can improve the productivity of a gamut of professionals not just DevOps, SysAdmin, Networking, ML or Cloud Engineers. It can aid product owners, scrum masters, HR professionals and even time-crunched CXOs to do their repetitive tasks faster.\nPic source: The image was created by author\nTable of Contents\nI. Introduction  II. Bash Script Recipes (main topic)  III. NL2Bash using LLMs  IV. Concluding Remarks"
  },
  {
    "objectID": "posts/2023-08-16-II-Shell-Scripting.html#i.introduction",
    "href": "posts/2023-08-16-II-Shell-Scripting.html#i.introduction",
    "title": "Boost Your Productivity with Bash Script Recipes",
    "section": "I.Introduction",
    "text": "I.Introduction\n\nI.A. Why Bash Scripting\nEveryone of us deals with files and directories. We search/move/delete/copy files. We look for a particular file or directory. We may even want to search a word/phrase in the files. With Bash, we can do those tasks at scale and at great speed.\n\n\nI.B. Why Bash Script Recipes\nCreate your own simplified bash recipes for tasks that you do repetitively. The recipes could string together several bash commands underneath, but you have abstracted them for quick use\n\n\nI.C. What is in it for you\nThe bash script recipes discussed here are intended for 2 purposes. The reader can 1. directly use the recipes in their day-to-day work (like a mini cookbook), or 2. use the recipes to learn the fundamentals and create their own recipes\n\n\nI.D. Prerequisites\n\nUnderstanding of what a Bash is, what are the different types of Bash terminals, how Bash and Linux Kernel interactions could be helpful. This important theory is out of the purview of this blog.\n\n\nPic Source: Intro to Unix (uc.edu course) | Refer here if interested\n\nNot mandatory, but other helpful pre-reading material:\n\nGoogle’s Shell Scripting Style guide: When to use a Shell Script | how to write function and comments\nWhich Shell to use"
  },
  {
    "objectID": "posts/2023-08-16-II-Shell-Scripting.html#ii.-the-bash-cli-recipes",
    "href": "posts/2023-08-16-II-Shell-Scripting.html#ii.-the-bash-cli-recipes",
    "title": "Boost Your Productivity with Bash Script Recipes",
    "section": "II. The Bash CLI Recipes",
    "text": "II. The Bash CLI Recipes\nGeneric Recipes for everyone\n\nsearch_file & search_file_with_regex\nsearch_string_with_regex & search_n_replace_with_string\nmanage_files\ninside_csv\nAWS Recipes\n\nEach of the recipes has the following details:\n\nCore Function\nLearnings\nHow to run it as a bash command\n\nYou can always use your own data to run the recipes. If you would like to replicate what I have, follow below instructions. For sections II.1 and II.2, we are using using this github repo as dataset to play with - Download the data as follows\n# in any new directory of yours\nmkdir -p dir_to_run_bash_scripts && cd dir_to_run_bash_scripts\ncurl -L https://github.com/PacktPublishing/Bash-Cookbook/archive/refs/heads/master.zip -O && mv master.zip Bash-Coolbook.zip\nunzip Bash-Coolbook.zip\nFor section II.4, use this csv file:\ncurl https://github.com/altair-viz/vega_datasets/blob/master/vega_datasets/_data/la-riots.csv &gt;&gt; la-riots.csv\n\nII. 1. Searching Files\nRecipe Title: search_file\n\nCore Function in the bash script search_file.bash\nfunction search_file_in_a_dir()\n{\n    find \"$1\" -type f -name \"$2\"\n}\nLearnings:\n\nfind allows to search for a file recursively under every dir in a specific dir\nParameters such as are passed to functions as positional arguments\n\nHow to run the bash script as a command:\n% search_file -d ./dir_to_search -f \"*partial_file_to_search*\"\n% search_file -f \"some_partial_file_name*\" # OR search_file -f \"full_file_name\"\nFor full recipe details and bash outputs, refer here\n\nRecipe Title: search_file_with_regex\n\nCore Functions in the bash script search_file_with_regex.bash\nfunction 1_search_file_in_a_dir()\n{\n    find \"$1\" -type f\n}\n\nfunction 2_to_treat_space_in_file_path()\n{\n    sed \"s| |?|g\"\n}\n\nfunction 3_isolate_file_name()\n{\n    rev | cut -d'/' -f1 | rev\n}\n\nfunction 4_run_regex_file_search()\n{\n    grep -E \"$2\"\n}\n\nfunction main_function()\n{\n    1_search_file_in_a_dir $1 | \\\n    2_to_treat_space_in_file_path | \\\n    3_isolate_the_file_name | \\\n    4_run_regex_file_search $2\n}\n\n# in one line, the above command is \n# find \"$1\" -type f |  sed \"s| |?|g\" | rev | cut -d'/' -f1 | rev | grep -E \"$2\"\nLearnings:\n\nNote the piping ( | ) in the main function search_regex_file_in_a_dir\nThere may be space in a file path. E.g.: “/path/to/an amazing dir/a file name.csv”\nsed (streaming editor) is introduced here to find_and_replace a space as we are parsing the output of find which could have space\n3_isolate_file_name function isolates the filename in the end by the dir separator “/”\ngrep -E allows for execution of regex filtering on the previous output we have piped\nIn a regex search, [ . * ( ) ] + are metacharacters. If you need to match them as is, escape with a backslash. E.g.: “[0-9]+_[a-z]+.sh” will match a file_name like “02_some_file_name.sh”\nWhile you can use “*” in search_file but not here search_file_with_regex\n\nHow to run the bash script as a command:\n# if you know the source directory where to search\n% search_file_with_regex -d ./dir_to_search -rf \"[0-9]+_[a-z]+\\.sh\"\n# if you do not know the directory where to search, we will search from $HOME\n% search_file_with_regex -rf \"some_regex_pattern\" \nFor full recipe details and bash outputs, refer here\n\n\n\nII. 2. Searching Strings\nRecipe Title: search_string\n\nCore Function\nfunction search_a_string()\n{\n    find \"$1\" -type f -name \"$2\" -exec grep -H -n -E \"$3\" -o {} \\;\n}\nLearnings:\n\n-exec which will direct grep to search inside every matching file from find\ngrep -n gives out number of line that matches\ngrep -o outputs the matched string\ngrep -E allows “Extended” Regex patterns as input\n\nHow to run the bash script as a command:\n# do note, it need not be just regex_pattern search, \n# even a normal word as is will also be fetched\n% search_string -d dir_name -f file_name -s regex_search_string\n# if you do not know directory or type of file, \n# you can simply do the below search string itself\n% search_string -s regex_search_string\n# example\n# search_string -d \"/some/dir\" -f \"*.sh\" -s \"[a-z_]+\\(\\)\"\nFor full recipe details and bash outputs, refer here\n\nRecipe Title:search_n_replace_strings\n\nCore Function:\nfunction search_n_replace_the_string()\n{\n    search_string=\"$1\"\n    replacement_string=\"$2\"\n    full_file_path=\"$3\"\n    echo \"$search_string\" \n    echo \"$replacement_string\"\n    sed -i'.original' -e \"s|$search_string|$replacement_string|g\" $full_file_path\n}\nLearnings:\n- `sed -i ''`` command replaces the file in-place and leaves no backup. If you want a backup, you could give something like this `sed -i '.backup' to retrieve the original file later`\nHow to run the bash script as a command: (refer here)\n# if you do not know directory, you can specify the other 3 parameters\n% search_n_replace_strings -f filename -s search_string -r replacement_string\nFor full recipe details and bash outputs, refer here\n\n\n\nII.3. Manage files\nRecipe Title: manage_files:\n\nCore Functions\n# functions below are run as \n# func_name arg1 arg2 ...\n\nfunction count_files_inside_dir() \n{\n    directory=\"$1\"\n    num_files=$(ls -l \"$directory\" | grep -v \"^d\" | wc -l)\n    echo \"Number of files in $directory: $num_files\"\n}\n\nfunction backup_file() \n{\n   file_full_path=\"$1\"\n   timestamp=$(date +\"%Y%m%d%H%M%S\")\n   cp \"$file_full_path\" \"$file_full_path.$timestamp\"\n   echo \"Backup created: $file_full_path.$timestamp\"\n}\n\nfunction get_size() \n{\n   file_or_dir=\"$1\"\n   if [ -f \"$file_or_dir\" ]; then\n       size=$(du -sh \"$file_or_dir\" | awk '{print $1}')\n       echo \"Size of $file: $size\"\n   elif [ -d \"$file_or_dir\" ]; then\n       size=$(du -sh \"$file_or_dir\" | awk '{print $1}')\n       echo \"Size of directory $file: $size\"\n   else\n       echo \"$file not found or is not a regular file or directory.\"\n   fi\n}\n\nfunction split_file_into_n_chunks()\n{\n    num_of_chunks=$2\n    file_to_split=$1\n    file_path=$(echo $file_to_split | rev | cut -d'/' -f2- | rev)\n    output_prefix=$3\n    echo \"The number of chunks: $num_of_chunks\" \n    echo \"The file to split: $file_to_split\"\n    echo \"Outputs are saved as: $file_path/${output_prefix}\"\n    split -n $num_of_chunks $file_to_split \"$file_path/${output_prefix}\"\n    echo \"The new files are:\"\n    ls $file_path | grep \"$output_prefix\"\n}\n\n# the below functions are hardcoded for better understandability\nfunction split_file_based_on_size()\n{\n    any_file=$1\n    max_split_file_size=$2 #100K 50M 2G refer to KB, MB and GB\n    file_path=$(echo $any_file | rev | cut -d'/' -f2- | rev)\n    output_prefix=$3\n    echo \"The size of split file: $max_split_file_size\" \n    echo \"The file to split: $any_file\"\n    echo \"Outputs are saved as: \"$file_path/${output_prefix}\"\"\n    split -b $max_split_file_size $any_file \"$file_path/${output_prefix}\"\n    echo \"The new files are:\"\n    ls $file_path | grep \"$output_prefix\"\n}\n\nfunction join_files()\n{\n    files_prefix=$1\n    complete_file_name=$2\n    cat $files_prefix  &gt; $complete_file_name\n}\nLearnings:\n\nThe commands we have covered here include\n\na combination of list dir command ls, grep \"^d\" (anything but a directory) and word count by line wc -l\nbackup based on time using date and cp\nconditions like [ -d $file_or_dir] to detect if the value is a directory\nsplit and cat\n\n\nHow to run the bash script as a command:\n# inside the recipe, there will be a if clause to direct to the right function\n# refer full recipe for details\n% manage_files --function_name arg1 arg2\n# You can also add any number of other file operations \n# that you want to club with `manage_files`\nFor full recipe details, refer here\n\n\n\nII.4. Inside CSV\nRecipe Title: inside_csv\n# `display_column_names`, `display_n_rows_in_a_column` `basic_conditional_operations`\n\nfunction display_column_names()\n{\n    file_name=$1\n    head -n 1 $file_name | sed 's|,|\\n|g'\n}\n\nfunction display_n_rows_in_a_column()\n{\n    file_name=$1\n    column_name=$2\n    num_rows=$3\n    specific_column_number=$(head -n 1 $file_name | \\\n    sed 's|,|\\n|g' | \\\n    nl | \\\n    grep \"$column_name\" | \\\n    grep -E \"[0-9]+\" -o)\n    awk -F',' \\\n    -v column_number=$specific_column_number '{print $column_number}' \\\n    $file_name | head -n num_rows\n}\n\n# the below functions are hard-coded for better understandability\n# can you count the number of rows where gender==\"Male\"\nfunction filter_a_text_column()\n{\n    file_name=\"la-riots.csv\"\n    column_name=\"gender\"\n    text_to_filter=\"Male\"\n    specific_column_number=$(head -n 1 $file_name | \\\n    sed 's|,|\\n|g' | \\\n    nl | \\\n    grep \"$column_name\" | \\\n    grep -E \"[0-9]+\" -o)\n    num_of_males=$(awk -F',' \\\n    -v column_number=$specific_column_number '$column_number==\"$text_to_filter\" { print }' \\\n    $file_name | wc -l)\n    echo \"Number of males: $num_of_males\"\n}\n\nfunction filter_a_numeric_column()\n{\n    file_name=\"la-riots.csv\"\n    column_name=\"age\"\n    numeric_column_condition=\"&gt;= 18\"\n    specific_column_number=$(head -n 1 $file_name | \\\n    sed 's|,|\\n|g' | \\\n    nl | \\\n    grep \"$column_name\" | grep -E \"[0-9]+\" -o)\n    age_gt_18=$(awk -F',' \\\n    -v column_number=$specific_column_number \\\n    '$column_number $numeric_column_condition { print }' \\\n    $file_name | wc -l)\n    echo \"Num of ppl greater than or equal to 18: $age_gt_18\"\n}\nLearnings:\n\nWe have used primarily awk to parse inside files\nawk uses column_number as input. We infer column_number from column_name using sed, nl and grep\n\nHow to run the bash script as a command:\n# inside the recipe, there will be a if clause to direct to the right function\n# refer full recipe for details\n% inside_csv --function_name arg1 arg2\n# You can also add any number of other file operations \n# that you want to club with `manage_files`\nFor full recipe details, refer here\n\n\n\nII.5. AWS Recipes\nThanks for hanging on, so far. Now let us try some AWS specific recipes.\nRecipe Title: Manage Multiple AWS Accounts\nfunction recieve_and_verify_clipboard_contents\n{\n    pbpaste &gt; ~/.aws/credentials\n    verify=$(cat ~/.aws/credentials | \\\n    head -n 2 | \\\n    tail -n 1 | \\\n    grep \"aws_access_key_id\")\n    if [[ -z $verify ]] ; then\n        echo \"Your content below in Clipboard are not valid. \\\n            Please copy the correct short term credentials\"\n        echo $clipboard_content\n    fi\n}\n\nfunction create_aws_environment_variables()\n{\n    export REGION=$1\n    # typically AWS_PROFILE is a combination like below\n    # &lt;AWS_ACCOUNT_ID&gt;_&lt;IAM_ROLE&gt; \n    export AWS_PROFILE=$(cat ~/.aws/credentials | \\\n    head -n 1 | \\\n    cut -c 2- | rev | cut -c 2- | rev)\n    echo \"AWS PROFILE: $AWS_PROFILE\" \n    export AWS_ACCOUNT_ID=$(echo $AWS_PROFILE | awk -F'_' '{print $1}')\n    aws configure set region $REGION --profile $AWS_PROFILE\n    echo -n \"Logging into the AWS ACCOUNT:\"\n    echo $AWS_ACCOUNT_ID\n}\n\nThe methodology to make manage_multiple_aws_accounts work anywhere in terminal is slightly different than the rest of the functions\nThis because every time a shell script is run, it makes a copy of current shell and kills that shell once the shell script is executed.\nBut we want the environment variables - $AWS_PROFILE, $REGION, $AWS_ACCOUNT_ID to persist in our current shell window\n\n \n\nHence add the below bash function to ~/.zshrc.\n\nmanage_multiple_aws_accounts()\n{\n    source /path/to/bash_script_recipes/aws_recipes/manage_multiple_aws_accounts.bash\n    recieve_and_verify_clipboard_contents \n    if [[ $# -gt 0 ]]; then\n        case \"$1\" in\n            --region)\n                region_name=$2       \n                ;;         \n            *)\n                echo \"Unknown option: $1\"\n                exit 1\n                ;;\n        esac\n    fi\n    create_aws_environment_variables $region_name\n    echo -n \"You have chosen Region:\"\n    echo $region_name\n}\n\nThis function manage_multiple_aws_accounts is sourced in every new terminal. It executes the commands in create_aws_environment_variables in every terminal and hence persisting the environment variables like REGION, AWS_PROFILE, AWS_ACCOUNT_ID in your current terminal\nNote the source command which sources the manage_multiple_aws_accounts.bash script\n\n\n\nRun the script (this is as usual)\n# keep the AWS Credentials copied \n# from your aws_sso_start url like https://my-sso-portal.awsapps.com/start\nany/location/in/your/terminal % manage_multiple_aws_accounts --region ap-south-1\n\nCopied Credentials successfully\nAWS PROFILE: 123456789_DevOps-Engineer\nLogging into the AWS ACCOUNT: 123456789\nYou have chosen Region: ap-south-1\n\nIf you are a DevOps personnel, you may be interested in a fully AWS CLI solution | refer link\n\nFor full recipe details, refer here\nThere are two sample AWS recipes created from scratch using AWS CLI and some basic bash commands. Do check it out at your leisure.\n\nhow to create and connect to an EC2 instance | link\n\nThis is a Taskfile.yml. But in essence, you are executing individual bash commands\nTaskfile is so intuitively good. Check it out here.\n\n\n\n\nFrom scratch, how to create_a_vpc_with_private_subnet_and_nat.bash | link"
  },
  {
    "objectID": "posts/2023-08-16-II-Shell-Scripting.html#iii.-natural-language-2-bash-nl2bash-using-llms",
    "href": "posts/2023-08-16-II-Shell-Scripting.html#iii.-natural-language-2-bash-nl2bash-using-llms",
    "title": "Boost Your Productivity with Bash Script Recipes",
    "section": "III. Natural Language 2 Bash (NL2Bash) using LLMs …",
    "text": "III. Natural Language 2 Bash (NL2Bash) using LLMs …\nNL2Bash can be done both via Paid and Free LLMs. 1. If you are not constrained by budget, a paid Large Language Model based option is possible for productional use. Do explore AI-Shell and Yolo , powered by ChatGPT. 2. If you want a fully local and free version, there is the repo Ollama for Mac OS, which simplifies running llama2 locally.\nIf there is a solid case for giving users/developers a Natural Language way of accessing the application, the NL2Bash is a really good option.\nIn fact, in the same API, NL2Py or NL2SQL can also be implemented to interact with your application.\nNL2bash is truly exciting. However, if you won’t be running an extensively varied list of bash commands, then using an NL option is akin to using a bulldozer to mow a lawn. You’d be better off mowing the old-fashioned way with custom Bash recipes like the ones mentioned above, which leave no memory footprint and execute exceptionally swiftly."
  },
  {
    "objectID": "posts/2023-08-16-II-Shell-Scripting.html#iv.-concluding-remarks",
    "href": "posts/2023-08-16-II-Shell-Scripting.html#iv.-concluding-remarks",
    "title": "Boost Your Productivity with Bash Script Recipes",
    "section": "IV. Concluding Remarks",
    "text": "IV. Concluding Remarks\nAt its core, the bash script recipes discussed here consist of just a simple transformation\nfunction a_specific_function()\n{\n    # some simple transformation\n}\nSource: Refer Google’s Bash Scripting Style Guide\nIt is NOT a replacement for Python or Rust or even a Taskfile. Neither are the languages a replacement for Bash. But in combination with your core programming language, they are really powerful.\nIf I take some technical liberty, you did not execute bash scripting when you used find , grep, sed and awk, you actually leveraged really efficiently written C codes (source). Unequivocally, it is a great skill in your toolbox.\nBash scripting is foundational to Software Engineering and more pervasive than you think. If you have used git, docker, kubectl or even just mkdir & cd, you have tip-toed into bash scripting. Why not dive in?\nHappy Bash Scripting !"
  },
  {
    "objectID": "posts/2022-02-03-Primer-on-Probability.html",
    "href": "posts/2022-02-03-Primer-on-Probability.html",
    "title": "Part 1 - A Primer on Probability",
    "section": "",
    "text": "Introduction to Prob:\n\nThe essential understanding of Permutations, Combinations and Percentiles\n\nMultiple Event Probability:\n\nThe Addition Rule of Probability\nThe Multiplication Rule of Probability\nConditional Probability\nProbability Trees\nBayes Theorem\n\nDiscrete and Continious Probabilities\n\n\n\n\nProbability - Odds of a particular event happening over all possible outcomes\nProb =  # of Desired Outcomes  /  Total # of Possible Outcomes \nBasic types of Probability: \n\nClassical Probability\nEmpirical Probability\nSubjective Probability\n\nBoth Classical and Empirical Probabilities are Objective Probabilities where\n\nin Classical: The odds won’t change. They are based on formal reasoning. They based on established events/theory. E.g.: flipping a fair coin, picking a card from a usual pack of 52 cards\nin Emprical: The odds are based on experimental or historical data E.g.: What is the chance of a particular player scoring above 50 runs this match? (this can be determined by the historical data of that player)\n\nSubjective probabilities are based on personal beliefs\nTypes of odds:\n\nEven Odds | Equally likely events\n\nE.g.: Flipping a coin or Rolling a fair die\n\nWeighted Odds | Events with unequal chances of occuring\n\nE.g.: Chances of occurrence of rain in Chennai today\n\n\n\n\n\n\n\nPermutations: If interested in the order of things\n\nE.g.: What is the chance for Students A and B to win top 2 prizes of a competition from a class of 10; There are 8 contestants and 3 prizes. How many different possible outcomes are there?\nnPk = n! / (n-k)!\n\nCombinations: If order is not important\n\nE.g.: What are the number of ways you can pick 4 members from a team of 12 members?\nnCk = n! / k!*(n-k)!\n\nQ1) There are 8 contestants and 3 prizes. How many different possible outcomes are there?\n\n“Permutations” problem because order is important\n8! / (8-3)! = 8 X 7 X 6 = 336\n\nQ2A) In how many ways can you “pick” 4 member team from a total 12 members?\n\n“Combinations” problem\n12! / 4! * (12-4)! = 495\n\nQ2B) In how many of these 495 combinations, do sisters Layla and Olivia join the same team?\n\nAssuming the sisters have already been inducted into the team,there are -10! / 2! * (10-2)!=45 ways to pick the remaining two members\n\nPercentile Rank = Percentile rank of a given score is the percentage of scores in its freq distribution that are less than that score\n  PR =( CF' + (0.5 X F) ) / N * 100\n      CF' = Cumulative Freq (excluding the current score) = Count of all scores less than the score of interest\n      F  = Freq of the score of interest \n      N = Total number of scores in the distribution\n\n\n\n\n\nE.g. of single event probabilities:  Heads or Tails:  - Heads or Tails  - Rain or No Rain \nE.g. of Multiple event probabilities:  Sports Example:  - 30% chance of a player scoring a goal  - 40% chance of that player’s team winning \nQ) Is there a relationship between the team winning and this player’s success in scoring? \nHealthcare example:  - 1/ 10K people gets a particular rare disease  - Test is accurate only 98% of the time \nQ) What are the chances a person who is tested positive is actually false positive? \nEmployment example:  - Only 4 out of 20 get an interview  - What is the prob that friends Mohan and Lily both get a slot in the interview?  - What if Mohan gets one of the 4 slots, what is the prob of Lily securing one of the other 3 slots? \nMore Probability Tools:  - Conditional Probability  - Dependent vs Independent events  - Probability Trees and Bayes Theorem (both useful in managing multiple event scenarios) \n\n\n\nQ1. there are 6 people getting rewarded, what are the chances that 2 people - X and Y - win the gift?\nTotal number of outcomes = \n6! / (4! * 2!) = 15 \n\nNumber of outcomes where X, Y are both winning = [(X,Y), (Y,X)]\nOrder does not matter, hence number of outcomes of X and Y winning = 1\n= 1/15 = 0.0667 \nQ2.What is the probability of rolling two dice with each die throwing 1?\n1/6 * 1/6 = 0.0278 = 0.0278 2.7%\nQ3. There are 10 cards with 3 of them having X on them. What are the odds that 2 cards picked at random have X on them?\nSolution method 1: `Combinations` approach\nTotal number of combinations = 10C2 \n= 10! /(8! x 2!) = 45 \n\nTotal number of combinations with 2 X on them = 3C2 = 3! / (2! * 1!) = 3 \n\nProb of picking 2 cards at random  where both are X = 3/45 = 0.0667 \n\n\nSolution method 2: `Conditional Prob` approach\n\nChances of picking X card in attempt 1 = 3/10 = 0.3 \n\nChances of picking an X card in attempt 2 as well = 2/9 = 0.2222 \n\nChance of picking 2 X cards (2 dependent events) = 0.3 * 0.2222 = 0.0667   \nQ4. What are the chances that a medical test taken was false positive? (it is a conditional probability where given that the result is positive, what are the chances that it is false)\nDisease or No Disease\nPositive or Negative Test Result (could be false positive or true negative also)\n\nStats:\n    1. only 1 in 10,000 has disease\n    2. those with disease test positive 99% of time (that remaining 1% is False Negative or Type II)\n    3. 2% of healthy paitents will test positive\n\n\nTree \n    - Stage 1:\n    Disease 1/10000 = 0.0001 \n        - Stage 2:\n        Positive: 99/100 = 0.99 \n        Negative: 1/100 = 0.01 \n\n    No Disease 9999/10000 = 0.9999 \n        Positive: 2/100 = 0.02 \n        Negative: 98/100 = 0.98 \n\n    - Total share of people tested positive = (0.0001 * 0.99 +   0.9999 * 0.02) = 0.0201 \n    - Total share of people tested positive false \n    = 0.9999 * 0.02 = 0.02 \n    - Total share of people tested true positive \n= 0.99 * 0.0001\n\n\n    Prob of false positive = Share of False Positive / Total # of Positives = 0.02 / 0.0201 = 0.995 = 99.5%\n\n    Prob of True Positive = Share of True Positive / Total # of positives = (0.99 * 0.0001) / 0.0201  = 0.0049 = 0.5%\n\nQ5. 70% of the population has brown eyes, 30% do not have brown eyes. 60% of the population requires reading glasses, 40% do not need reading glasses. In a city of 10,000 people, how many would both not have brown eyes and not require reading glasses?\nIndependent events - just multiply the prob. \nP(not_brown_eyes) * P(not_require_glasses) = 0.3 * 0.4 = 0.12 \n\n0.12 * 10000 = 1200 \nQ6. There are two stacks of cards. Each stack has 4 cards. Each stack has a card with the numbers 1, 2, 3, and 4. There are 16 possible outcomes. You will be allowed to take one card from each stack. Two cards total. What is the probability of drawing at least one card with a 4 from either deck\nTotal_num_of_cards = 8\nTotal_num_of_possible_outcomes = 4C1 * 4C1 = 16\n\n= 1/4 * 1 + 1 * 1/4 - 1/16 = 0.4375\n\nQ7. Suppose you have 3 coins, each with heads on one side and tails on the other. There are 8 possible outcomes. What is the probability that when all three coins are flipped at least 2 coins will result in heads?\ntotal_num_of_outcomes = 2 * 2 * 2 = 8 \n\n\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\nQ8. There are ten people in a class. Ari and Jamaal are twins in this class. At random two people will be chosen as the class representatives. What are the odds that Ari and Jamaal will both be chosen?\nTotal_num_of_combinations = 10C2 = 10! / (8! * 2!) = 45 \n\nOnly one combination is Ari and Jammal = 1/45 = 0.0222  \nQ9. A company has 1000 employees. 70% get the flu vaccine. 95% of those that get the vaccine do NOT get the flu, 5% get the flu. 30% do not get the flu vaccine. 80% of those that do not get the vaccine do not get the flu; 20% that do not get the vaccine do get the flu. How many of the 1000 employees get the vaccine but still get the flu?\n700 Vaccinated\n-- NoFlu: 0.95 * 700 = 665 \n--Flu: 0.05 * 700 = 35 \n300 Non vaccinated\n-- NoFlu: 0.8*300 = 240 \n-- Flu: 0.2 * 300 = 60 \nSource: - LinkedIn Courses - Statistics Foundations: Probability and Statistics Foundations: The Basics | refer"
  },
  {
    "objectID": "posts/2022-02-03-Primer-on-Probability.html#quick-introduction-to-probability",
    "href": "posts/2022-02-03-Primer-on-Probability.html#quick-introduction-to-probability",
    "title": "Part 1 - A Primer on Probability",
    "section": "",
    "text": "Probability - Odds of a particular event happening over all possible outcomes\nProb =  # of Desired Outcomes  /  Total # of Possible Outcomes \nBasic types of Probability: \n\nClassical Probability\nEmpirical Probability\nSubjective Probability\n\nBoth Classical and Empirical Probabilities are Objective Probabilities where\n\nin Classical: The odds won’t change. They are based on formal reasoning. They based on established events/theory. E.g.: flipping a fair coin, picking a card from a usual pack of 52 cards\nin Emprical: The odds are based on experimental or historical data E.g.: What is the chance of a particular player scoring above 50 runs this match? (this can be determined by the historical data of that player)\n\nSubjective probabilities are based on personal beliefs\nTypes of odds:\n\nEven Odds | Equally likely events\n\nE.g.: Flipping a coin or Rolling a fair die\n\nWeighted Odds | Events with unequal chances of occuring\n\nE.g.: Chances of occurrence of rain in Chennai today"
  },
  {
    "objectID": "posts/2022-02-03-Primer-on-Probability.html#how-to-count-number-of-possible-outcomes",
    "href": "posts/2022-02-03-Primer-on-Probability.html#how-to-count-number-of-possible-outcomes",
    "title": "Part 1 - A Primer on Probability",
    "section": "",
    "text": "Permutations: If interested in the order of things\n\nE.g.: What is the chance for Students A and B to win top 2 prizes of a competition from a class of 10; There are 8 contestants and 3 prizes. How many different possible outcomes are there?\nnPk = n! / (n-k)!\n\nCombinations: If order is not important\n\nE.g.: What are the number of ways you can pick 4 members from a team of 12 members?\nnCk = n! / k!*(n-k)!\n\nQ1) There are 8 contestants and 3 prizes. How many different possible outcomes are there?\n\n“Permutations” problem because order is important\n8! / (8-3)! = 8 X 7 X 6 = 336\n\nQ2A) In how many ways can you “pick” 4 member team from a total 12 members?\n\n“Combinations” problem\n12! / 4! * (12-4)! = 495\n\nQ2B) In how many of these 495 combinations, do sisters Layla and Olivia join the same team?\n\nAssuming the sisters have already been inducted into the team,there are -10! / 2! * (10-2)!=45 ways to pick the remaining two members\n\nPercentile Rank = Percentile rank of a given score is the percentage of scores in its freq distribution that are less than that score\n  PR =( CF' + (0.5 X F) ) / N * 100\n      CF' = Cumulative Freq (excluding the current score) = Count of all scores less than the score of interest\n      F  = Freq of the score of interest \n      N = Total number of scores in the distribution"
  },
  {
    "objectID": "posts/2022-02-03-Primer-on-Probability.html#multiple-event-probabilities",
    "href": "posts/2022-02-03-Primer-on-Probability.html#multiple-event-probabilities",
    "title": "Part 1 - A Primer on Probability",
    "section": "",
    "text": "E.g. of single event probabilities:  Heads or Tails:  - Heads or Tails  - Rain or No Rain \nE.g. of Multiple event probabilities:  Sports Example:  - 30% chance of a player scoring a goal  - 40% chance of that player’s team winning \nQ) Is there a relationship between the team winning and this player’s success in scoring? \nHealthcare example:  - 1/ 10K people gets a particular rare disease  - Test is accurate only 98% of the time \nQ) What are the chances a person who is tested positive is actually false positive? \nEmployment example:  - Only 4 out of 20 get an interview  - What is the prob that friends Mohan and Lily both get a slot in the interview?  - What if Mohan gets one of the 4 slots, what is the prob of Lily securing one of the other 3 slots? \nMore Probability Tools:  - Conditional Probability  - Dependent vs Independent events  - Probability Trees and Bayes Theorem (both useful in managing multiple event scenarios)"
  },
  {
    "objectID": "posts/2022-02-03-Primer-on-Probability.html#more-probability-questions-using-above-concepts",
    "href": "posts/2022-02-03-Primer-on-Probability.html#more-probability-questions-using-above-concepts",
    "title": "Part 1 - A Primer on Probability",
    "section": "",
    "text": "Q1. there are 6 people getting rewarded, what are the chances that 2 people - X and Y - win the gift?\nTotal number of outcomes = \n6! / (4! * 2!) = 15 \n\nNumber of outcomes where X, Y are both winning = [(X,Y), (Y,X)]\nOrder does not matter, hence number of outcomes of X and Y winning = 1\n= 1/15 = 0.0667 \nQ2.What is the probability of rolling two dice with each die throwing 1?\n1/6 * 1/6 = 0.0278 = 0.0278 2.7%\nQ3. There are 10 cards with 3 of them having X on them. What are the odds that 2 cards picked at random have X on them?\nSolution method 1: `Combinations` approach\nTotal number of combinations = 10C2 \n= 10! /(8! x 2!) = 45 \n\nTotal number of combinations with 2 X on them = 3C2 = 3! / (2! * 1!) = 3 \n\nProb of picking 2 cards at random  where both are X = 3/45 = 0.0667 \n\n\nSolution method 2: `Conditional Prob` approach\n\nChances of picking X card in attempt 1 = 3/10 = 0.3 \n\nChances of picking an X card in attempt 2 as well = 2/9 = 0.2222 \n\nChance of picking 2 X cards (2 dependent events) = 0.3 * 0.2222 = 0.0667   \nQ4. What are the chances that a medical test taken was false positive? (it is a conditional probability where given that the result is positive, what are the chances that it is false)\nDisease or No Disease\nPositive or Negative Test Result (could be false positive or true negative also)\n\nStats:\n    1. only 1 in 10,000 has disease\n    2. those with disease test positive 99% of time (that remaining 1% is False Negative or Type II)\n    3. 2% of healthy paitents will test positive\n\n\nTree \n    - Stage 1:\n    Disease 1/10000 = 0.0001 \n        - Stage 2:\n        Positive: 99/100 = 0.99 \n        Negative: 1/100 = 0.01 \n\n    No Disease 9999/10000 = 0.9999 \n        Positive: 2/100 = 0.02 \n        Negative: 98/100 = 0.98 \n\n    - Total share of people tested positive = (0.0001 * 0.99 +   0.9999 * 0.02) = 0.0201 \n    - Total share of people tested positive false \n    = 0.9999 * 0.02 = 0.02 \n    - Total share of people tested true positive \n= 0.99 * 0.0001\n\n\n    Prob of false positive = Share of False Positive / Total # of Positives = 0.02 / 0.0201 = 0.995 = 99.5%\n\n    Prob of True Positive = Share of True Positive / Total # of positives = (0.99 * 0.0001) / 0.0201  = 0.0049 = 0.5%\n\nQ5. 70% of the population has brown eyes, 30% do not have brown eyes. 60% of the population requires reading glasses, 40% do not need reading glasses. In a city of 10,000 people, how many would both not have brown eyes and not require reading glasses?\nIndependent events - just multiply the prob. \nP(not_brown_eyes) * P(not_require_glasses) = 0.3 * 0.4 = 0.12 \n\n0.12 * 10000 = 1200 \nQ6. There are two stacks of cards. Each stack has 4 cards. Each stack has a card with the numbers 1, 2, 3, and 4. There are 16 possible outcomes. You will be allowed to take one card from each stack. Two cards total. What is the probability of drawing at least one card with a 4 from either deck\nTotal_num_of_cards = 8\nTotal_num_of_possible_outcomes = 4C1 * 4C1 = 16\n\n= 1/4 * 1 + 1 * 1/4 - 1/16 = 0.4375\n\nQ7. Suppose you have 3 coins, each with heads on one side and tails on the other. There are 8 possible outcomes. What is the probability that when all three coins are flipped at least 2 coins will result in heads?\ntotal_num_of_outcomes = 2 * 2 * 2 = 8 \n\n\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\nQ8. There are ten people in a class. Ari and Jamaal are twins in this class. At random two people will be chosen as the class representatives. What are the odds that Ari and Jamaal will both be chosen?\nTotal_num_of_combinations = 10C2 = 10! / (8! * 2!) = 45 \n\nOnly one combination is Ari and Jammal = 1/45 = 0.0222  \nQ9. A company has 1000 employees. 70% get the flu vaccine. 95% of those that get the vaccine do NOT get the flu, 5% get the flu. 30% do not get the flu vaccine. 80% of those that do not get the vaccine do not get the flu; 20% that do not get the vaccine do get the flu. How many of the 1000 employees get the vaccine but still get the flu?\n700 Vaccinated\n-- NoFlu: 0.95 * 700 = 665 \n--Flu: 0.05 * 700 = 35 \n300 Non vaccinated\n-- NoFlu: 0.8*300 = 240 \n-- Flu: 0.2 * 300 = 60 \nSource: - LinkedIn Courses - Statistics Foundations: Probability and Statistics Foundations: The Basics | refer"
  },
  {
    "objectID": "posts/2021-06-25-spacy_ml_ner.html#introduction",
    "href": "posts/2021-06-25-spacy_ml_ner.html#introduction",
    "title": "How to train a Spacy NER Model",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nTL;DR Summary of the Blog\nHow do we create ML NER Model from no labeled data?:\n\nPrepare rules-bootstrapped training data from unlabeled corpus\n\nIf it is possible/easy to annotate directly, one can do that.\nHowever, if rules taggging is possible\n\nIn the Disease NER dataset example here\nthere is an opportunity to use a huge list of words to tag via rules first\nthen labeling becomes easier than labeling from scratch\n\n\nRules-boostrapped data is then reviewed/edited by human annotators\nStratify Split the human-reviewed data into train-dev-test at sentences level\nOptimize and Train one or more Spacy ML NER Models\nCompare and Evaluate the accuracy of the models\n\n\nNow, we can list the above steps with a DISEASE NER example …\n\nWe have a ncbi_disease dataset of 7295 sentences speaking of various entities of which disease entity is of focus for us. &gt; E.g.: “Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .”  &gt; adenomatous polyposis coli tumour is a DISEASE entity\nSource of this dataset: link\nFor the sake of the argument of this blog, we assume this dataset does not have labels.\nIn most real world datasets, we are not likely to have labeled data\n\n\n\nHence the below pipeline helps in building an ML model &gt; Unlabeled Sentences speaking of various diseases  &gt; Tag DISEASE NER via Rules using a huge list of disease words  &gt; Review/Edit the Rules-bootstrapped NER (tagging NER from scratch is a lot tougher)  &gt; Split the Data into train-dev-test &gt; Train an ML model on train and dev datasets and evaluate on unseen test dataset &gt; Evaluate & Compare the Rules Model (baseline) and Spacy ML NER models (built from spacy-small and roberta base)"
  },
  {
    "objectID": "posts/2021-06-25-spacy_ml_ner.html#prepare-rules-bootstrapped-data-from-unlabeled-data",
    "href": "posts/2021-06-25-spacy_ml_ner.html#prepare-rules-bootstrapped-data-from-unlabeled-data",
    "title": "How to train a Spacy NER Model",
    "section": "2. Prepare Rules-bootstrapped Data from unlabeled data",
    "text": "2. Prepare Rules-bootstrapped Data from unlabeled data\n\n2A. Loading the disease words from an external file\n\n\nCode\nimport json\nimport re\n\nwith open('./spacy_model_ner/diseases_ner.json','r') as f:\n    diseases_json = json.load(f)\n    \nlist_of_diseases = [each['disease'] for each in diseases_json if not re.search('[,]|test',each['disease'],re.I)]\n\nlist_of_diseases.extend(['tumor','tumour']) #adding some custom word\n\nlist_of_diseases[0:10]\n\n\n['Hemophilia',\n 'Hemophilia A',\n 'Hepatitis A',\n 'Abdominal Aortic Aneurysm',\n 'AAA',\n 'Alpha 1 Antitrypsin Deficiency',\n 'AAT',\n 'AATD',\n 'Scar Tissue',\n 'Abdominal Adhesions']\n\n\n\n\n2B. Convert Disease Words into Spacy Patterns\n\n\nCode\nimport spacy\nnlp = spacy.load('en_core_web_sm',disable=['ner']) #ner component is not needed; \n\ndef list_of_words_2_spacy_patterns(list_of_words,\n                                   nlp_model,\n                                   label_name\n                                  ):\n    spacy_patterns = []\n    for each_word in list_of_words:\n        sub_pattern_list = [] # [{\"ORTH\": user_text_entity_df.loc[each_pattern_index,'TEXT']}]\n        for token in  nlp_model(each_word.lower()):\n            if re.search('^\\W{1,}$',token.text):\n                sub_pattern_list.append({\"ORTH\": token.text,\"OP\":\"*\"})\n            else:\n                sub_pattern_list.append({\"LOWER\":token.text})\n        temp_dict = {\"label\": label_name,\n                     \"pattern\": sub_pattern_list}\n        spacy_patterns.append(temp_dict)\n    return spacy_patterns\n\ndisease_spacy_rules_patterns = list_of_words_2_spacy_patterns(list_of_diseases,\n                               nlp,\n                               \"DISEASE\"\n                              )\n\ndisease_spacy_rules_patterns[0:5]\n\n\n[{'label': 'DISEASE', 'pattern': [{'LOWER': 'hemophilia'}]},\n {'label': 'DISEASE', 'pattern': [{'LOWER': 'hemophilia'}, {'LOWER': 'a'}]},\n {'label': 'DISEASE', 'pattern': [{'LOWER': 'hepatitis'}, {'LOWER': 'a'}]},\n {'label': 'DISEASE',\n  'pattern': [{'LOWER': 'abdominal'},\n   {'LOWER': 'aortic'},\n   {'LOWER': 'aneurysm'}]},\n {'label': 'DISEASE', 'pattern': [{'LOWER': 'aaa'}]}]\n\n\n\n\n2C. Create Disease NER out of spacy patterns\n\n\nCode\ndef load_rules_nlp_model_from_spacy_patterns(spacy_patterns):\n    rules_nlp = spacy.load('en_core_web_sm',disable=['ner'])\n    rules_config = {\n        \"validate\": True,\n        \"overwrite_ents\": True,\n    }\n\n    disease_rules = rules_nlp.add_pipe(\"entity_ruler\", # invoke entity_ruler pipe \n                                       \"disease_rules\", # give a name to the pipe\n                                       config=rules_config)\n    disease_rules.add_patterns(spacy_patterns)\n    return rules_nlp\n\ndisease_ner_rules_nlp = load_rules_nlp_model_from_spacy_patterns(disease_spacy_rules_patterns)\n\nprint(\"The pipeline components are:\")\nprint(disease_ner_rules_nlp.pipe_names)\nprint(\"NER entities tracked are:\")\nprint(disease_ner_rules_nlp.pipe_labels['disease_rules'])\n\n\nThe pipeline components are:\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'disease_rules']\nNER entities tracked are:\n['DISEASE']\n\n\n\nWant to know more about creating rules NER?  Refer the below blog article learn_by_blogging/How_to_Leverage_Spacy_Rules_NER\n\n\n\n2D. Infer Spacy Rules NER as token-level results\n\n\nCode\nimport pandas as pd\n\n## The token-level results from the above model on 7.2K sentences looks like below\ntoken_level_rules_output_example = pd.read_csv('spacy_model_ner/token_level_tags_on_one_unlabeled_sentence.csv',index_col=False)\ntoken_level_rules_output_example = token_level_rules_output_example[['New_Sentence_Id','Token','Rules_Tag_BIO']]\ntoken_level_rules_output_example.columns = ['Ramdom_Sentence_Id','Token','Rules_Tag_BIO']\n\n\n\n\nCode\ntoken_level_rules_output_example\n\n\n\n\n\n\n\n\n\nRamdom_Sentence_Id\nToken\nRules_Tag_BIO\n\n\n\n\n0\ntr_0\nIdentification\nO\n\n\n1\ntr_0\nof\nO\n\n\n2\ntr_0\nAPC2\nO\n\n\n3\ntr_0\n,\nO\n\n\n4\ntr_0\na\nO\n\n\n5\ntr_0\nhomologue\nO\n\n\n6\ntr_0\nof\nO\n\n\n7\ntr_0\nthe\nO\n\n\n8\ntr_0\nadenomatous\nB-DISEASE\n\n\n9\ntr_0\npolyposis\nI-DISEASE\n\n\n10\ntr_0\ncoli\nI-DISEASE\n\n\n11\ntr_0\ntumour\nO\n\n\n12\ntr_0\nsuppressor\nO\n\n\n13\ntr_0\n.\nO\n\n\n14\ntr_0\n[SEP]\n[SEP]\n\n\n\n\n\n\n\n\nOfcourse, there are mistakes in this rules_ner output like in row #11 where tumour is not tagged I-DISEASE\nWe rectify the mistakes of rules by human annotion"
  },
  {
    "objectID": "posts/2021-06-25-spacy_ml_ner.html#human-review-of-the-rules-output",
    "href": "posts/2021-06-25-spacy_ml_ner.html#human-review-of-the-rules-output",
    "title": "How to train a Spacy NER Model",
    "section": "3. Human Review of the Rules Output",
    "text": "3. Human Review of the Rules Output\n\n3A. Edit token-level results in csv by human annotation/review\n\n\nCode\n## The token-level results from the above model on 7.2K sentences looks like below\ntoken_level_rules_plus_human_output_example = pd.read_csv('spacy_model_ner/token_level_tags_on_one_unlabeled_sentence_2.csv',index_col=False)\ntoken_level_rules_plus_human_output_example\n\n\n\n\n\n\n\n\n\nNew_Sentence_Id\nToken\nRules_Tag_BIO\nHuman_Annotated_Tag_BIO\n\n\n\n\n0\ntr_0\nIdentification\nO\nO\n\n\n1\ntr_0\nof\nO\nO\n\n\n2\ntr_0\nof\nO\nO\n\n\n3\ntr_0\nof\nO\nO\n\n\n4\ntr_0\nof\nO\nO\n\n\n5\ntr_0\nAPC2\nO\nO\n\n\n6\ntr_0\n,\nO\nO\n\n\n7\ntr_0\na\nO\nO\n\n\n8\ntr_0\nhomologue\nO\nO\n\n\n9\ntr_0\nthe\nO\nO\n\n\n10\ntr_0\nadenomatous\nB-DISEASE\nB-DISEASE\n\n\n11\ntr_0\npolyposis\nI-DISEASE\nI-DISEASE\n\n\n12\ntr_0\ncoli\nI-DISEASE\nI-DISEASE\n\n\n13\ntr_0\ntumour\nO\nI-DISEASE\n\n\n14\ntr_0\nsuppressor\nO\nO\n\n\n15\ntr_0\n.\nO\nO\n\n\n16\ntr_0\n[SEP]\n[SEP]\n[SEP]"
  },
  {
    "objectID": "posts/2021-06-25-spacy_ml_ner.html#stratify-split-the-data-based-on-human-annotations",
    "href": "posts/2021-06-25-spacy_ml_ner.html#stratify-split-the-data-based-on-human-annotations",
    "title": "How to train a Spacy NER Model",
    "section": "4. Stratify Split the Data based on human annotations",
    "text": "4. Stratify Split the Data based on human annotations\n\n\nCode\nsentence_level_count_df = pd.read_csv('spacy_model_ner/split_of_classes_sentence_level.csv',index_col=False)\nprint(f\"Total number of Sentences: {sum(sentence_level_count_df['Count'])}\")\nsentence_level_count_df\n\n\nTotal number of Sentences: 7295\n\n\n\n\n\n\n\n\n\nSentence_level_tags\nCount\n%_contribution\n\n\n\n\n0\nO\n3337\n46.0\n\n\n1\nB-DISEASE|I-DISEASE|O\n2698\n37.0\n\n\n2\nB-DISEASE|O\n1260\n17.0\n\n\n\n\n\n\n\nFrom the above table, we can infer that  - there are more multi-token diseases than single-token diseases - there are 3.3K sentences with only O as the token   We have to ensure all three splits - train, dev and test - have the same percentage of O, B-DISEASE|I-DISEASE|O and B-DISEASE|O\nAfter spliting into train-dev-test in a 80-10-10 split …\n\n\nCode\ntrain_count_df = pd.read_csv('spacy_model_ner/split_of_classes_sentence_level_train.csv',index_col=False)\nprint(f\"Total number of Train Sentences: {sum(train_count_df['Count'])}\")\ntrain_count_df\n\n\nTotal number of Train Sentences: 5836\n\n\n\n\n\n\n\n\n\nSentence_level_tags\nCount\n%_contribution\n\n\n\n\n0\nO\n2670\n46.0\n\n\n1\nB-DISEASE|I-DISEASE|O\n2158\n37.0\n\n\n2\nB-DISEASE|O\n1008\n17.0\n\n\n\n\n\n\n\n\n\nCode\ndev_count_df = pd.read_csv('spacy_model_ner/split_of_classes_sentence_level_dev.csv',index_col=False)\nprint(f\"Total number of Train Sentences: {sum(dev_count_df['Count'])}\")\ndev_count_df\n\n\nTotal number of Train Sentences: 729\n\n\n\n\n\n\n\n\n\nSentence_level_tags\nCount\n%_contribution\n\n\n\n\n0\nO\n333\n46.0\n\n\n1\nB-DISEASE|I-DISEASE|O\n270\n37.0\n\n\n2\nB-DISEASE|O\n126\n17.0\n\n\n\n\n\n\n\n\n\nCode\ntest_count_df = pd.read_csv('spacy_model_ner/split_of_classes_sentence_level_test.csv',index_col=False)\nprint(f\"Total number of Train Sentences: {sum(test_count_df['Count'])}\")\ntest_count_df\n\n\nTotal number of Train Sentences: 730\n\n\n\n\n\n\n\n\n\nSentence_level_tags\nCount\n%_contribution\n\n\n\n\n0\nO\n334\n46.0\n\n\n1\nB-DISEASE|I-DISEASE|O\n270\n37.0\n\n\n2\nB-DISEASE|O\n126\n17.0"
  },
  {
    "objectID": "posts/2021-06-25-spacy_ml_ner.html#train-ml-model",
    "href": "posts/2021-06-25-spacy_ml_ner.html#train-ml-model",
    "title": "How to train a Spacy NER Model",
    "section": "5. Train ML Model",
    "text": "5. Train ML Model\n\n5A. Convert token-level Results 2 Spacy Model-acceptable Conll Data\n\n\nCode\ndef convert_token_df_2_conll_string(token_tag_df,\n                                    token_column_name,\n                                    tag_column_name\n                                   ):\n    token_string = ''\n    for each in range(len(token_tag_df)):\n        if each %1000 == 0:\n            print(f\"{each} tokens processed\")\n        current_token_string = str(token_tag_df.loc[each,token_column_name])\n        current_tag_string = str(token_tag_df.loc[each,tag_column_name])\n        \n        if current_token_string !='[SEP]':\n            current_line = current_token_string + \"\\t\" + current_tag_string + \"\\n\"\n        else:\n            current_line = \"\\n\"\n        token_string = token_string + current_line\n    return token_string\n\n\n\n!tail -n 25 ../data/diease_ner/train_dev_test_split_conll_data/test_data.conll\n\ninvestigate O\nthe O\nrate    O\nof  O\nBRCA2   O\nmutation    O\nin  O\nsporadic    B-DISEASE\nbreast  I-DISEASE\ncancers I-DISEASE\nand O\nin  O\na   O\nset O\nof  O\ncell    O\nlines   O\nthat    O\nrepresent   O\ntwelve  O\nother   O\ntumour  B-DISEASE\ntypes   O\n.   O\n\n\n\n\n\n5B. Train a Spacy Small ML Model\nCLI command for Spacy Model Training:\n!python3 -m spacy train $CONFIG_DIR/original_spacy_small_ner_config.cfg \\\n--output $SPACY_SMALL_MODEL_DIR_GPU \\\n--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n--verbose \\\n-g 0\nThe output from the Spacy Model Training:\n[2022-05-24 07:09:39,410] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\nℹ Saving to output directory:\n../data/model_weights/spacy_small\nℹ Using GPU: 0\n\n=========================== Initializing pipeline ===========================\n[2022-05-24 07:09:41,789] [INFO] Set up nlp object from config\n[2022-05-24 07:09:41,797] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:09:41,798] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n[2022-05-24 07:09:41,798] [INFO] Pipeline: ['tok2vec', 'ner']\n[2022-05-24 07:09:41,803] [INFO] Created vocabulary\n[2022-05-24 07:09:41,804] [INFO] Finished initializing nlp object\n\n[2022-05-24 07:09:51,839] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n✔ Initialized pipeline\n\n============================= Training pipeline =============================\n[2022-05-24 07:09:51,852] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:09:51,853] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\nℹ Pipeline: ['tok2vec', 'ner']\nℹ Initial learn rate: 0.001\nE    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  ------------  --------  ------  ------  ------  ------\n  0       0          0.00     41.00    0.31    0.33    0.29    0.00\n  0     200        195.83   1820.96   41.44   55.56   33.05    0.41             \n  0     400        106.20   1131.87   47.27   66.58   36.64    0.47             \n  0     600         64.99    969.69   74.94   77.17   72.84    0.75             \n  0     800         87.66   1096.80   74.39   76.96   71.98    0.74             \n  1    1000         82.51   1134.93   77.53   79.28   75.86    0.78             \n  1    1200        113.61   1122.73   80.73   82.36   79.17    0.81             \n  1    1400        128.84   1178.08   84.40   86.10   82.76    0.84             \n...   \n 26    5400        287.07    591.02   85.96   87.04   84.91    0.86             \n 27    5600        382.16    540.78   86.21   87.56   84.91    0.86             \n 28    5800        406.03    615.57   86.29   86.67   85.92    0.86             \nEpoch 29:   0%|                                         | 0/200 [00:00&lt;?, ?it/s]✔ Saved pipeline to output directory\n../data/model_weights/spacy_small/model-last\nCommand for Evaluating the Model Results:\n!python3 -m spacy evaluate $SPACY_SMALL_MODEL_DIR_GPU/model-best $SPACY_DATA_DIR/test_data.spacy \\\n--output $SPACY_SMALL_MODEL_DIR_GPU/model-best/spacy_small_model_evaluation.json \\\n--gpu-id 0\nOutput of Evaluate Command :\nℹ Using GPU: 0\n\n================================== Results ==================================\n\nTOK     -    \nNER P   89.75\nNER R   82.81\nNER F   86.14\nSPEED   20752\n\n\n=============================== NER (per type) ===============================\n\n              P       R       F\nDISEASE   89.75   82.81   86.14\n\n✔ Saved results to\n../data/model_weights/spacy_small/model-best/spacy_small_model_evaluation.json\n\n\n5C. Train a Spacy Roberta Base ML Model\nCLI command for Spacy Model Training:\n!python3 -m spacy train $CONFIG_DIR/original_trf_config.cfg \\\n--output $SPACY_ROBERTA_MODEL_DIR_GPU \\\n--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n--verbose \\\n-g 0\nThe output from the Spacy Model Training:\n\n[2022-05-24 07:44:08,351] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n✔ Created output directory:\n../data/model_weights/spacy_roberta_base_\nℹ Saving to output directory:\n../data/model_weights/spacy_roberta_base_\nℹ Using GPU: 0\n\n=========================== Initializing pipeline ===========================\n[2022-05-24 07:44:11,169] [INFO] Set up nlp object from config\n[2022-05-24 07:44:11,178] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:44:11,180] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n[2022-05-24 07:44:11,180] [INFO] Pipeline: ['transformer', 'ner']\n[2022-05-24 07:44:11,184] [INFO] Created vocabulary\n[2022-05-24 07:44:11,185] [INFO] Finished initializing nlp object\n\n[2022-05-24 07:44:22,286] [INFO] Initialized pipeline components: ['transformer', 'ner']\n✔ Initialized pipeline\n\n============================= Training pipeline =============================\n[2022-05-24 07:44:22,298] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n[2022-05-24 07:44:22,299] [DEBUG] Loading corpus from path: ../data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\nℹ Pipeline: ['transformer', 'ner']\nℹ Initial learn rate: 0.0\nE    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  -------------  --------  ------  ------  ------  ------\n  0       0        4392.55    285.04    0.21    0.17    0.29    0.00\n  1     200      126102.97  33471.51   84.94   81.78   88.36    0.85\n  3     400        1782.34   2642.55   89.50   90.83   88.22    0.90\n  5     600        1123.23   1596.50   90.69   89.06   92.39    0.91\n...\n 27    2800         153.06    176.41   90.94   90.35   91.52    0.91\n 29    3000         103.80    128.69   90.86   88.98   92.82    0.91\n 30    3200         121.04    141.70   91.47   90.56   92.39    0.91\n 32    3400          90.05    116.95   90.51   89.93   91.09    0.91\n 34    3600         111.25    131.25   91.12   90.15   92.10    0.91\n 36    3800          79.87     82.69   90.30   89.66   90.95    0.90\n 38    4000          82.07     82.97   90.97   90.01   91.95    0.91\n✔ Saved pipeline to output directory\n../data/model_weights/spacy_roberta_base_/model-last\nCPU times: user 11.1 s, sys: 2.78 s, total: 13.9 s\nWall time: 22min 37s\n\nCommand for Evaluating the Model Results:\n!python3 -m spacy evaluate $SPACY_SMALL_MODEL_DIR_GPU/model-best $SPACY_DATA_DIR/test_data.spacy \\\n--output $SPACY_SMALL_MODEL_DIR_GPU/model-best/spacy_small_model_evaluation.json \\\n--gpu-id 0\nOutput of Evaluate Command :\nℹ Using GPU: 0\n\n================================== Results ==================================\n\nTOK     -    \nNER P   88.61\nNER R   90.26\nNER F   89.43\nSPEED   12020\n\n\n=============================== NER (per type) ===============================\n\n              P       R       F\nDISEASE   88.61   90.26   89.43\n\n✔ Saved results to\n../data/model_weights/spacy_roberta_base_/model-best/spacy_roberta_base_evaluation.json"
  },
  {
    "objectID": "posts/2021-06-25-spacy_ml_ner.html#evaluate-the-models",
    "href": "posts/2021-06-25-spacy_ml_ner.html#evaluate-the-models",
    "title": "How to train a Spacy NER Model",
    "section": "6. Evaluate the models",
    "text": "6. Evaluate the models\n\n6A. Comparing the entity-level F1-score of (1) Rules, (2) Spacy small and (3) Spacy Roberta-base model\n\nresults = pd.read_csv('spacy_model_ner/results_of_the_models.csv',index_col=0)\n\n\nresults\n\n\n\n\n\n\n\n\nPrecision\nRecall\nF1_Score\n\n\n\n\nRules_Model\n39.85\n2.21\n28.52\n\n\nSpacy_Small_Model\n89.75\n82.81\n86.14\n\n\nSpacy_Roberta_Base_Model\n88.61\n90.26\n89.43"
  },
  {
    "objectID": "posts/2021-06-25-spacy_ml_ner.html#conclusion",
    "href": "posts/2021-06-25-spacy_ml_ner.html#conclusion",
    "title": "How to train a Spacy NER Model",
    "section": "7. Conclusion",
    "text": "7. Conclusion\nIn this blog article, we have shown how to effectively build a NER model on unlabeled data.  - We have compared Rules NER, a Spacy Small NER and roberta-base NER models. - We found roberta_base model is having the highest F1 score of 89%.\n- We can also ensemble results of Spacy Small NER and Spacy Roberta Base NER models.\n- A digression from the scope of this article: There are umpteen good tools (paid mostly) aiding the annotation. Sometimes, for simple NER problem (like tagging only one entity like this Disease NER), even excel is good for annotation\nReferences: - https://spacy.io/api/cli#evaluate - If you would like to replicate the above results, refer to the DiseaseNER notebooks in this link"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-ML.html#key-learnings-from-intro-to-ml-course-in-kaggle-learn",
    "href": "posts/2022-03-04-Intro-to-ML.html#key-learnings-from-intro-to-ml-course-in-kaggle-learn",
    "title": "Understanding Machine Learning Fundamentals - from a coder’s viewpoint",
    "section": "Key Learnings from Intro to ML Course in Kaggle Learn",
    "text": "Key Learnings from Intro to ML Course in Kaggle Learn\n\nHow to train_test_split the data\nBriefly discussed the concept of underfitting and overfitting (Loss vs model complexity curve)\nHow to train a typical scikit-learn model like DecisionTreeRegressor or RandomForestRegressor\n\nboth need no scaling of continuous or discrete data;\nfor sklearn might have to convert categorical data into encoded values\n\nAfter finding out the best parameters, one should train with the identified hyperparameters on the whole data\n\n(so that model will learn a bit more from held out data too)"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-ML.html#key-learnings-from-intermediary-ml-course-in-kaggle-learn",
    "href": "posts/2022-03-04-Intro-to-ML.html#key-learnings-from-intermediary-ml-course-in-kaggle-learn",
    "title": "Understanding Machine Learning Fundamentals - from a coder’s viewpoint",
    "section": "Key Learnings from Intermediary ML Course in Kaggle Learn",
    "text": "Key Learnings from Intermediary ML Course in Kaggle Learn\n\nMissing Value Treatment:\n\nRemove the Null Rows OR Columns (by column meaning the whole feature containing the missing value)\nImpute (by some strategy like Mean, Median, some regression like KNN)\nImpute + Add a boolean variable for every column imputed (so as to make the model hopefully treat the imputed row differently)\nDo removing missing values help or imputing missing values help more for the model accuracy?\nOpinion shared by the Author: SimpleImputer works as effectively as a complex imputing algorithm when used inside sophisticated ML models\n\n\n\n\nCategorical Column Treatment:\n- `Drop Categorical columns` (worst approach)\n- `OrdinalEncoder` \n- `OneHotEncoder` (most cases, the best approach)\n\nLearnt the concept of “good_category_cols” and “bad_category_columns”  (if a particular class occurs new in the unseen dataset; handle_unknown argument in “OneHotEncoder” is possible)\nThink twice before applying onehot encoding because of “high cardinality columns”\n\n\n\n\nData Leakage\n\nAn example of Data Leakage:\n\nDoing Inputer before train_test_split. Validation data then would have “seen” training data\n\n\nExample 1 - Nike: \n\nObjective: How much shoelace material will be used?\nSituation: With the feature Leather used this month , the prediction accuracy is 98%+. Without this featur, the accuracy is just ~80%\nIsDataLeakage?: Depends !\n\n❌ Leather used this month is a bad feature if the number is populated during the month (which makes it not available to predict the amount of shoe lace material needed)\n✔️ Leather used this month is a okay feature to use if the number determined during the beginning of the month (making it available during predition time on unseen data)\n\n\n\nExample 2 - Nike: \n\nObjective: How much shoelace material will be used?\nSituation: Can we use the feature Leather order this month?\nIsDataLeakage? Most likely no, however ...\n\n❌ If Shoelaces ordered (our Target Variable) is determined first and then only Leather Ordered is planned,  then we won’t have Leather Ordered during the time of prediction of unseen data\n✔️ If Leather Ordered is determined before Shoelaces Ordered, then it is a useful feature\n\n\n\nExample 3 - Cryptocurrency: \n\nObjective: Predicting tomo’s crypto price with a error of &lt;$1\nSituation: Are the following features susceptible to leakage?\n\nCurrent price of Crypto\nChange in the price of crypto from 1 hour ago\nAvg Price of crypto in the largest 24 h0urs\nMacro-economic Features\nTweets in the last 24 hours\n\nIsDataLeakage? No, none of the features seem to cause leakage.\nHowever, more useful Target Variable Change in Price (pos/neg) the next day. If this can be consistently predicted higher, then it is a useful model\n\n\nExample 4 - Surgeon's Infection Rate Performance: \n\nObjective: How to predict if a patient who has undergone a surgery will get infected post surgery?\nSituation: How can information about each surgeon’s infection rate performance be carefully utilized while training?\n\nThe independent features are strictly data points collected until the surgery had taken place\nThe dependent variable - whether infected or not - should be post surgery measurement\n\nIsDataLeakage? Depends on the what are the features used.\n\nIf a surgeon’s infection rate is used as a feature while training the model (that predicts whether a patient will be infected post surgery), that will lead to data leakage"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-ML.html#key-learnings-from-feature-engineering-course-in-kaggle-learn",
    "href": "posts/2022-03-04-Intro-to-ML.html#key-learnings-from-feature-engineering-course-in-kaggle-learn",
    "title": "Understanding Machine Learning Fundamentals - from a coder’s viewpoint",
    "section": "Key Learnings from Feature Engineering Course in Kaggle Learn",
    "text": "Key Learnings from Feature Engineering Course in Kaggle Learn\n\nKey Topics of this course:\n\nMutual Information\nInventing New Features (like apparent temparature = {Air Temparature + Humidity + Wind Speed})\nSegmentation Features (using K-Means Clustering)\nVariance in the Dataset based features (using Principal Component Analysis)\nEncode (high cardinality) category variables using Target Encoding\n\nWhy Feature Engineering?\n\nTo improve model performance\nTo reduce computational complexity by combining many features into a few\nTo improve interpretability of results\n\nWherever the model cannot identify a proper relationship between a dependent and a particular independent variable, \n\nwe can engineer/transform 1 or more of the independent variables\nso as to let model learn a better relationship between the engineered features and dependent variable\n\nE.g.: In compressive_strength prediction in cement data, synthetic feature - ratio of Water to Cement helps\n\n\nMutual Information\n\nMutual information is similar to correlation but correlation only looks for linear relationship whereas Mutual information can talk about any relationship\nMutual Information decribes relationship between two variables in terms of uncertainty (or certainty)\n\nFor e.g.: Knowing ExteriorQuality of a house (one of 4 values - Fair, Typical, Good and Excellent) can help one reduce uncertainty over SalePrice. Better the ExteriorQuality, more the SalesPrice\nTypical values: If two variables have a MI score of 0.0 - they are totally indepndent.\nMutual Information is a logarithmic quantity. So it increases slowly\nMutual Information is a univariate metric. MI can’t detect interactions between features Meaning, if multiple features together make sense to a dependent variable but not independently, then MI cannot determine that. Before deciding a feature is unimportant from its MI score, it’s good to investigate any possible interaction effects\n\nParallel Read for MI like metrics:\n\nFeature Importances from fitted attribute\n\nRecursive Feature Elimination\n\n\n\n\nTypes of New Features:\n\nMathematical Transformations (Ratio, Log)\nGrouping Columns Features - df['New_Group_Feature'] = df[list_of_boolean_features].sum(axis=1) - df['New_Group_Feature'] = df[list_of_numerical_features].gt(0).sum(axis=1) gt – greater than\nGrouping numerical Rows Features - customer['Avg_Income_by_State'] = customer.groupby('State')['Income'].transform('mean')\nGrouping categorical columns features - customer['StateFreq'] = customer.groupby('State')['State'].transform('count')/customer.State.count()\nSplit Features - df[[‘Type’, ‘Count’]] = df[‘some_var’].str.split(” “,expand=True)\nCombine Features - df['new_feture'] = df['var1'] + \"_\" + df['var2']\n\n\n\nUseful Tips on Feature Engineering:\n\nLinear models learn sum and differences naturally\nNeural Networks work better with scaled features\nRatios are difficult for many models, so can yeild better results when incorporated as additional feature\nTree models do not have the ability to factor in cound feature\nClustering as feature discovery tool (add a categorical feature based on clustering of a subset of features)\n\n\n\nPrincipal Component Analysis\n\nPCA is like partitioning of the variation in data\nInstead of describing the data with the original features,\nyou do an orthogonl transformation of the features and compute “principal components”\nwhich are used to explain the variation in the data.\nConvert the correlated variables into mutually orthogonal (uncorrelated) principal components\nPrincipal components can be more informative than the original features\nAdvantages of PCA: - Dimensionality Reducton - Anamoly Detection - Boosting signal to noise ratio - Decorrelation\nPCA works only for numeric variables; works best for scaled data\n\nPipeline for PCA: original_features –&gt; Scaled_features –&gt; PCA Features –&gt; MI_computed_on_PCA_features\n\n\n\nTarget Encoding\n\nTarget Encoding: A Supervised Feature Engineering technique for encoding categorical variables by including the target labels\nTarget Encoding is basically assigning a number to a categorical variable where in the number is derived from target variable - autos['target_encoded_make'] =  autos.groupby('make')['Price'].transform('mean')\nDisadvantages of Target Encoding: - Overfits for low volume (rare) classes - What if there are missing values\n\nWhere is Target Encoding most suitable? - For High cardinality features - Domain-motivated features (features that could have been scored poorly using feature importance metric function, we can unearth its real usefulness using target encoding)"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-ML.html#ml-coding-tips",
    "href": "posts/2022-03-04-Intro-to-ML.html#ml-coding-tips",
    "title": "Understanding Machine Learning Fundamentals - from a coder’s viewpoint",
    "section": "ML Coding Tips",
    "text": "ML Coding Tips\n\nsklearn:\n\nPipleine: Bundles together preprocessing and modeling steps | makes codebase easier for productionalizing\nColumnTransformer: Bundles together different preprocessing steps\nSklearn classes used often:\nModel\n\nfrom sklearn.tree import DecisionTreeRegressor AND from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(my_pipeline, X, y, scorung=‘neg_mean_absolute_error’)\n\nfrom xgboost import XGBRegressor\n\nn_estimators: Number of estimators is same as the number of cycles the data is processed by the model (100-1000)\nearly_stopping_rounds: Early stopping stops the iteration when the validation score stops improving\nlearning_rate - xgboost_model = XGBRegressor(n_estimators=500) - xgboost_model.fit(X_train,y_train,early_stopping_rounds=5,eval_set=[(X_valid, y_valid)], verbose=False)\n\n\nPreprocessing & Feature Engineering\n\nfrom sklearn.feature_selection import mutual_info_regression, mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer, KNNImputer\n`from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\n\nPandas:\n\nX = df.copy() y = X.pop(‘TheDependentVariable’) # remove the dependent variable from the X (features) and save in y)\ndf[encoded_colname], unique_values = df[colname].factorize() # for converting a categorical list of values into encoded numbers using pandas\ndf[list_of_oh_encoded_col_name_values] = pd.get_dummies(df[colname]) # for converting a categorical variable into a list of oh-encoded-values using pandas\nExclude all categorical columns at once:\n\ndf = df.select_dtypes(exclude=['object'])\n\nCreating a new column just to show the model if which row of a particular column have null values\n\ndf[col + ’__ismissing’] = df[col].isnull()\n\nIsolate all categorical columns:\n\nobject_cols = [col for col in df.columns if df[col].dtype == \"object\"]\n\nSegregate good and bad object columns (defined by the presence of “unknown” or new categories in validation or test dataset)\n\ngood_object_cols = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col])]\nbad_object_cols = list(set(good_object_cols) - set(bad_object_cols))\n\nGetting number of unique entries (cardinality) across object or categorical columns\n\nnum_of_uniques_in_object_cols = list(map(lambda col: df[col].nunique(), object_cols))\n\nsorted(list(zip(object_cols, num_of_uniques_in_object_cols)), key=lambda x: x[1], reverse=True)\n\n\n\nSource:  - Kaggle.com/learn"
  },
  {
    "objectID": "posts/2022-12-09-Encoding-Decoding.html",
    "href": "posts/2022-12-09-Encoding-Decoding.html",
    "title": "Demystifying the basics of Encoding and Decoding in Python",
    "section": "",
    "text": "What is a Unicode?\n\nUnicode is a unique number for every character irrespective of the spoken language (JP, En, Fr, etc.,) they come from or the programming language (Python, Java, etc.,) they are used in\n\n\nWhat is the purpose of Unicode?\n\nThere are innumerable number of languages in this world. Some follow the Latin writing system (English, French, Spanish, etc.,), and there are so many non-latin writing styles when we look at Asian languages. Unicodes are unique numerical representations for each character of the known, major languages in the world.\n\n\nThe uniqueness of the unicodes help in transmission of information in digial channels\n\nAgain, What is encoding and decoding, if you wonder\n\nComputers transmit information in bytes. Encoding is the process of converting unicodes to bytes\nDecoding is the process of converting bytes back to unicodes so humans can interpret\n\nWhat is Unicode Character Set (UCS)\n\nFor all major languages in the world, every unique character is assigned a unique value or “code point”. This set of unique values, also representing emojis and other symbols, is the Unicode Character Set. Unicode includes characters from Latin, Greek, Cyrillic, Arabic, Hebrew, Chinese, Japanese, Korean, and many others.\nCode points are typically represented in hexadecimal format, such as U+0041 for the Latin capital letter “A” or U+30A2 for the Japanese hiragana character “ア”.\n\nWhat are some of the commonly used Encoding techniques\n\nEncoding Table\n\n\n\n\n\n\n\n\nEncoding Type\nFull Description\nNum of bits\nWhere Used/Supported Character set\n\n\n\n\nASCII\nAmerican Standard Code for Information Interchange\n7 bits\nFor English text/ supports basic Latin letters, numbers and punctuation marks\n\n\nUTF-8\nUnicode Transformation Format\nvariable-length min 8 bits\nCan support multiple languages; 8 bits for most ASCII characters; Supports upto 32 bits for some characters\n\n\nUTF-16\nUnicode Transformation Format\nvariable-length min 16 bits\nCommonly used for applications which require multi-lang support\n\n\nLatin-1\nISO-8859-1 or Western European Encoding\n8 bits\nLimited to Western European languages and does not cover entire unicode characters set\n\n\nUTF-32\nUnicode Transformation Format\nfixed-length 32 bits\nProvides direct mapping between unicodes and characters; Less commonly used; High Storage"
  },
  {
    "objectID": "posts/2022-12-09-Encoding-Decoding.html#basics-of-encoding-and-decoding",
    "href": "posts/2022-12-09-Encoding-Decoding.html#basics-of-encoding-and-decoding",
    "title": "Demystifying the basics of Encoding and Decoding in Python",
    "section": "",
    "text": "What is a Unicode?\n\nUnicode is a unique number for every character irrespective of the spoken language (JP, En, Fr, etc.,) they come from or the programming language (Python, Java, etc.,) they are used in\n\n\nWhat is the purpose of Unicode?\n\nThere are innumerable number of languages in this world. Some follow the Latin writing system (English, French, Spanish, etc.,), and there are so many non-latin writing styles when we look at Asian languages. Unicodes are unique numerical representations for each character of the known, major languages in the world.\n\n\nThe uniqueness of the unicodes help in transmission of information in digial channels\n\nAgain, What is encoding and decoding, if you wonder\n\nComputers transmit information in bytes. Encoding is the process of converting unicodes to bytes\nDecoding is the process of converting bytes back to unicodes so humans can interpret\n\nWhat is Unicode Character Set (UCS)\n\nFor all major languages in the world, every unique character is assigned a unique value or “code point”. This set of unique values, also representing emojis and other symbols, is the Unicode Character Set. Unicode includes characters from Latin, Greek, Cyrillic, Arabic, Hebrew, Chinese, Japanese, Korean, and many others.\nCode points are typically represented in hexadecimal format, such as U+0041 for the Latin capital letter “A” or U+30A2 for the Japanese hiragana character “ア”.\n\nWhat are some of the commonly used Encoding techniques\n\nEncoding Table\n\n\n\n\n\n\n\n\nEncoding Type\nFull Description\nNum of bits\nWhere Used/Supported Character set\n\n\n\n\nASCII\nAmerican Standard Code for Information Interchange\n7 bits\nFor English text/ supports basic Latin letters, numbers and punctuation marks\n\n\nUTF-8\nUnicode Transformation Format\nvariable-length min 8 bits\nCan support multiple languages; 8 bits for most ASCII characters; Supports upto 32 bits for some characters\n\n\nUTF-16\nUnicode Transformation Format\nvariable-length min 16 bits\nCommonly used for applications which require multi-lang support\n\n\nLatin-1\nISO-8859-1 or Western European Encoding\n8 bits\nLimited to Western European languages and does not cover entire unicode characters set\n\n\nUTF-32\nUnicode Transformation Format\nfixed-length 32 bits\nProvides direct mapping between unicodes and characters; Less commonly used; High Storage"
  },
  {
    "objectID": "posts/2022-12-09-Encoding-Decoding.html#encoding-and-decoding-strings-in-python",
    "href": "posts/2022-12-09-Encoding-Decoding.html#encoding-and-decoding-strings-in-python",
    "title": "Demystifying the basics of Encoding and Decoding in Python",
    "section": "Encoding and Decoding Strings in Python",
    "text": "Encoding and Decoding Strings in Python\n\nIn Python, all strings by default are Unicode strings\nIf it is unicode, computer reads it by “encoding” into a byte string\nBy default, Python uses utf-8 encoding. You can also encode in utf-16\n\nbyte_string = \"センティル・クマール\".encode()\nbyte_string\nb'\\xe3\\x82\\xbb\\xe3\\x83\\xb3\\xe3\\x83\\x86\\xe3\\x82\\xa3\\xe3\\x83\\xab\\xe3\\x83\\xbb\\xe3\\x82\\xaf\\xe3\\x83\\x9e\\xe3\\x83\\xbc\\xe3\\x83\\xab'\nbyte_string_utf16 = \"センティル・クマール\".encode('utf-16')\nbyte_string_utf16\nb'\\xff\\xfe\\xbb0\\xf30\\xc60\\xa30\\xeb0\\xfb0\\xaf0\\xde0\\xfc0\\xeb0'\nprint(byte_string.decode())\nprint(byte_string_utf16.decode('utf-16'))\nセンティル・クマール\nセンティル・クマール\n\nAbout Byte Strings in Python\n\nByte strings are used to represent binary data, such as images, audio files, or serialized objects. Binary data is not directly representable as text and needs to be stored and processed as a sequence of bytes.\n\n&gt;&gt; type(byte_string)\nbytes\n\nIt is possible to save the byte strings directly in python using the prefix “b”\n\n&gt;&gt; forced_byte_string = b\"some_string\"\n&gt;&gt; type(forced_byte_string)\nbytes\n\nIt is NOT possible to save Non-ASCII characters as byte strings\n\nforced_byte_string = b\"センティル・クマール\"\nSyntaxError: bytes can only contain ASCII literal characters.\n\nOne example of using byte strings is when we serialize objects (such as python objects) using pickle module\n\n\nimport pickle\nan_example_dict = {\n  \"English\": \"Senthil Kumar\", \n  \"Japanese\": \"センティル・クマール\",\n  \"Chinese\": \"森蒂尔·库马尔\",\n  \"Korean\": \"센틸 쿠마르\",\n  \"Arabic\": \"سينتيل كومار\",\n  \"Urdu\": \"سینتھل کمار\"\n}\n\nserialized_data = pickle.dumps(an_example_dict)\nprint(type(serialized_data))\n\nwith open(\"serialized_dict.pkl\", \"wb\") as file:\n    file.write(serialized_data)\nbytes"
  },
  {
    "objectID": "posts/2022-12-09-Encoding-Decoding.html#encoding-and-decoding-files-in-python",
    "href": "posts/2022-12-09-Encoding-Decoding.html#encoding-and-decoding-files-in-python",
    "title": "Demystifying the basics of Encoding and Decoding in Python",
    "section": "Encoding and Decoding Files in Python",
    "text": "Encoding and Decoding Files in Python\n\nSaving Text Files in ASCII and UTF Formats\n\nThe below code will throw NO error, because it is a English only text\n\nnormal_text = 'Hot: Microsoft Surface Pro 4 Tablet Intel Core i7 8GB RAM 256GB.. now Pound 1079.00! #SpyPrice #Microsoft'\nwith open(\"saving_eng__only_text.txt\",\"w\",encoding=\"ascii\") as f:\n    f.write(normal_text)\n\n\nThe below code will throw an error, because you have latin character “£”\n\nnon_ascii_text = 'Hot: Microsoft Surface Pro 4 Tablet Intel Core i7 8GB RAM 256GB.. now £1079.00! #SpyPrice #Microsoft'with open(\"saving_eng__only_text.txt\",\"w\",encoding=\"ascii\") as f:\n    f.write(non_ascii_text)\n---------------------------------------------------------------------------\nUnicodeEncodeError                        Traceback (most recent call last)\nInput In [21], in &lt;cell line: 1&gt;()\n      1 with open(\"saving_a_latin_string.txt\",\"w\",encoding=\"ascii\") as f:\n----&gt; 2     f.write(non_ascii_text)\n\nUnicodeEncodeError: 'ascii' codec can't encode character '\\xa3' in position 70: ordinal not in range(128)\n\nChanging the encoding to “utf-8” fixed the error\n\nwith open(\"saving_a_latin_string.txt\",\"w\",encoding=\"utf-8\") as f:\n    f.write(non_ascii_text )\n\n\n\nSaving Non-ASCII JSON Files in different formats\n\nSaving a dict using json.dump, utf-8 encoding\nSaving the same dict as a json_string using json.dumps, utf-8 encoding\nSaving the same dict using json.dump, utf-16 encoding\n\nimport json\nan_example_dict = {\n  \"English\": \"Senthil Kumar\", \n  \"Japanese\": \"センティル・クマール\",\n  \"Chinese\": \"森蒂尔·库马尔\",\n  \"Korean\": \"센틸 쿠마르\",\n  \"Arabic\": \"سينتيل كومار\",\n  \"Urdu\": \"سینتھل کمار\"\n}\n\nwith open(\"saving_the_names_dict_utf8.json\",\"w\",encoding=\"utf-8\") as f:\n    json.dump(an_example_dict, f,ensure_ascii=False)\n\nan_example_dict_str = json.dumps(an_example_dict,ensure_ascii=False)\nwith open(\"saving_the_names_dict_utf8_using_json_string.json\",\"w\",encoding=\"utf-8\") as f:\n    f.write(an_example_dict_str)\n    \nwith open(\"saving_the_names_dict_utf16.json\",\"w\",encoding=\"utf-16\") as f:\n    json.dump(an_example_dict, f,ensure_ascii=False)\n\nHow to load the dict?\n\nwith open(\"saving_the_names_dict_utf8.json\",\"r\",encoding=\"utf-8\") as f:\n    loaded_dict = json.load(f)\n\nprint(loaded_dict)\n{'English': 'Senthil Kumar', 'Japanese': 'センティル・クマール', 'Chinese': '森蒂尔·库马尔', 'Korean': '센틸 쿠마르', 'Arabic': 'سينتيل كومار', 'Urdu': 'سینتھل کمار'}\n&gt;&gt; cat saving_the_names_dict_utf8.json\n{\"English\": \"Senthil Kumar\", \"Japanese\": \"センティル・クマール\", \"Chinese\": \"森蒂尔·库马尔\", \"Korean\": \"센틸 쿠마르\", \"Arabic\": \"سينتيل كومار\", \"Urdu\": \"سینتھل کمار\"}\n&gt;&gt; echo \"the file size:\" && du -hs saving_the_names_dict.json\necho \"the utf8 file size in bytes:\" && wc -c saving_the_names_dict_utf8.json \necho \"the utf8 file size in bytes:\" && wc -c saving_the_names_dict_utf8_using_json_string.json\necho \"the utf16 file size in bytes:\" && wc -c saving_the_names_dict_utf16.json\n\nthe utf8 file size in bytes:\n     209 saving_the_names_dict_utf8.json\nthe utf8 file size in bytes:\n     209 saving_the_names_dict_utf8_using_json_string.json\nthe utf16 file size in bytes:\n     292 saving_the_names_dict_utf16.json\nConclusion: - In the example above, the byte size of utf16 file is more than that of utf8 file"
  },
  {
    "objectID": "posts/2022-12-09-Encoding-Decoding.html#conclusion",
    "href": "posts/2022-12-09-Encoding-Decoding.html#conclusion",
    "title": "Demystifying the basics of Encoding and Decoding in Python",
    "section": "Conclusion",
    "text": "Conclusion\n\nUse utf8 everywhere | check more here\n\nUTF-8 can be used to encode anything that UTF-16 can. So most of the usecases can be met with utf-8.\nUTF-16 starts with a minimum of 2 bytes (16-bits) and hence not compatible with 7 bit ASCII. But UTF-8 is backwards compatible with ASCII."
  },
  {
    "objectID": "posts/2022-12-09-Encoding-Decoding.html#good-sources",
    "href": "posts/2022-12-09-Encoding-Decoding.html#good-sources",
    "title": "Demystifying the basics of Encoding and Decoding in Python",
    "section": "Good Sources",
    "text": "Good Sources\n\nWhy UTF-8 should be used?\n\nhttps://stackoverflow.com/a/18231475\nhttp://utf8everywhere.org/\n\nOther good resources\n\nEncoding-Decoding in Python 3 https://www.pythoncentral.io/encoding-and-decoding-strings-in-python-3-x/"
  },
  {
    "objectID": "posts/2021-08-28-pytorch_for_nlp_part_i.html",
    "href": "posts/2021-08-28-pytorch_for_nlp_part_i.html",
    "title": "PyTorch Fundamentals for NLP - Part 1",
    "section": "",
    "text": "Why NLP has grown in recent years? - Because of the improvement in the ability of Language Models (such as BERT or GPT-3) to accurately understand human language - Easy to train these LMs as they learn from performing unsupervised pretraining tasks\nWhat are the common types of NLP Applications for which NNs are built? - Text Classification | E.g.: Email Spam classification, Intent Classification of incomming messages in Chatbots - Sentiment Analysis | A regression task (outputs a number from most negative -1 to most positive +1 | Note: Training data needs to have outputs in range too) - NER | a component of Information Retrieval | We classify every token (typically tokens that are proper nouns) a pre-defined entity which is then used for some downstream - NER and Intent Classification can be used together with intent classification - E.g.: “Ok Google, Search apartments in Thoraipakam” - Intent: Search | Entity_1 (search_entity) apartments | Entity_2 (search_filter_location) Thoraipakkam - Text Summarization - Question-Answer Systems | Typicall Closed domain system where in the answer to a question is in the context - Context: “Joe Biden became US President in 2021 succedding Donald Trump” - Query: “Who was the President of the US before Joe Biden”\nIn this blog piece, let us cover - text classification task using a bow based vectorizer + nn.Linear layer"
  },
  {
    "objectID": "posts/2021-08-28-pytorch_for_nlp_part_i.html#introduction",
    "href": "posts/2021-08-28-pytorch_for_nlp_part_i.html#introduction",
    "title": "PyTorch Fundamentals for NLP - Part 1",
    "section": "",
    "text": "Why NLP has grown in recent years? - Because of the improvement in the ability of Language Models (such as BERT or GPT-3) to accurately understand human language - Easy to train these LMs as they learn from performing unsupervised pretraining tasks\nWhat are the common types of NLP Applications for which NNs are built? - Text Classification | E.g.: Email Spam classification, Intent Classification of incomming messages in Chatbots - Sentiment Analysis | A regression task (outputs a number from most negative -1 to most positive +1 | Note: Training data needs to have outputs in range too) - NER | a component of Information Retrieval | We classify every token (typically tokens that are proper nouns) a pre-defined entity which is then used for some downstream - NER and Intent Classification can be used together with intent classification - E.g.: “Ok Google, Search apartments in Thoraipakam” - Intent: Search | Entity_1 (search_entity) apartments | Entity_2 (search_filter_location) Thoraipakkam - Text Summarization - Question-Answer Systems | Typicall Closed domain system where in the answer to a question is in the context - Context: “Joe Biden became US President in 2021 succedding Donald Trump” - Query: “Who was the President of the US before Joe Biden”\nIn this blog piece, let us cover - text classification task using a bow based vectorizer + nn.Linear layer"
  },
  {
    "objectID": "posts/2021-08-28-pytorch_for_nlp_part_i.html#representing-text-as-tensors---a-quick-introduction",
    "href": "posts/2021-08-28-pytorch_for_nlp_part_i.html#representing-text-as-tensors---a-quick-introduction",
    "title": "PyTorch Fundamentals for NLP - Part 1",
    "section": "2.Representing Text as Tensors - A Quick Introduction",
    "text": "2.Representing Text as Tensors - A Quick Introduction\nHow do computers represent text? - Using encodings such as ASCII values to represent each character\n\nSource: github.com/MicrosoftDocs/pytorchfundamentals\n\nStill computers cannot interpret the meaning of the words , they just represent text as ascii numbers in the above image\n\nHow is text converted into embeddings? \n\nTwo types of representations to convert text into numbers\n\nCharacter-level representation\nWord-level representation\nToken or sub-word level representation\n\nWhile Character-level and Word-level representations are self explanatory, Token-level representation is a combination of the above two approaches.\n\nSome important terms: \n\nTokenization (sentence/text –&gt; tokens): In the case sub-word level representations, for example, unfriendly will be tokenized as un, #friend, #ly where # indicates the token is a continuation of previous token.\nThis way of tokenization can make the model learnt/trained representations for friend and unfriendly to be closer to each other in the vector spacy\nNumericalization (tokens –&gt; numericals): This is the step where we convert tokens into integers.\nVectorization (numericals –&gt; vectors): This is the process of creating vectors (typically sparse and equal to the length of the vocabulary of the corpus analyzed)\nEmbedding (numericals –&gt; embeddings): For text data, embedding is a lower dimensional equivalent of a higher dimensional sparse vector. Embeddings are typically dense. Vectors are sparse.\n\n\nTypical Process of Embedding Creation  - text_data &gt;&gt; tokens &gt;&gt; numericals &gt;&gt; sparse vectors or dense embeddings"
  },
  {
    "objectID": "posts/2021-08-28-pytorch_for_nlp_part_i.html#a-text-classification-pipeline-to-build-bow-classifier",
    "href": "posts/2021-08-28-pytorch_for_nlp_part_i.html#a-text-classification-pipeline-to-build-bow-classifier",
    "title": "PyTorch Fundamentals for NLP - Part 1",
    "section": "3. A Text Classification Pipeline to build BoW Classifier",
    "text": "3. A Text Classification Pipeline to build BoW Classifier\n\nDataset considered: AG_NEWS dataset that consists of 4 classes - World, Sports, Business and Sci/Tech\n\n┣━━ 1.Loading dataset  ┃ ┣━━ torch.data.utils.datasets.AG_NEWS  ┣━━ 2.Load Tokenization  ┃ ┣━━ torchtext.data.utils.get_tokenizer('basic_english')  ┣━━ 3.Build vocabulary  ┃ ┣━━ torchtext.vocab.build_vocab_from_iterator(train_iterator)  ┣━━ 4.Create BoW supporting functions ┃ ┣━━ Convert text_2_BoW_vector  ┃ ┣━━ Create collate_fn to create a pair of label-feature tensors for every minibatch  ┣━━ 5.Create train, validation and test DataLoaders ┣━━ 6.Define Model_Architecture ┣━━ 7.define training_loop and testing_loop functions ┣━━ 8.Train the model and Evaluate on Test Data ┣━━ 9.Test the model on sample text\nImporting basic modules\n\n\nCode\nimport torch\nimport torchtext\nimport os\nimport collections\nimport random\nimport numpy as np\n\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\n\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n3.1. Loading dataset\n\n\nCode\ndef load_dataset(ngrams=1):\n    print(\"Loading dataset ...\")\n    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n    train_dataset = list(train_dataset)\n    test_dataset = list(test_dataset)\n    return train_dataset, test_dataset\n\n\n\ntrain_dataset, test_dataset = load_dataset()\n\nLoading dataset ...\n\n\n\nclasses = ['World', 'Sports', 'Business', 'Sci/Tech']\n\n\n\n3.2. Loading Tokenizer\n\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n\n\n\n3.3. Building Vocabulary\n\n\nCode\ndef _yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n\ndef create_vocab(train_dataset):\n    print(\"Building vocabulary ..\")\n    vocab = build_vocab_from_iterator(_yield_tokens(train_dataset),\n                                      min_freq=1,\n                                      specials=['&lt;unk&gt;']\n                                     )\n    vocab.set_default_index(vocab['&lt;unk&gt;'])\n    return vocab\n\n\n\nvocab = create_vocab(train_dataset)\n\nBuilding vocabulary ..\n\n\n\nvocab_size = len(vocab)\nprint(\"Vocab size =\", vocab_size)\n\nVocab size = 95811\n\n\n\nvocab(['this', 'is', 'a', 'sports', 'article','&lt;unk&gt;'])\n\n[52, 21, 5, 262, 4229, 0]\n\n\nLooking at some sample data\n\nfor label, text in random.sample(train_dataset, 3):\n    print(label,classes[label-1])\n    print(text)\n    print(\"******\")\n\n1 World\nBurgers for the Health Professional Even as obesity and its consequences are increasingly taxing the health care system, fast food places are serving as hospital cafeterias.\n******\n4 Sci/Tech\nClimate Talks Bring Bush #39;s Policy to Fore  Glaciers in the Antarctic and in Greenland are melting much faster than expected, and the fastest moving glacier in the world has doubled its speed.\n******\n3 Business\nBush Health Savings Accounts Slow to Gain Acceptance So far employers and their workers have been slow to accept health savings accounts as an alternative to conventional health insurance.\n******\n\n\n\n\n3.4. Creating BoW related functions\n\nThe text pipeline purpose is to convert text into tokens\nthe label pipeline is to have labels from 0 to 3\n\n\n_text_pipeline = lambda x: vocab(tokenizer(x))\n_label_pipeline = lambda x: int(x) - 1\n\n\n_text_pipeline(\"this is a sports article\")\n\n[52, 21, 5, 262, 4229]\n\n\n\n_label_pipeline('3')\n\n2\n\n\nIn Bag of Words (BOW) representation,  - each word is linked to a vector index - where the vector value in that index is the frequency of occurrence of the word in the given document\n\nSource: Microsoft Docs\n\n3.4.1 Creating text_2_bow_vector\n\ndef to_bow(text,\n           bow_vocab_size=vocab_size\n          ):\n    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n    for i in _text_pipeline(text):\n        if i&lt;bow_vocab_size:\n            res[i] += 1\n    return res\n\nprint(f\"sample text:\\n{train_dataset[0][1]}\")\nprint(f\"\\nBoW vector:\\n{to_bow(train_dataset[0][1])}\")\n\nsample text:\nWall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n\nBoW vector:\ntensor([0., 2., 1.,  ..., 0., 0., 0.])\n\n\n\n\n3.4.2 Create Collate Function\n\n# the collate function\n# this collate function gets list of batch_size tuples, and needs to \n# return a pair of label-feature tensors for the whole minibatch\ndef bowify(b):\n    return (\n            torch.tensor([t[0]-1 for t in b],dtype=torch.float32),\n            torch.stack([to_bow(t[1]) for t in b])\n    )\n\n\n\n\n3.5. Prepare DataLoaders\n\nBATCH_SIZE = 4\n\n\nfrom torch.utils.data.dataset import random_split\n\nnum_train = int(len(train_dataset) * 0.95)\nsplit_train_, split_valid_ = \\\n    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n\n\ntrain_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=bowify)\nvalid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=bowify)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                             shuffle=True, collate_fn=bowify)\n\n\n\n3.6. Model Architecture\n\nfrom torch import nn\n\nclass BOW_TextClassification(nn.Module):\n    def __init__(self, vocab_size):\n        # initialize the layers in the __init__ constructor\n        super(BOW_TextClassification,self).__init__()\n        # supercharge the sub-class by inheriting the defaults from parent class\n        self.simple_linear_stack = torch.nn.Sequential(\n            torch.nn.Linear(vocab_size,4),\n            # torch.nn.Tanh(),\n            # torch.nn.Linear(512,4), # 4 denotes the number of classes\n            )\n        \n    def forward(self,features):\n        softmax_values = self.simple_linear_stack(features)\n        return softmax_values\n\nbow_model = BOW_TextClassification(vocab_size).to(device)        \n\n\nprint(bow_model)\n\nBOW_TextClassification(\n  (simple_linear_stack): Sequential(\n    (0): Linear(in_features=95811, out_features=4, bias=True)\n  )\n)\n\n\n\n\n3.7. Define train_loop and test_loop functions\n\n\nCode\n# setting hyperparameters\nlr = 0.01\noptimizer = torch.optim.Adam(bow_model.parameters(), lr=lr)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nepoch_size = 1 # just for checking how much time it takes\n\n\n\n# number of training batches\nlen(train_dataloader)\n\n28500\n\n\n\npred.get_device()\n\n0\n\n\n\ndef train_loop(bow_model, \n               train_dataloader,\n               validation_dataloader,\n               epoch,\n               lr=lr,\n               optimizer=optimizer,\n               loss_fn=loss_fn,\n              ):\n    train_size = len(train_dataloader.dataset)\n    validation_size = len(validation_dataloader.dataset)\n    training_loss_per_epoch = 0\n    validation_loss_per_epoch = 0\n    for batch_number, (labels, features) in enumerate(train_dataloader):\n        if batch_number %100 == 0:\n            print(f\"In epoch {epoch}, training of {batch_number} batches are over\")\n        if batch_number == 100:\n            break\n        labels, features = labels.to(device), features.to(device)\n        labels = labels.clone().detach().requires_grad_(True).long().to(device)\n        # labels = torch.tensor(labels, dtype=torch.long, device=device)\n        # compute prediction and prediction error\n        pred = bow_model(features)\n        # print(pred.dtype, pred.shape)\n        loss = loss_fn(pred, labels)\n        # print(loss.dtype)\n        \n        # backpropagation steps\n        # key optimizer steps\n        # by default, gradients add up in PyTorch\n        # we zero out in every iteration\n        optimizer.zero_grad()\n        \n        # performs the gradient computation steps (across the DAG)\n        loss.backward()\n        \n        # adjust the weights\n        optimizer.step()\n        training_loss_per_epoch += loss.item()\n        \n    for batch_number, (labels, features) in enumerate(validation_dataloader):\n        if batch_number == 100:\n            break\n        labels, features = labels.to(device), features.to(device)\n        labels = labels.clone().detach().requires_grad_(True).long().to(device)\n        #labels, features = labels.to(device), features.to(device)\n        #labels = torch.tensor(labels, dtype=torch.float32)\n        # compute prediction error\n        pred = bow_model(features)\n        loss = loss_fn(pred, labels)\n        \n        validation_loss_per_epoch += loss.item()\n    \n    avg_training_loss = training_loss_per_epoch / train_size\n    avg_validation_loss = validation_loss_per_epoch / validation_size\n    print(f\"Average Training Loss of {epoch}: {avg_training_loss}\")\n    print(f\"Average Validation Loss of {epoch}: {avg_validation_loss}\")\n\n\ndef test_loop(bow_model,test_dataloader, epoch, loss_fn=loss_fn):\n    test_size = len(test_dataloader.dataset)\n    # Failing to do eval can yield inconsistent inference results\n    bow_model.eval()\n    bow_model.to(device)\n    test_loss_per_epoch, accuracy_per_epoch = 0, 0\n    # disabling gradient tracking while inference\n    with torch.no_grad():\n        for labels, features in test_dataloader:\n            labels, features = labels.to(device), features.to(device)\n            labels = labels.clone().detach().requires_grad_(True).long().to(device)\n            # labels = torch.tensor(labels, dtype=torch.long, device=device)\n            # labels = torch.tensor(labels, dtype=torch.float32)\n            pred = bow_model(features)\n            loss = loss_fn(pred, labels)\n            test_loss_per_epoch += loss.item()\n            accuracy_per_epoch += (pred.argmax(1)==labels).type(torch.float).sum().item()\n    print(f\"Average Test Loss of {epoch}: {test_loss_per_epoch/test_size}\")\n    print(f\"Average Accuracy of {epoch}: {accuracy_per_epoch/test_size}\")\n\n\n\n3.8 Training the Model\n\nepoch_size\n\n1\n\n\n\n%%time\n# it takes a lot of time to run this model\n# hence running only for 100 batches (of size 4) in 1 epoch\nfor epoch in range(epoch_size):\n    print(f\"Epoch Number: {epoch} \\n---------------------\")\n    train_loop(bow_model, \n               train_dataloader, \n               valid_dataloader,\n               epoch\n              )\n    test_loop(bow_model, \n              test_dataloader,\n              epoch)\n\nEpoch Number: 0 \n---------------------\nIn epoch 0, training of 0 batches are over\nIn epoch 0, training of 100 batches are over\nAverage Training Loss of 0: 0.0004964731066373357\nAverage Validation Loss of 0: 0.008571766301679114\nAverage Test Loss of 0: 0.12454833071194835\nAverage Accuracy of 0: 0.8268421052631579\nCPU times: user 3h 22min 19s, sys: 13.6 s, total: 3h 22min 32s\nWall time: 6min 14s\n\n\n\n\n3.9.Test the model on sample text\n\nag_news_label = {1: \"World\",\n                 2: \"Sports\",\n                 3: \"Business\",\n                 4: \"Sci/Tec\"}\n\ndef predict(text, model):\n    with torch.no_grad():\n        bow_vector = to_bow(text)\n        output = bow_model(bow_vector)\n        output_label = ag_news_label[output.argmax().item() + 1]\n        return output_label\n    \nsample_string = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n    enduring the season’s worst weather conditions on Sunday at The \\\n    Open on his way to a closing 75 at Royal Portrush, which \\\n    considering the wind and the rain was a respectable showing. \\\n    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n    was another story. With temperatures in the mid-80s and hardly any \\\n    wind, the Spaniard was 13 strokes better in a flawless round. \\\n    Thanks to his best putting performance on the PGA Tour, Rahm \\\n    finished with an 8-under 62 for a three-stroke lead, which \\\n    was even more impressive considering he’d never played the \\\n    front nine at TPC Southwind.\"\n\ncpu_model = bow_model.to(\"cpu\")\n\nprint(f\"This is a {predict(sample_string, model=cpu_model)} news\")\n\nThis is a Sports news"
  },
  {
    "objectID": "posts/2021-08-28-pytorch_for_nlp_part_i.html#conclusion",
    "href": "posts/2021-08-28-pytorch_for_nlp_part_i.html#conclusion",
    "title": "PyTorch Fundamentals for NLP - Part 1",
    "section": "4. Conclusion",
    "text": "4. Conclusion\n\nIn this blog piece, we looked at how bow vectorizer was used as input to build a shallow NN (without non-linear activation function) classification.\nIn the next parts to this Pytorch series, I will cover better ways to build a text classification NN model from scratch\n\nSources \n\nMSFT PyTorch NLP Course | link\nMSFT PyTorch Course - BoW Classifier | link\nTorchtext Tutorial on Text Classification | link"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html",
    "href": "posts/2023-12-14-taskfile.html",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "",
    "text": "Taskfile , written in Go and officially tagged as task runner, is giving us an intuitive and modular way to run any CLI command.\nI have been using Taskfiles to abstract away a long AWS CLI command or a few lines of bash script and execute them in the below fashion\n\ntask run_this_aws_command\ntask run_this_bash_script\n\nBasically, instead of running a command in a terminal, a repetitive task can be saved and run from a Taskfile.yml instead.\nRefer here if you would like to read from the official source. You could also learn how to use Go-Task from the simplified, personalised examples that I have listed below.\n\n\n\n\n\nWhenever we build an application or even a part of that part application, we have a long list of setup terminal/CLI commands to execute. Often, those terminal commands are written in markdown files (like Readme.md). We are made to copy-paste those commands from the markdown file to run.\nWhat if you run those commands grouped together as tasks in yaml file?\nGo-Task simplifies the execution of the terminal commands and the modular aspect of the tasks act as good code-cum-documentation too. You can add comments too in those tasks residing in the yaml file.\n\n\n\n\n\nWon’t believe me on the ease of use? Get ready to start using it after reading the below breezy 9 sections!\nThe 10th section could be a bit much! Sorry about that, it was purposefully intense as all real-world projects are. Still, I have attempted to get you the intent - so that you can create your own Taskfile magic for your real-world projects !\n\n\n\n\n\nOn a Linux or MacOS, you could do\n\nbrew install go-task/tap/go-task (other ways of installation include npm , chocolatey, etc., | refer here)\n\n\n\nAll the codes below are in this repo https://github.com/senthilkumarm1901/taskfile_tutorial"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#introduction",
    "href": "posts/2023-12-14-taskfile.html#introduction",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "",
    "text": "Taskfile , written in Go and officially tagged as task runner, is giving us an intuitive and modular way to run any CLI command.\nI have been using Taskfiles to abstract away a long AWS CLI command or a few lines of bash script and execute them in the below fashion\n\ntask run_this_aws_command\ntask run_this_bash_script\n\nBasically, instead of running a command in a terminal, a repetitive task can be saved and run from a Taskfile.yml instead.\nRefer here if you would like to read from the official source. You could also learn how to use Go-Task from the simplified, personalised examples that I have listed below.\n\n\n\n\n\nWhenever we build an application or even a part of that part application, we have a long list of setup terminal/CLI commands to execute. Often, those terminal commands are written in markdown files (like Readme.md). We are made to copy-paste those commands from the markdown file to run.\nWhat if you run those commands grouped together as tasks in yaml file?\nGo-Task simplifies the execution of the terminal commands and the modular aspect of the tasks act as good code-cum-documentation too. You can add comments too in those tasks residing in the yaml file.\n\n\n\n\n\nWon’t believe me on the ease of use? Get ready to start using it after reading the below breezy 9 sections!\nThe 10th section could be a bit much! Sorry about that, it was purposefully intense as all real-world projects are. Still, I have attempted to get you the intent - so that you can create your own Taskfile magic for your real-world projects !\n\n\n\n\n\nOn a Linux or MacOS, you could do\n\nbrew install go-task/tap/go-task (other ways of installation include npm , chocolatey, etc., | refer here)\n\n\n\nAll the codes below are in this repo https://github.com/senthilkumarm1901/taskfile_tutorial"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#starting-with-a-simple-taskfile",
    "href": "posts/2023-12-14-taskfile.html#starting-with-a-simple-taskfile",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "1. Starting with a Simple Taskfile",
    "text": "1. Starting with a Simple Taskfile\n\nA Taskfile yml consists of tasks attribute. The tasks in that attribute can be invoked in a terminal as task task_name\n\n% cat hello_world/Taskfile.yml\nversion: '3'\n\ntasks:\n    hello:\n        cmds:\n            - echo \"Hello World!\"\n        silent: false\n    hello_again:\n        cmds:\n            - echo \"Hello Again!\"\n        silent: true\n\nResult:\n\n\nNote: - We had seen task and version attributes in the Hello World example Taskfile.yml above. - We are going to see many other attributes in the below sections. Here is a Taskfile Schema for a quick look at the different attributes"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#calling-a-task-from-another-task",
    "href": "posts/2023-12-14-taskfile.html#calling-a-task-from-another-task",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "2. Calling a Task from Another Task",
    "text": "2. Calling a Task from Another Task\n\nThe below YAML file is from the official documentation where the tasks task-to-be-called and another-task is called inside main-task task\n\nversion: '3'\n\ntasks:\n  main-task:\n    cmds:\n      - task: task-to-be-called\n      - task: another-task\n      - echo \"Both done\"\n\n  task-to-be-called:\n    cmds:\n      - echo \"Task to be called\"\n\n  another-task:\n    cmds:\n      - echo \"Another task\"\n(also refer 3.1 section task run_same_task_multiple_times_with_different_variables)"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#environment-variables",
    "href": "posts/2023-12-14-taskfile.html#environment-variables",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "3. Environment Variables",
    "text": "3. Environment Variables\n\nEnvironment Variables are written under a attribute env\n\n\n3.1 Global Environment Variables\n\nYou can use the environment variables listed under env attribute using $ operator inside tasks\nThere is also a provision to source the environment variables from an external hidden file\nIn the below example, .env file is used under the section titled dotenv: ['.env']\n\nversion: '3'\n\nenv:\n  PARENT_DIR: ../../taskfile_tutorial\n  CURRENT_DIR:\n    sh: echo $PWD\n\ndotenv: ['.env']\n\ntasks:\n  list_directories_under_parent_dir:\n    cmds:\n      - find $PARENT_DIR -type d\n  \n  list_files_under_current_dir:\n    cmds:\n      - find $CURRENT_DIR -type f\n  \n  create_new_dir:\n    cmds:\n      - mkdir -p $DIR_TO_CREATE\n\nenvironment_variables % cat .env\nDIR_TO_CREATE=\"task_specific_env_variable\"\n\n\n\n3.2 Task-specific Environment Variables\n\nWhile in the above global environment varibales, I placed the env section just below the version section, in the below yaml file env section is a sub-section under the tasks\n\nversion: '3'\n\n\ntasks:\n  list_directories_under_parent_dir:\n    env:\n      DIR: ../../../taskfile_tutorial\n    cmds:\n      - find $DIR -type d\n  \n  list_files_under_current_dir:\n    env:\n      DIR: \n        sh: echo $PWD\n    cmds:\n      - find $DIR -type f\n  \n  create_new_dir:\n    cmds:\n      - # this $DIR is passed during run time `DIR=new_dir_name task create_new_dir`\n      - # the above line is an example of passing environment variable from outside the taskfile\n      - mkdir -p $DIR && ls ./\n\nResults:\n\nThe same result below would have come for Global variables as well. Ignored showing it to save space"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#variables",
    "href": "posts/2023-12-14-taskfile.html#variables",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "4. Variables",
    "text": "4. Variables\n\nVariables are written under the section vars\nYou can use the variables using {{.VARIABLE_NAME}} operator in tasks\nLike environment variables, the generic variables can be used both in global and task-specific way\n\n\n4.1 Global and Task-specific Variables\n\nIn the below example, the GLOBAL_VAR is defined globally and SOME_VAR is defined locally\n\nversion: '3'\n\nvars:\n  GLOBAL_VAR: Hello\n\ntasks:\n  run_same_task_multiple_times_with_different_variables:\n    cmds:\n      - task: length_of_word\n        vars: { SOME_VAR: 'case 1' }\n      - task: length_of_word\n        vars: { SOME_VAR: 'someother case 2' }\n        silent: true\n\n  length_of_word:\n    cmds:\n      - echo {{.SOME_VAR}} | wc -c \n    silent: true\n  \n  print-global-variable:\n    # both underscores and hyphens are accepted in the task name\n    cmds:\n      - echo {{.GLOBAL_VAR}}\n    silent: true\n\n\n\n\n4.2 Special Variables & Passing Arguments\n\nI have listed two special variables in the tasks ‘{{.USER_WORKING_DIR}}’ and ‘{{.CLI_ARGS}}’\nRefer here for other Special Variables that can be used: https://taskfile.dev/api/#special-variables\n\nversion: '3'\n\ntasks:\n  # introducing a special variable '{{.USER_WORKING_DIR}}'\n  count_files:\n    dir: '{{.USER_WORKING_DIR}}'\n    cmds:\n      - echo \"Number of files in {{.USER_WORKING_DIR}} \"\n      - find . -maxdepth 1 -type f | wc -l \n    silent: true\n\n  # introducing a special variable '{{.CLI_ARGS}}'\n  # introducing passing special arguments\n  # task count_only_txt_files -- '*.txt'\n  count_only_txt_files:\n    dir: '{{.USER_WORKING_DIR}}'\n    cmds:\n      - echo \"Number of txt files in {{.USER_WORKING_DIR}}\"\n      - find . -maxdepth 1 -type f -name '{{.CLI_ARGS}}' | wc -l \n    silent: true\nResults:\n\nNote: - Even though the Taskfile.yml is in the main directory special_variables, it works inside the subdirectory because of '{{.USER_WORKING_DIR}}'\n\n\n\n4.3 Passing Multiple Arguments\n  version: '3'\n  \n  # what if you need to pass two arguments used in two different commands\n  # there is still only the hacky of splitting by a delimiter\n  # since '{{.CLI_ARGS}}' in essense takes just 1 string\n  # how to invoke the below task: `task passing_multiple_arguments -- arg 1,arg 2`\n  passing_multiple_arguments:\n    cmds:\n      - echo 'First - {{(split \",\" .CLI_ARGS)._0}}'\n      - echo 'Second - {{(split \",\" .CLI_ARGS)._1}}'\n    silent: true\n\nResults:"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#a-global-taskfile",
    "href": "posts/2023-12-14-taskfile.html#a-global-taskfile",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "5. A Global Taskfile",
    "text": "5. A Global Taskfile\n\nThe below Taskfile needs to be placed in $HOME directory\nYou can then use the tasks using task -g or task --global command in any directory you want\n\nversion: '3'\n\ntasks:\n  count_files_inside_current_dir:\n    dir: \"{{.USER_WORKING_DIR}}\"\n    cmds:\n      - echo \"Number of files in {{.USER_WORKING_DIR}} \"\n      - ls -l | grep -v \"^d\" | wc -l\n    silent: true\n\n  search_files_inside_current_dir:\n    dir: \"{{.USER_WORKING_DIR}}\"\n    cmds:\n      - find {{.USER_WORKING_DIR}} -type f -name {{.CLI_ARGS}}\n    silent: true\n\n  search_all_files_except:\n    cmds:\n      - find {{.USER_WORKING_DIR}} -type f | grep -v {{.CLI_ARGS}}\n    silent: true%\nResults:"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#multiline-commands",
    "href": "posts/2023-12-14-taskfile.html#multiline-commands",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "6. Multiline Commands",
    "text": "6. Multiline Commands\n\nWhat if you do not wish to write a long line and want to split the lines?\nYou can use the pipe command | to write the long lin in multiple commands\n\nversion: '3'\n\ntasks:\n  multi-line-replace-command:\n    cmds: \n     - |\n        sed -e \"s|&lt;NAME&gt;|SENTHIL|g\" \\\n        -e \"s|&lt;EMAIL&gt;|senthilkumar.m1901@gmail.com|g\" \\\n        contact_details.txt \n    silent: true\n\n  will_not_work:\n    cmds:\n      - echo \"Running in different lines\"\n      - export A=\"some_value\"\n      - echo \"Value of A - $A\"\n    silent: true\n\n  will_work:\n    cmds:\n      - echo \"Running in the same line\"\n      - |\n        export A=\"some_value\"\n        echo \"Value of A - $A\"\n    silent: true\nResults:\n\n\nPlease note the presence of export A=\"some_value\" and echo \"Value of A - $A coming under the pipe command | in task will_work"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#should-you-always-name-the-file-taskfile.yml",
    "href": "posts/2023-12-14-taskfile.html#should-you-always-name-the-file-taskfile.yml",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "7. Should you always name the file Taskfile.yml ?",
    "text": "7. Should you always name the file Taskfile.yml ?\n\nYes and No.\n\nYes, because only from the supported file names such as [“Taskfile.yaml”, “Taskfile.dist.yaml”, “taskfile.yaml”, “taskfile.dist.yaml”], you can invoke a task as task task_name (note:both “yaml” and “yml” extensions are supported) | refer here\nNo, because you could include tasks in other files but they have to be “included” in one of the supported file names above.\nThe example from official documentation where they have used a different name is given below\n\n\nversion: '3'\n\nincludes:\n  docs: ./documentation # will look for ./documentation/Taskfile.yml\n  docker: ./DockerTasks.yml\n\nDid not get what I meant above? Refer section 8 below for a better example"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#multiple-taskfiles-in-1-taskfile-or-calling-a-task-from-another-taskfile",
    "href": "posts/2023-12-14-taskfile.html#multiple-taskfiles-in-1-taskfile-or-calling-a-task-from-another-taskfile",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "8. Multiple Taskfiles in 1 Taskfile OR (calling a task from another taskfile)",
    "text": "8. Multiple Taskfiles in 1 Taskfile OR (calling a task from another taskfile)\n# file #1\n# this file is named as Taskfile.yml\nversion: '3'\n\nincludes:\n  conda_tasks:\n    taskfile: ./CondaTasks.yml\n  docker_tasks:\n    taskfile: ./DockerTasks.yml\n  pipenv_tasks:\n    taskfile: ./PipenvTasks.yml\n\ntasks:\n  list_python_environments:\n    cmds:\n      - task conda_tasks:list_python_environments\n      - task docker_tasks:list_python_images\n      - task pipenv_tasks:list_locations_of_pipenv_environments\n    silent: true\n\n# file #2\n# this file is named as\n# CondaTasks.yml\nversion: '3'\n\ntasks:\n  list_python_environments:\n    cmds:\n      - echo \"The Python Conda Environments are\"\n      - conda info -e | awk -F' ' '{ print $1 }' | grep -E '[a-z_-]+'\n      - echo \"\"\n    silent: true\n\n# file #3\n# this file is named as\n# DockerTasks.yml\nversion: '3'\n\ntasks:\n  list_python_images:\n    cmds:\n      - echo \"The Python Docker images are\"\n      - docker image ls | grep -E \"python|ubuntu\" | awk '{ print $1, $2 }'\n      - echo \"\"\n    silent: true\n\n# file #4\n# this file is named as\n# PipenvTasks.yml\nversion: '3'\n\ntasks:\n  list_locations_of_pipenv_environments:\n  vars: { 'location1': '~/my_projects', 'location2': '~/my_learnings', }\n    cmds:\n      - echo \"The Python pipenv Environments are\"\n      - echo\n      - echo \"Locating pipenv environments in `~/my_projects`\"\n      - find {{.location1}} -type d -name \".venv\" | rev | cut -d / -f 2- | rev\n      - echo \"Locating pipenv environments in `~/my_learnings`\"\n      - echo\n      - find {{.location2}} -type d -name \".venv\" | rev | cut -d / -f 2- | rev\n    silent: true\n\nResult:"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#want-to-know-some-more",
    "href": "posts/2023-12-14-taskfile.html#want-to-know-some-more",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "9. Want to know some more?",
    "text": "9. Want to know some more?\n\n9.1 Want to list all tasks in your taskfile?\n% task --list-all\n\n# for indentifying the global tasks (refer section 4)\n% task --global --list-all\n\n\n9.2 Want a tab auto-complete?\n\nI use zsh hence used the instructions here to enable tab-based auto-complete feature\n\n## find where `_task` file is (if using linux, change `_task` to `task.bash`\n\nany/directory % find /usr -type f -name \"_task\"\n/path/to/_task\n\n## move the _task file to `site_functions`\n\nany/directory % mv /path/to/_task /usr/local/share/zsh/site-functions/_task\n\n## add the following lines to ~/.zshrc \nautoload -U compinit\ncompinit -i\n\n\n9.3. What to know about some more features?\nWhat have I missed?:  I have not used the following features of Taskfile, yet. But if you are interested, take a look at the official documentation: \n\nsummary\ndefer\ndeps\nTask aliases and Namespace aliases\nOptional includes\nInternal inclides"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#real-world-aws-cloud-recipes",
    "href": "posts/2023-12-14-taskfile.html#real-world-aws-cloud-recipes",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "10. Real-world AWS Cloud Recipes",
    "text": "10. Real-world AWS Cloud Recipes\n\nLet us learn to apply some of the above concepts in 2 real-world cloud examples\nThe purpose of the below two cloud recipes is not just to share replicable infrastructure codes but also to act as a good source of documentation as well.\nI am learning for a AWS certification exam, and what better way to learn than create AWS infra from scratch, in a replicable way, using Taskfiles!\n\n\n10.1. Taskfiles to create a Simple AWS Lambda from Scratch using AWS CLI commands\n\nHow are we going to create a simple AWS Lambda using Taskfiles?\n\n\n\nsource: Image is created by author\n\n\nAll these files for this section are located here\nThe actual code is just the simple python file below hello_lambda.py.\n\n\"\"\"\nA Lambda Function to Greet Hello, &lt;User&gt;. The code is a simplified version of the official example below\n\nSource: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html\n\"\"\"\n\nimport json\n\ndef lambda_handler(event, context):\n    \"\"\"\n    event argument needs to be in the fasion `{\"greeter\": \"Senthil\"}`\n    context argument is a default passed to the handler function. Know more here: https://docs.aws.amazon.com/lambda/latest/dg/python-context.html\n    \"\"\"\n    greeter = event['greeter']\n\n    res = {\n    'statusCode': 200,\n    'headers': {\n        'Content-Type': '*/*'\n    },\n    'body': 'Hello everyone, This is '+greeter+'!'\n    }\n    return res\n\nAll the infrastructure as codes (IAC) are abstracted in the main Taskfile.yml (which in turn sources from modularized yml files). The supporting files needed for the creation of lambda are listed below as well.\n\nhello_lambda % tree -L 2 .\n\n.\n├── IAM_tasks.yml\n├── Taskfile.yml\n├── lambda_creation_tasks.yml\n├── lambda_invoke_and_cleanup_tasks.yml\n└── supporting_files\n    ├── iam_tasks\n    │   ├── sample_iam_policy.json\n    │   └── trust_policy.json\n    ├── lambda_creation\n    │   └── aws_cli_command_for_lambda_creation.bash\n    └── lambda_testing\n        ├── test_event_1.json\n        └── test_event_2.json\n\nIf you want to see the folder ./supporting_files in detail, check the repo here\nThe main Taskfile.yml\n\nversion: \"3\"\n\nincludes:\n  iam_tasks:\n    taskfile: ./IAM_tasks.yml\n  lambda_creation_tasks:\n    taskfile: ./lambda_creation_tasks.yml\n  testing_tasks:\n    taskfile: ./lambda_invoke_tasks.yml\n\ntasks:\n  1_setup_iam_policy_and_role:\n    summary: |\n      Goal - Steps to create IAM Policy and Role\n      Step 0- Prepare the IAM_Policy json (the permissions that the lambda function needs such as Cloudwatch or S3) and \n        trust policy json (the role needs to assume a policy to invoke the lambda)\n      Step 1- Create the policy (we are having an empty policy; no extra permission needed)\n      Step 2- Create the role with the trust_policy\n      Step 3- Attach policy to role\n    cmds:\n      - task iam_tasks:create_policy\n      - task iam_tasks:create_role_with_lambda_trust_policy\n      - task iam_tasks:attach_policy_to_role\n      - task iam_tasks:get_role_arn\n    silent: false\n    \n  2_create_lambda:\n    summary: |\n      Goal: Create a lambda \n    cmds:\n      - task lambda_creation_tasks:create_lambda_function\n    silent: false\n\n  3_invoke_lambda_multiple_times:\n    summary: |\n      Goal: Test the lambda with various test events\n    cmds:\n      - task testing_tasks:test_event_1\n      - task testing_tasks:test_event_2\n    silent: true\n    \n  \n  4_cleanup:\n    summary: |\n      Goal: Clean all the resources you had created such as \n        lambda, iam_policy and role\n\n    cmds:\n      - task iam_tasks:clean_iam_policy_and_role\n      - task lambda_creation_tasks:delete_lambda_function\n    silent: false\n\nLet us test the tasks now !\n\n\n\n\n\n10.2. Taskfile to create a EC2 instance from scratch using AWS CLI commands\n\nWhat are we going to accomplish\n\nCreate a EC2 instance ensconced inside a public subnet in a VPC, with a security group layer protection\nThere are so many reasons why would need a EC2 instance. You could use one host your model served in a flask app. You could serve a FastAPI app or a streamlit app in that EC2 instance or do a zillion other things !\n\n\n\n\nsource: Image is created by author\n\n\nCheckout this Taskfile.yml which has the following tasks. The taskfile also acts as a documentation in itself and also for replicating the creation of infrastructure.\n\ncreate_and_connecte_to_ec2_instance % task --list-all\n\ntask: Available tasks for this project:\n* 1_create_vpc_resources:\n* 2_complete_vpc_setup:\n* 3_create_ec2_resources:\n* 4_ssh_into_ec2_instance:\n* 5_cleanup_the_ec2_instance:\n* 6_cleanup_other_resources:\n\nLet us look into the sub tasks in the above tasks\n\nversion: \"3\"\n\nincludes:\n  sub_tasks:\n    taskfile: ./sub_tasks.yml\n\ntasks:\n  1_create_vpc_resources:\n    summary: |\n      Step 1 - Create a VPC\n      Step 2 - Create a Public Subnet\n      Step 3 - Create a Route table\n      Step 4 -  Create a Internet Gateway\n      Step 5 - Creata a Security Group \n...\n    \n  2_complete_vpc_setup:\n    summary: | \n      Step 6 - allow_public_subnet_to_auto_assign_ipv4_address\n      Step 7 - attach_int_gw_to_vpc\n      Step 8 - create_route_in_route_table\n      Step 9 - associate_public_subnet_in_route_table\n      Step 10 - allow_ssh_port_access_in_sg\n...\n  \n  3_create_ec2_resources:\n    summary:  |\n      Step 11 -  generate_ec2_key_pair\n      Step 12 - create_ec2_instance\n...\n\n  4_ssh_into_ec2_instance:\n    summary: |\n      Step 13 -  running_ec2_instance_details\n      Step 14 - ssh_into_the_ec2_instance\n...\n\n  5_cleanup_the_ec2_instance:\n    summary: |\n      Step 15 - stop_ec2_instance\n      Step 16 - terminate_ec2_instance\n...\n  \n  6_cleanup_other_resources:\n    summary: |\n      Step 17 - delete_security_group\n      Step 18 - modify_public_subnet_attribute\n      Step 19 - delete_public_route_table, delete_intetnet_gateway and delete_subnet\n      Step 20 - delete_vpc\n...\n\nP.S. A digression alert: - If I take another stab at the above sample projects, I would have now used Cloudformation or Terraform, instead of AWS CLI. - But it would still be with Tasfiles for packaging those commands. For example, infra would created using aws cloudformation create-stack --stack-name myteststack but this command would be inside a task create_vpc_subnet_stack"
  },
  {
    "objectID": "posts/2023-12-14-taskfile.html#conclusion",
    "href": "posts/2023-12-14-taskfile.html#conclusion",
    "title": "All You Need to Know about the Golang-based Task Runner Build Tool",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe have seen how useful taskfiles are for different types of requirements. Be it a AWS CLI or terraform script,a bash or python script, or a set of docker/K8s commands, abstract them away using easy to create and use taskfiles.\nOne more thing to note is that, environment variable values can be kept hidden in a .env (or .any_file_name for that matter.) You do not have to explicitly save them inside a git shared Taskfile.yml. This process ensures more security and replicability of the codebase across projects\n\n\nIf you think Taskfile is so useful to you, consider donating the author of this amazing Go module here.\nThis is a definitely useful tool to have in your tool box. Happy learning, everyone."
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html",
    "href": "posts/2022-08-03-I-Shell-Scripting.html",
    "title": "A Practical Guide to Bash Scripting",
    "section": "",
    "text": "This is a quick guide manual to advance your understanding of Bash Scripting for real-world application, beyond just one-liner commands. \nTo solve specific real-world use cases, like the below need, we will be writing a combination of the commands in a single line (in Section III)\nYou will see how to write interactive shell file scripts (in Section IV)"
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html#ii.-a-brief-intro-to-bash-scripting",
    "href": "posts/2022-08-03-I-Shell-Scripting.html#ii.-a-brief-intro-to-bash-scripting",
    "title": "A Practical Guide to Bash Scripting",
    "section": "II. A Brief Intro to Bash Scripting",
    "text": "II. A Brief Intro to Bash Scripting\n\nWhat is a Linux CLI/Shell Terminal\n\nA non-graphical text-based interface to the Kernel\nA program to access the features and functionalities offered by kernel\nHow instructions flow: Hardware &lt;– Kernel &lt;– Shell/Terminal &lt;– Command Libraries and Applications (like mail) &lt;– User\n\n\n\nWhat is a Shell or Bash?\n\nThe shell is the operating system’s command-line interface (CLI) and interpreter for the set of commands that are used to communicate with the system.\n\nsource\n\n\nDifferent Types of Shell\n\nBourne Shell developed by Steve Bourne at AT&T Labs (sh)\nC shell (csh)\nKorn Shell (ksh) - better version of sh\nBourne-Again Shell (bash) had features of csh and ksh\nZ Shell (zsh) - A more modern shell adopted by MacOS!\n\nSources for this section: Refer here and here"
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html#example-1-convert-a-counting_files_recursively.sh-shell-file-into-a-command-counting_files_recursively",
    "href": "posts/2022-08-03-I-Shell-Scripting.html#example-1-convert-a-counting_files_recursively.sh-shell-file-into-a-command-counting_files_recursively",
    "title": "A Practical Guide to Bash Scripting",
    "section": "Example 1: Convert a counting_files_recursively.sh shell file into a command counting_files_recursively",
    "text": "Example 1: Convert a counting_files_recursively.sh shell file into a command counting_files_recursively\n#!/bin/bash\n\ncount_files() {\n    file_count=$(find \"$1\" -type f | wc -l)\n    echo \"Local Directory: $1\"\n    echo \"Number of files: $file_count\"\n    echo\n}\n\n# Specify the root directory to start counting files recursively\nroot_directory=$1\n\n# Call the function for each directory within the root directory\nfor local_directory in \"$root_directory\"/*; \ndo\n    if [ -d \"$local_directory\" ]; then\n        count_files \"$local_directory\"\n    fi\ndone\nsenthilkumar.m@BashScripter ~/text_datasets % echo \"alias counting_files_recursively=/path/to/counting_files_recursively.sh\" &gt;&gt; ~/.zshrc && source ~/.zshrc\nsenthilkumar.m@BashScripter ~/text_datasets % counting_files_recursively ~/text_datasets\n\nLocal Directory: ~/text_datasets/dbpedia\nNumber of files:        7\n\nLocal Directory: ~/text_datasets/shell_file_in_path\nNumber of files:        1\n\nLocal Directory: ~/text_datasets/yelp_reviews\nNumber of files:        6"
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html#example-2-simpler-one-line-alias-command.-converting-ls--al-into-ls_all",
    "href": "posts/2022-08-03-I-Shell-Scripting.html#example-2-simpler-one-line-alias-command.-converting-ls--al-into-ls_all",
    "title": "A Practical Guide to Bash Scripting",
    "section": "Example 2: Simpler one-line alias command. Converting ls -al into ls_all",
    "text": "Example 2: Simpler one-line alias command. Converting ls -al into ls_all\nsenthilkumar.m@BashScripter ~/text_datasets % echo \"alias ls_all=ls -al\" &gt;&gt; ~/.zshrc && source ~/.zshrc\nsenthilkumar.m@BashScripter ~/text_datasets % ls_all ~/text_datasets"
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html#the-typical-ways",
    "href": "posts/2022-08-03-I-Shell-Scripting.html#the-typical-ways",
    "title": "A Practical Guide to Bash Scripting",
    "section": "The Typical Ways",
    "text": "The Typical Ways\n\n&gt; (redirect afresh) , &gt;&gt; (redirect and append) and | (pipe\n\n\nBy now you may have got familiarized with the standard ways of redirecting outputs - &gt; and &gt;&gt; and |\n\n# step 1\n/some/local/path % echo \"Hello, world!\" &gt; output.txt\n/some/local/path % cat output.txt\n\nHello, world!\n\n# step 2\n/some/local/path % echo \"Additional text\" &gt;&gt; output.txt\n/some/local/path % cat output.txt\n\nHello, world!\nAdditional text\n\n# step 3\n/some/local/path % echo \"Again, Hello, world!\" &gt; output.txt\n/some/local/path % cat output.txt\n\n\nAgain, Hello, world!\n\n# step 4\n/some/local/path % echo \"Hello, world!\"  | wc -c \n    13\n\nNote: In the above bash example, by step 3, the output.txt gets rewritten."
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html#additional-aids-the-file-descriptors-0-1-and-2",
    "href": "posts/2022-08-03-I-Shell-Scripting.html#additional-aids-the-file-descriptors-0-1-and-2",
    "title": "A Practical Guide to Bash Scripting",
    "section": "Additional Aids: The File Descriptors (0, 1, and 2)",
    "text": "Additional Aids: The File Descriptors (0, 1, and 2)\n\nA file descriptor is a descriptor OR unique identifier OR a index of the files that are opened when a shell file is run.\n\n\nA shell file opens 3 files with the file descriptors 0, 1, and 2\n\n0 (stdin): Standard Input\n1 (stdout): Standard Output\n2 (stderr): Standard Error\n/some/local/path % cat simple_bash_script_for_reading_stdin.bash\n\n#!/bin/bash\n\n# Variable to store all lines\nall_lines=\"\"\n\n# Read each line from stdin and append to the variable\nwhile IFS= read -r line; do\n    # Process each line (you can replace this with your own logic)\n    echo \"Read line: $line\"\n    \n    # Append the line to the variable\n    all_lines=\"$all_lines$line\"\ndone\n\n/some/local/path % cat file_input.txt\n\nHello,\nThis is an example text file.\nIt contains multiple lines.\nEach line is processed by the program script.\nYou can replace this content with your own data.\n\n/some/local/path % bash simple_bash_script_for_reading_stdin.bash &lt; file_input.txt &gt; file_output.txt\n\n/some/local/path % cat file_output.txt\n\nRead line: Hello,\nRead line: This is an example text file.\nRead line: It contains multiple lines.\nRead line: Each line is processed by the program script.\nRead line: You can replace this content with your own data.\n\nIn the above example, program &lt; file_input &gt; file_output structure is followed.\nfile_input.txt was the stdin (0)\nfile_output.txt was the standard output (1)\nIf I introduce some error in the a_simple_python_file_with_error.py, we can capture error alone separately\n\n/some/local/path % cat a_simple_python_file_with_error.py\n\nprint(\"output line 1\")\nprint(\"output line 2\")\n\na=b+1 #this will throw an error as `b` is not defined\n\n\n/some/local/path % python a_simple_python_file_with_error.py 2&gt;error.txt 1&gt;output.txt\n\n/some/local/path % cat error.txt \n\nTraceback (most recent call last):\n  File \"/a_simple_python_file_with_error.py\", line 4, in &lt;module&gt;\n    a=b+1 #this will throw an error as `b` is not defined\nNameError: name 'b' is not defined\n\n\n/some/local/path % cat output.txt\n\noutput line 1\noutput line 2\n\nif you want the error and the output to be redirected to the same file, you can use &gt;& or 2&gt;&1\n\n/some/local/path % python a_simple_python_file_with_error.py &gt; output_with_error.txt 2&gt;&1\n\n# Or the following also works\n# /some/local/path %  python a_simple_python_file_with_error.py &gt;& output_with_error.txt\n\n/some/local/path % cat output_with_error.txt\n\noutput line 1\noutput line 2\nTraceback (most recent call last):\n  File \"/a_simple_python_file_with_error.py\", line 4, in &lt;module&gt;\n    a=b+1 #this will throw an error as `b` is not defined\nNameError: name 'b' is not defined"
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html#devnull",
    "href": "posts/2022-08-03-I-Shell-Scripting.html#devnull",
    "title": "A Practical Guide to Bash Scripting",
    "section": "/dev/null",
    "text": "/dev/null\nIf you are wondering, how to ignore some outputs or errors from being printed or displayed, redirecting them to /dev/null is the answer\n# ignore both stderr (2) and stdout (1)\n/some/local/path % python a_simple_python_file_with_error.py &gt; /dev/null 2&gt;&1\n\n# ignore the stderr (2)\n/some/local/path % python a_simple_python_file_with_error.py 2&gt;/dev/null\noutput line 1\noutput line 2\n\n# ignore only the stdout (1)\n/some/local/path % python a_simple_python_file_with_error.py 1&gt;/dev/null\n\nTraceback (most recent call last):\n  File \"/a_simple_python_file_with_error.py\", line 4, in &lt;module&gt;\n    a=b+1 #this will throw an error as `b` is not defined\nNameError: name 'b' is not defined"
  },
  {
    "objectID": "posts/2022-08-03-I-Shell-Scripting.html#concluding-remarks",
    "href": "posts/2022-08-03-I-Shell-Scripting.html#concluding-remarks",
    "title": "A Practical Guide to Bash Scripting",
    "section": "Concluding Remarks …",
    "text": "Concluding Remarks …\n\nWhy is Shell Scripting so hard?\n\nComplexity compounds | Commands like awk and sed are progamming lang on their own\nEvery quote, space and stringed together commands have meaning\nLess errors | More unexpected behaviour\n\n\n\nWhere is Shell Scripting most useful?\n\nWhen the commands are only 5-20 lines long\n\nSmall repetitive tasks. E.g. For small needs involving AWS, small scripting jobs with jmespath\n\n\nfor f in *.csv\ndo\n     echo $f...\n     some transformation\ndone\n\nA pure shell script written 15-20 years ago could still yield the same result today\nEasier than packaging a Python application | Build process is smaller\nShell in combination with other language is more useful\n\n\n\nWhere Shell Scripting can be avoided?\n\nCo-development is hard. Interpretability is tough.\nErrors do not always stop your flow. It will go to the next command\nWhen the shell script becomes very complicated, better to go with your programming lang of choice\n\n\nUnequivocally, Bash Scripting is definitely a useful skill to know in your toolbox"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-DL.html#single-neuron",
    "href": "posts/2022-03-04-Intro-to-DL.html#single-neuron",
    "title": "Understanding Deep Learning Fundamentals - from a coder’s viewpoint",
    "section": "1. Single Neuron",
    "text": "1. Single Neuron\n\nA linear unit with 1 input\n\n\n\n\nimage\n\n\n\nA liniear unit with 3 inputs &gt; y = w0x0 + w1x1 + w2x2 + b\n\n\n\n\nimage\n\n\n\nIn Keras, the input_shape is a list &gt; model = keras.Sequential([layers.Dense(units=1, input_shape=[3]]) &gt; where unit represents the number of neurons in the Dense layer &gt; input_shape determines the size of input\nwhere for\n\ntabular data: &gt; input_shape = [num_columns]\nimage_data: &gt; input_shape = [height, width, channels]\n\nIn PyTorch, the same model is defined as follows:\n\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(3, 1)\n    \n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\nmodel = Model()\n\nIn PyTorch, the model architecture is explicitly given by subclassing nn.Module and implementing the __init__ and forward methods"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-DL.html#deep-neural-network",
    "href": "posts/2022-03-04-Intro-to-DL.html#deep-neural-network",
    "title": "Understanding Deep Learning Fundamentals - from a coder’s viewpoint",
    "section": "2. Deep Neural Network",
    "text": "2. Deep Neural Network\n\nA dense layer consists of multiple neurons\n\n\n\n\nimage\n\n\n\nEmpirical fact: Two dense layers with no activation function is not better than one dense layer\nWhy Activation functions? \n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\nRectifier function “rectifies” the negative values to zero. ReLu puts a “bend” in the data and it is better than simple linear regression lines\nA single neuron with ReLu\n\n\n\n\nimage\n\n\n\nA Stack of Dense Layers with ReLu for non-linearity. An example of a Fully-Connected NN:\n\n\n\n\nimage\n\n\n\nthe final layer is linear for a regression problem; can have softmax for a classification problem\n\nKeras Version:\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# defining a model\nmodel = keras.Sequential([\n    # the hidden ReLu layers\n    layers.Dense(units=4, activation='relu', input_shape=[2]),\n    layers.Dense(units=3, activation='relu'),\n    layers.Dense(unit=1),\n    ])\n\nThe above multilayer NN code in PyTorch can be written as:\n\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(2, 4)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(4, 3)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(3, 1)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        return x\n\nmodel = Model()"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-DL.html#loss-function",
    "href": "posts/2022-03-04-Intro-to-DL.html#loss-function",
    "title": "Understanding Deep Learning Fundamentals - from a coder’s viewpoint",
    "section": "3. Loss Function",
    "text": "3. Loss Function\n\nAccuracy cannot be used as loss function in NN because as ratio (num_correct / total predictions) changes in “jumps”. We need a loss function that changes smoothly.\nCross Entropy = - 1/N ∑ (i=1 to N) {(y_actual(i) * log(y_predicted(i)) + (1-y_actual(i)) * log(1-y_predicted(i)) }\nCE is measure to compute distance between probabilities.\n\nIf y_predicted(i) is farther from y_actual(i), CE(i) will be closer to 1. Vice versa, if y_predicted(i) is closer to y_actual(i), then CE(i) will be close to 0\n\n\n\n\n\nimage"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-DL.html#gradient-descent",
    "href": "posts/2022-03-04-Intro-to-DL.html#gradient-descent",
    "title": "Understanding Deep Learning Fundamentals - from a coder’s viewpoint",
    "section": "4. Gradient Descent",
    "text": "4. Gradient Descent\n\nGradient Descent is an optimization algorithm that tells the NN\n\nhow to change its weight so that\nthe loss curve shows a descending trend\n\n\n\n\n\nimage\n\n\nDefinition of terms:\n\nGradient: Tells us in what direction the NN needs to adjust its weights. It is computed as a partial derivative of a multivariable cost func\ncost_func: Simplest one: Mean_absolute_error: mean(abs(y_true-y_pred))\nGradient Descent: You descend the loss curve to a minimum by reducing the weights w = w - learning_rate * gradient\nstochastic - occuring by random chance. batch_size = 1 (OR)\nmini batch: The selection of samples in each mini_batch is by random chance. 1 &lt; mini_batch &lt; size_of_the_data (OR)\nbatch: When batch_size == size_of_the_data\n\nHow GD works:\n\n\nSample some training data (called minibatch) and predict the output by doing forward propagation on the NN architecture\n\n\nCompute loss between predicted_values and target for those samples\n\n\nAdjust weights so that the above loss is minimized in the next iteration\n\nRepeat steps 1, 2, and 3 for an entire round of data, then one epoch of training is over\nFor every minibatch there is only a small shift in the weights. The size of the shifting of weights is determined by learning_rate parameter"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-DL.html#how-to-train-the-model",
    "href": "posts/2022-03-04-Intro-to-DL.html#how-to-train-the-model",
    "title": "Understanding Deep Learning Fundamentals - from a coder’s viewpoint",
    "section": "5. How to train the Model",
    "text": "5. How to train the Model\n\n5.A. Instantiating the Model\nKeras Version:\n# define the optimizer\nmodel.compile(optimizer=\"adam\", loss=\"mae\")\nPyTorch Version:\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Instantiate the model, refer to the Model class created above\nmodel = Model()\n\n# Define the loss function\nloss_function = nn.L1Loss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters())\n\n\n5.B. Training the Model with data\nKeras Version:\n# fitting the model\nhistory = model.fit(X_train, y_train, \n    validation_data=(X_valid,y_valid),\n    batch_size=256,\n    epoch=10,\n    )\n\n# plotting the loss curve\nhistory_df = pd.DataFrame(history.history)\nhistory_df['loss'].plot()\nPyTorch Version:\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute the loss\n    loss = loss_function(outputs, targets)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n\n5.C. Underfitting and Overfitting\nUnderfitting - Capacity Increase - If you increase the number of neurons in each layer (making it wider), it will learn the “linear” relationships in the features better - If you add more layers to the network (making it deeper), it will learn the “non-linear” relationships in the features better - Decision on Wider or Deeper networks depends on the dataset\nOverfitting - Early Stopping: Interrupt the training process when the validation loss stops decreasing (stagnant) - Early stopping ensures the model is not learning the noises and generalizes well\n\n\n\nimage\n\n\n\nOnce we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured.\n\nKeras Version:\nfrom tensorflow.keras.callbacks import EarlyStopping\n# a callback is just a function you want run every so often while the network trains\n\n# defining the early_stopping class\nearly_stopping = EarlyStopping(min_delta = 0.001, # minimum about of change to qualify as improvement\n                               restore_best_weights=True,\n                               patience=20, # number of epochs to wait before stopping\n                              )\n\n\nhistory = model.fit(X_train, y_train, \n    validation_data=(X_valid,y_valid),\n    batch_size=256,\n    epoch=500,\n    callbacks=[early_stopping],\n    verbose=0 #turn off logging\n    )\n    \nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n\n\n\nimage\n\n\nPyTorch Version: - In PyTorch, there is no built-in EarlyStopping callback like in Keras\n\n# Define the early stopping criteria\nclass EarlyStopping:\n    def __init__(self, min_delta=0.001, restore_best_weights=True, patience=20):\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.patience = patience\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, loss):\n        if self.best_loss is None:\n            self.best_loss = loss\n        elif loss &gt; self.best_loss + self.min_delta:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = loss\n            self.counter = 0\n\n# Instantiate the early stopping class\nearly_stopping = EarlyStopping()\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Train the model and compute the loss\n    model.train()\n    # ...\n    loss = loss_function(outputs, targets)\n    \n    # Call the early stopping function and check for early stopping\n    early_stopping(loss)\n    if early_stopping.early_stop:\n        print(\"Early stopping!\")\n        break\n\n    # ...\n    # Other training loop code\n# After training, if restore_best_weights=True, you can load the best weights\nif early_stopping.restore_best_weights:\n    model.load_state_dict(torch.load('best_model_weights.pt'))\n\n\n5.D. Batch Normalization\nWhy BatchNorm? - Can prevent unstable training behaviour - the changes in weights are proportion to how large the activations of neurons produce - If some unscaled feature causes so much fluctuation in weights after gradient descend, it can cause unstable training behaviour - Can cut short the path to reaching the minima in the loss curve (hasten training) - models with BatchNorm tend to need fewer epochs for training\nWhat is BatchNorm? - On every batch of data subjected to training - normalize the batch data with the batch’s mean and standard deviation - multiply them with rescaling parameters that are learnt while training the model\nKeras Version:  Three places where BatchNorm can be used 1. After a layer\nkeras.Sequential([\n    layers.Dense(16,activation='relu'),\n    layers.BatchNormalization(),\n    ])\n\nin-between the linear dense and activation function\n\nkeras.Sequential([\n    layers.Dense(16),\n    layers.BatchNormalization(),\n    layers.Activation('relu')\n    ])\n\nAs the first layer of a network (role would then be similar to similar to Sci-Kit Learn’s preprocessor modules like StandardScaler)\n\nkeras.Sequential([\n    layers.BatchNormalization(),\n    layers.Dense(16),\n    layers.Activation('relu')\n    ])\nPyTorch Version\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n\n5.E. LayerNormalization\n\nIt seems that it has been the standard to use batchnorm in CV tasks, and layernorm in NLP tasks Source\n\n\nLayer normalization normalizes input across the features instead of normalizing input features across the batch dimension in batch normalization. … The authors of the paper claims that layer normalization performs better than batch norm in case of RNNs. Source\n\n\n\n5.F. Dropout\nWhat is Dropout? - It is NN way of regularizing data (to avoid overfitting) by - randomly dropping certain proportion of neurons in a layer\nHow Dropout regularizes? - It makes it harder for neural network to overfit for the noise\n\n\n\nimage\n\n\nkeras.Sequential([\n    # ....\n    layers.Dropout(0.5), # add dropout before the next layer\n    layers.Dense(512, activation='relu'),\n    # ...\n\n])\nWhen adding Dropout, it is important to add more neurons to the layers\n# define the model\nmodel = keras.Sequential([\n    layers.Dense(1024, activation='relu',input_shape=[11],\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu',\n    layers.Dense(1024),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\n# compile the model\nmodel.compile(optimizer='adam', loss='mae')\n\n# fit the model\nhistory = model.fit(X_train, y_train,\n                    validation_set=(X_valid, y_valid),\n                    batch_size=256,\n                    epochs=100,\n                    verbose=1,\n)\n\n# plot the learning curves\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss','val_loss']].plot()"
  },
  {
    "objectID": "posts/2022-03-04-Intro-to-DL.html#building-a-nn-two-class-classifier",
    "href": "posts/2022-03-04-Intro-to-DL.html#building-a-nn-two-class-classifier",
    "title": "Understanding Deep Learning Fundamentals - from a coder’s viewpoint",
    "section": "6. Building a NN Two-class Classifier",
    "text": "6. Building a NN Two-class Classifier\nLet us apply all the learnings from above in building a Binary Classifier \nKeras Version: - The loss_function used in a binary classifier is binary_crossentropy - The last layer in a binary classifier is sigmoid\n\n\n\nimage\n\n\n# define the model\nmodel = keras.Sequential([\n    layers.Dense(1024,activation='relu',input_shape=[13]), #13 features\n    layers.Dense(512,activation='relu'), # hidden layer\n    layers.Dense(1,avtiation='sigmoid'), # output sigmoid layer for binary classification\n  ])\n  \n # compile the model with optimizer, loss function and metric function\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metric=['binary_accuracy'] # accuracy metric is not used in the training of the model but just for evaluation\n              )\n     \n# define callback function which is called periodically while training the NN     \nearly_stopping = keras.callbacks.EarlyStopping(min_delta=0.001, #minimum amount of change in loss to qualify as improvement \n                                               patience=10, # no. of epochs with no change happening but to keep trying before stopping\n                                               restor_best_weights=True\n                                               )\n  \n# train the model\nhistory = model.fit(X_train, y_train,,\n                    validation_set=(X_valid,y_valid),\n                    batch_size=512,\n                    epochs=1000,\n                    callbacks=[early_stopping]),\n                    verbose=0, # hide the logging because we have so many epochs\n)\n\n\n# plot the curve after training is over\nhistory_df = pd.DataFrame(history.history)\n\n# plotting the loss and accuracy curves from epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:,['binary_accuracy','val_binary_accuracy']].plot()\n\nprint(\"Best Training Accuracy {:.04f}\".format(history_df['binary_accuracy'].max())\nprint(\"Best Validation Accuracy {:.04f}\".format(history_df['val_binary_accuracy'].max())\n\nprint(\"Best Training Loss {:04f}\".format(history_df['loss'].min())\nprint(\"Best Validation Loss {:.04f}\".format(history_df['val_loss'].min())\n\n\n# predicting from a trained model\ny_test_predicted = model.predict_classes(X_test)\nprint(y_test_predicted[0:5])\n# [0, 1, 1, 0, 0]\n\ny_test_predicted_proba = model.predict_proba(X_test)\nprint(y_test_predicted_proba[0:5])\n# [0.08, 0.82, 0.78, 0.01, 0.0]\nPyTorch Version:\nimport torch\nimport torch.nn as nn\n\n# building the model class by sub-classing nn.Module\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n# instantiating the model \nmodel = BinaryClassifier()\n\n\n# defining the loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Training loop\n\nmodel.train()  # Enable training mode\n\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n    \nmodel.eval()  # Enable evaluation mode when evaluating\n\n# Use the model for evaluation or inference\n\nWe have incorporated both batch_noamralization and dropout to reduce overfitting in the above PyTorch model\n\nSource:  - Kaggle.com/learn (for Keras version of the codes)"
  },
  {
    "objectID": "posts/2022-04-11-evolution_of_tl_in_nlp.html",
    "href": "posts/2022-04-11-evolution_of_tl_in_nlp.html",
    "title": "The Evolution of Transfer Learning until the Advent of the First Tranformers in NLP",
    "section": "",
    "text": "Want to read this blog in a easier to digest slides format? Refer Link"
  },
  {
    "objectID": "posts/2022-04-11-evolution_of_tl_in_nlp.html#quick-introduction",
    "href": "posts/2022-04-11-evolution_of_tl_in_nlp.html#quick-introduction",
    "title": "The Evolution of Transfer Learning until the Advent of the First Tranformers in NLP",
    "section": "1. Quick Introduction",
    "text": "1. Quick Introduction\n\nWhat is powering the emergence of better models in NLP?\n\nBetter representation of the text data (with no supervision) by grasping the context better\n\nFrom Word2Vec to BERT and beyond, this is the underlying logic!\n\n\n\n\n\nHow are better text representations produced?\n\nBetter contextual representation of words using Transfer Learning\n\n\n\n\nWhat is Transfer Learning?\n Source: ULMFiT Paper | link\n\n\n\nWhat forms the crux of Transfer Learning models in NLP?\n\nLanguage Models! \n\n\nBuild a Language Model that understands the underlying features of the text\n\n\nFine-tune the Language Model with additional layers for downstream tasks\n\n\n\n\n\n\nWhy Language Model?\n\nLanguage modeling can be seen as the ideal source task as it captures many facets of language relevant for downstream tasks, such as long-term dependencies, hierarchical relations and sentiment \nRuder et al in the ULMFiT paper\n\n\n\n\nOk, What is a Language Model?\n\nA language model (LM) is a model that generates a probability distribution over sequences of words\nIn simpler words, LM is a model to predict the next word in a sequence of words\nIt is unsupervised or self-supervised (since we already know what is the next word in the corpus)\n\n\n\n\nWhat are those Language Models?\n\nExamples of Language Models: Word2Vec, Glove, ELMo, ULMFiT, BERT, and many more\n\n Source: A article by Sebastian Ruder: State of Transfer Learning in NLP | Link\n\n\n\nWhat are the two types of Transfer Learning built using the LMs?\n\nType 1: Feature Extraction\n\nExample: Universal Sentence Encoder produces just an embedding/numerical representation and that gets used by a downstream application \n\n\nSource of image: TF Hub Article on Universal Sentence Encoder | Link\n\n\nType 2: Fine Tuning\n\nE.g.: BERT Fine-tuned for Text Classification \n\n\nSource of image: An article in Research Gate | Link"
  },
  {
    "objectID": "posts/2022-04-11-evolution_of_tl_in_nlp.html#types-of-language-models",
    "href": "posts/2022-04-11-evolution_of_tl_in_nlp.html#types-of-language-models",
    "title": "The Evolution of Transfer Learning until the Advent of the First Tranformers in NLP",
    "section": "2. Types of Language Models",
    "text": "2. Types of Language Models\n\n2A. Count-based Language Models\n\n\n\n\n2B. Context-prediction based Pre-trained Language Models\n\nSources:  - Advanced NLP and Deep Learning course on Udemy (by LazyProgrammer) - Idea: http://www.marekrei.com/blog/dont-count-predict/\n\n\n\n2C. LSTM-based Pre-trained Language Models\n\nEvolution of RNN Architecture till LSTM\nWhy RNNs came into existence?  - Models such as the Multi-layer Perceptron Network, vector machines and logistic regression did not perform well on sequence modelling tasks (e.g.: text_sequence2sentiment_classification) - Why? Lack of memory element ; No information retention - RNNs attempted to redress this shortcoming by introducing loops within the network, thus allowing the retention of information.\n\n\n\nAn un-rolled RNN Cell\n\n\nAdvantage of a vanilla RNN:  - Better than traditional ML algos in retaining information\nLimitations of a vanilla RNN:  - RNNs fail to model long term dependencies. - the information was often “forgotten” after the unit activations were multiplied several times by small numbers - Vanishing gradient and exploding gradient problems\nLong Short Term Memory (LSTM):  - A special type of RNN architecture - Designed to keep information retained for extended number of timesteps\nAdvantage of a LSTM:  - Better equipped for long range dependencies - Resists better than RNNs for vanishing gradient problem\nLimitations of LSTM:  - Added gates lead to more computation requirement and LSTMs tend to be slower - Difficult to train - Transfer Learning never really worked - Very long gradient paths. LSTM on 100-word doc has gradients 100-layer network\n\n\n\nSeq2Seq Models - A higher form of LMs\n\n\n\n\nThe ImageNet moment in NLP; advent of LSTM models ULMFiT and ELMo\n\nELMo comes up with better word representations/embeddings using Language Models that learn the context of the word in focus \n\n\n\n\n\n2D. Transformer-based Pre-trained Language Models\n\nLSTM Seq2Seq Model with Attention\n\n\n\n\nTransformer - A Seq2Seq Model with Attention\nTransformer: - It is a sequence model forgoes the recurrent structure of RNN to adopt attention-based approach - In other words, Transformer is an attention Model without Recurrent LSTMs\nTransformer vs LSTM - Recurrent Structure: Processes all input elements SEQUENTIALLY - Attention-based Approach: Process all input elements SIMULTANEOUSLY\n\nThe BERT Mountain by Chris McCormick: \n\n\nTransformer for a Seq2Seq Task like Machine Translation: \n\n\n\n\nThe Advent of BERT and similar Transformers\n\nWhat has been the trend of recent Pre-trained Tranformer-based LMs?\n\nExponentially increasing model complexity (number of parameters)\n\n\n\nExponentially increasing data\n\n\nQuestion to ponder: - Are models bettering in performance because of more data or more model complexity? How much is the contribution from each? - Are models built with efficiency in mind? (not a lot can replicate these models given the large number of GPUs necessary)\n\n\n\nWhat direction should future Pre-trained Transformer-based LMs go?\n\nComputational Compexity is quadratic compared to input length. We curb input length to 512 tokens for most transformer models. &gt; Better model architectures are needed to capture long-range information\nAs models become bigger and complex, their explainability becomes difficult\nThere are models/methods/explaining the workings of attention mechanism but much more is needed in this space &gt; Need more efficient models with explainability in mind as well"
  },
  {
    "objectID": "posts/2022-04-11-evolution_of_tl_in_nlp.html#conclusion",
    "href": "posts/2022-04-11-evolution_of_tl_in_nlp.html#conclusion",
    "title": "The Evolution of Transfer Learning until the Advent of the First Tranformers in NLP",
    "section": "3. Conclusion",
    "text": "3. Conclusion\n\nIn summary, how has transfer learning evolved in NLP?\n\nStep -2: NLP started with rule-based and statistical methodologies\nStep -1: ML algorithms such as Naive Bayes, SVM, LR and Trees were fed with bag-of-words word representations\nStep 0: Initial Success of better representations using pre-trained LMs like Word2Vec which were built using shallow Neural Network\nStep 1: (Re)Emergence of RNN Architectures in NLP\nStep 2: Evolution of Sequence-to-Sequence Models built with RNN architectures from Language Models | source\nStep 3: ImageNet moment in NLP called upon by the first pre-Transformer era Transfer Learning models - ULMFiT and ELMo\nStep 4: Cometh the hour, cometh the Transformers !"
  },
  {
    "objectID": "posts/2022-04-11-evolution_of_tl_in_nlp.html#want-to-try-transfer-learning-hands-on",
    "href": "posts/2022-04-11-evolution_of_tl_in_nlp.html#want-to-try-transfer-learning-hands-on",
    "title": "The Evolution of Transfer Learning until the Advent of the First Tranformers in NLP",
    "section": "4. Want to try Transfer Learning hands-on?",
    "text": "4. Want to try Transfer Learning hands-on?\nExample notebooks for Text Classification Application\nJay Alamar’s Post: DistilBERT for Feature Extraction + Logitic Regression for classification | Link\n\n\n\nDistilBERT Sentiment Classifier\n\n\nJay Alamar’s Post: BERT Fine-tuned for Classification | Picture_Link | HuggingFace Example Fine-tuning Notebook\nReferences: - A suevey paper on Evolution of Transfer Learning in Natural Language Processing | Link  - A survey paper on Pre-trained Models for NLP | Link  - The State of Transfer Learning in NLP | Article by Sebastian Ruder | Link  - NLP's ImageNet Moment has arrived | Article by Sebastian Ruder | Link  - Recent Advances in LMs | Article by Sebastian | Link  - Sequence Modeling with Neural Networks  - Part 1: Evolution of Seq2Seq Models from Language Models  - Part 2: Seq2Seq with Attention - LSTM is dead. Long Live Transformers | YouTube Video by Leo Dirac | Presentation on the same title  - The Future of NLP video and slides by Thomas Wolf, HugginngFace Co-Founder | YouTube Video"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html",
    "href": "posts/2021-05-09-spacy_rules_ner.html",
    "title": "How to Leverage Spacy Rules NER",
    "section": "",
    "text": "SpaCy is a NLP library offering easy-to-use Python API for many information extraction and machine learning tasks in text data\nThey are internally written in Cython and hence occupies low memory foot print with its small models and are quite fast with decent accuracy\n\nSource: - more about spaCy\n\n\n\n\nNamed-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories\n\nSource: Wikipedia Article on NER\n\nIn information extraction, a named entity is a real-world object, such as a person, location, organization, product, etc., that can be denoted with a proper noun.  For example, in the sentence - “Biden is the president of the United States”,  “Biden” and “the United States” are named entities (proper nouns). “president” is not a named entity\n\nSource: Wikipedia Article on Named Entities\n\n\n\n\n\nCode\n!python3 -m spacy validate\n\n\n✔ Loaded compatibility table\n\n================= Installed pipeline packages (spaCy v3.1.3) =================\nℹ spaCy installation: /usr/local/lib/python3.7/dist-packages/spacy\n\nNAME             SPACY            VERSION                            \nen_core_web_sm   &gt;=3.1.0,&lt;3.2.0   3.1.0   ✔"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html#introduction-to-spacy-and-ner",
    "href": "posts/2021-05-09-spacy_rules_ner.html#introduction-to-spacy-and-ner",
    "title": "How to Leverage Spacy Rules NER",
    "section": "",
    "text": "SpaCy is a NLP library offering easy-to-use Python API for many information extraction and machine learning tasks in text data\nThey are internally written in Cython and hence occupies low memory foot print with its small models and are quite fast with decent accuracy\n\nSource: - more about spaCy\n\n\n\n\nNamed-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories\n\nSource: Wikipedia Article on NER\n\nIn information extraction, a named entity is a real-world object, such as a person, location, organization, product, etc., that can be denoted with a proper noun.  For example, in the sentence - “Biden is the president of the United States”,  “Biden” and “the United States” are named entities (proper nouns). “president” is not a named entity\n\nSource: Wikipedia Article on Named Entities\n\n\n\n\n\nCode\n!python3 -m spacy validate\n\n\n✔ Loaded compatibility table\n\n================= Installed pipeline packages (spaCy v3.1.3) =================\nℹ spaCy installation: /usr/local/lib/python3.7/dist-packages/spacy\n\nNAME             SPACY            VERSION                            \nen_core_web_sm   &gt;=3.1.0,&lt;3.2.0   3.1.0   ✔"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html#basic-featues-of-spacy-rules-ner",
    "href": "posts/2021-05-09-spacy_rules_ner.html#basic-featues-of-spacy-rules-ner",
    "title": "How to Leverage Spacy Rules NER",
    "section": "1. Basic Featues of SpaCy Rules NER",
    "text": "1. Basic Featues of SpaCy Rules NER\n\n1A. About Token Matcher\n\nSpacy’s Token Matcher lets you prepare Spacy Rules at token level invoking all complex\nExample 1 from Spacy Documentation\n\npatterns = [\n    [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}], # captures any case variant of \"hello-world\", \"hello!world\"\n    [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}] # captures any case variant of \"hello world\"\n]\n\nExample 2 using Token-level Regex\n\npattern = [{\"TEXT\": {\"REGEX\": \"^[Uu](\\.?|nited)$\"}}, #\n           {\"TEXT\": {\"REGEX\": \"^[Ss](\\.?|tates)$\"}},\n           {\"LOWER\": \"president\"}] # captures (U.S or US or United States or united states) President\n\n\n1B. About Phrase Matcher\n\nIf we have a huge list of phrases in a list or in a csv file, phrase matcher can be applied directly\n\nimport spacy\nfrom spacy.matcher import PhraseMatcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = PhraseMatcher(nlp.vocab)\nterms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n# Only run nlp.make_doc to speed things up\npatterns = [nlp.make_doc(text) for text in terms]\nmatcher.add(\"TerminologyList\", patterns)\n\ndoc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n          \"converse in the Oval Office inside the White House in Washington, D.C.\")\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(span.text)\nsource: https://spacy.io/usage/rule-based-matching#adding-phrase-patterns\n\n\n1C. Explaining Token and Phrase Matchers with a MODEL_NAMES NER Capture\n\n\nCode\n# goal: to capture Car Models to their corresponding Makes\nsample_sentence_1 = \"Go for ford mustang mache variants. Mustang has a deceivingly huge trunk and good horse power. If you want reliability, Toyota Lexus should be your choice. Lexus has good body style too\"\nsample_sentence_2 = \"Considering the harsh winters here, I am considering 2014 Nissan Murano or the '14 Subaru Forester\"\nsample_sentence_3 = \"Among used cars, I am still not sure what to choose - Civic or Corolla?\"\n\nsample_models_sentences = [sample_sentence_1, \n                    sample_sentence_2,\n                    sample_sentence_3\n                   ]\n\n\n\n\nCode\nimport spacy\nfrom spacy.matcher import Matcher, PhraseMatcher\nfrom spacy.tokens import Span\nfrom spacy.util import filter_spans\nfrom spacy import displacy\n\nmodel_names_nlp = spacy.load('en_core_web_sm',disable=['ner'])\nmatcher = Matcher(model_names_nlp.vocab)\n\n# pattern rules matching every token\nford_pattern = [{\"LOWER\": \"ford\", \"OP\":\"?\"},\n                   {\"LOWER\": \"mustang\"},\n                   {\"LOWER\":{\"IN\":[\"mache\",\"gt\",\"bulitt\"]},\"OP\":\"*\"}\n                  ]\ntoyota_pattern = [{\"LOWER\": \"toyota\",\"OP\":\"?\"},\n                 {\"LOWER\": {\"IN\":[\"lexus\",\"corolla\",\"camry\"]}}\n                ]\n\nhonda_pattern = [{\"LOWER\": \"honda\",\"OP\":\"?\"},\n                 {\"LOWER\": {\"IN\":[\"civic\",\"accord\"]}}\n                ]\n\ntoken_matcher_patterns = {\"FORD\": ford_pattern,\n                          \"TOYOTA\": toyota_pattern,\n                          \"HONDA\": honda_pattern,\n                         }\n\n# phrase pattern looks for exact match\nnissan_phrase_pattern = [\"Nissan Murano\", \"Murano\", \"murano\", \"nissan murano\"]\nsubaru_phrase_pattern = [\"Subaru Forester\", \"Forester\", \"forester\", \"subaru forester\"]\n\nphrase_matcher_patterns = {\"NISSAN\": nissan_phrase_pattern,\n                           \"SUBARU\": subaru_phrase_pattern\n                          }\n\ndef add_token_matcher_and_phrase_matcher_patterns(nlp_model,\n                                                  token_patterns_dict=token_matcher_patterns,\n                                                  phrase_patterns_dict=phrase_matcher_patterns\n                                                 ):\n    token_matcher = Matcher(nlp_model.vocab)\n    for key, value in token_patterns_dict.items():\n        token_matcher.add(key,[value])\n        \n    phrase_matcher = PhraseMatcher(nlp_model.vocab)\n    for key, terms_list in phrase_patterns_dict.items():\n        phrase_patterns = [nlp_model.make_doc(text) for text in terms_list]\n        phrase_matcher.add(key, phrase_patterns)\n    return token_matcher, phrase_matcher\n\ndoc = model_names_nlp(sample_sentence_1)\n\ndef modify_doc(token_matcher,\n               phrase_matcher,\n               doc):\n    original_ents = list(doc.ents)\n    matches = token_matcher(doc) + phrase_matcher(doc)\n    for match_id, start, end in matches:\n        span = Span(doc, start, end, match_id)\n        original_ents.append(span)\n    filtered = filter_spans(original_ents)\n    doc.ents = filtered\n    return doc\n\n\n\n\nCode\ntoken_matcher, phrase_matcher = add_token_matcher_and_phrase_matcher_patterns(model_names_nlp, \n                                                                              token_matcher_patterns,\n                                                                              phrase_matcher_patterns\n                                                                             )\nmodelnames_dict = {\n    \"HONDA\": \"#ffcccb\",  # light red (pink)\n    \"TOYOTA\": \"#A865C9\",  # light purple\n    \"NISSAN\": \"#FFD580\",  # light orange\n    \"SUBARU\": \" #FFCCCB\",  # green\n    \"FORD\": \"#ADD8E6\"  # light blue\n}\n\nmodels = list(modelnames_dict.keys())\n\noptions_dict = {\"ents\": models,\n           \"colors\": modelnames_dict\n          }\n\n\n\nfor i, doc in enumerate(model_names_nlp.pipe(sample_models_sentences,\n             as_tuples=False\n            )):\n    new_doc = modify_doc(token_matcher,\n                         phrase_matcher,\n                         doc\n                        )\n    print(f\"\\033[1mProcessing Sentence {i} \\033[1m\")\n    displacy.render(new_doc,\n                    style='ent',\n                    options=options_dict,\n                    minify=True\n                   )\n    print(\"***************\")\n\n\nProcessing Sentence 0 \n***************\nProcessing Sentence 1 \n***************\nProcessing Sentence 2 \n***************\n\n\nGo for ford mustang macheFORD variants. MustangFORD has a deceivingly huge trunk and good horse power. If you want reliability, Toyota LexusTOYOTA should be your choice. LexusTOYOTA has good body style too\n\n\nConsidering the harsh winters here, I am considering 2014 Nissan MuranoNISSAN or the '14 Subaru ForesterSUBARU\n\n\nAmong used cars, I am still not sure what to choose - CivicHONDA or CorollaTOYOTA?"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html#externalizing-rules-from-codes",
    "href": "posts/2021-05-09-spacy_rules_ner.html#externalizing-rules-from-codes",
    "title": "How to Leverage Spacy Rules NER",
    "section": "2. Externalizing Rules from Codes",
    "text": "2. Externalizing Rules from Codes\n\n2A. Saving spacy rules in a json format\n\nNote that the official spacy document advocates josnl format but json is much more readable for multitoken spacy patterns\n\n\n\nCode\nimport json \nfrom pprint import pprint\n\n# \"+1 (901)-985-4567\" or \"(901)-985-4567\" or 901-985-4567 or 901 985 4567\nphone_pattern_1 = [{\"ORTH\": {\"IN\":[\"+1\",\"1\"]},'OP':'?'},\n                   {\"ORTH\": '(', \"OP\":\"?\"}, \n                   {'SHAPE': 'ddd'}, \n                   {\"ORTH\": ')', \"OP\":\"?\"}, \n                   {'ORTH': '-', 'OP':'?'},\n                   {'SHAPE': 'ddd'}, \n                   {'ORTH': '-', 'OP':'?'}, \n                   {'SHAPE': 'dddd'}]\n\n# 901 985 4567\nphone_pattern_2 = [{\"TEXT\": {\"REGEX\": \"\\d{10}\"}}]\n\n# +19019854567\nphone_pattern_3 = [{\"TEXT\": {\"REGEX\": \"\\+1\\d{10}\"}}]\n\nphone_patterns_list = [phone_pattern_1, phone_pattern_2, phone_pattern_3]\n\nspacy_patterns_dict_list = []\n\nfor each_phone_pattern in phone_patterns_list:\n    spacy_patterns_dict_list.append({\"label\":\"PHONE\",\n                                     \"pattern\": each_phone_pattern\n                                    })\n\nwith open('spacy_rules_ner/phone_patterns.json', 'w', encoding='utf-8') as f:\n    json.dump(spacy_patterns_dict_list, f, ensure_ascii=False, indent=1)\n\n\n\n\n2B. Prepare an Entity Ruler loading rules from a json\n\nloaded_spacy_patterns = json.load(open('spacy_rules_ner/phone_patterns.json','r',encoding='utf-8'))\n\nprint(\"Asserting if the loaded spacy patterns are same as the prepared\")\nassert loaded_spacy_patterns == spacy_patterns_dict_list\nprint(\"--&gt; Assertion successful\")\n\nAsserting if the loaded spacy patterns are same as the prepared\n--&gt; Assertion successful\n\n\n\nprint(\"Token-level Spacy Phone Regex\")\npprint(loaded_spacy_patterns)\n\nToken-level Spacy Phone Regex\n[{'label': 'PHONE',\n  'pattern': [{'OP': '?', 'ORTH': {'IN': ['+1', '1']}},\n              {'OP': '?', 'ORTH': '('},\n              {'SHAPE': 'ddd'},\n              {'OP': '?', 'ORTH': ')'},\n              {'OP': '?', 'ORTH': '-'},\n              {'SHAPE': 'ddd'},\n              {'OP': '?', 'ORTH': '-'},\n              {'SHAPE': 'dddd'}]},\n {'label': 'PHONE', 'pattern': [{'TEXT': {'REGEX': '\\\\d{10}'}}]},\n {'label': 'PHONE', 'pattern': [{'TEXT': {'REGEX': '\\\\+1\\\\d{10}'}}]}]\n\n\n\nphone_nlp = spacy.load('en_core_web_sm', disable=['ner'])\nrules_config = {\n    \"validate\": True,\n    \"overwrite_ents\": True,\n}\n\nphone_nlp_rules = phone_nlp.add_pipe(\"entity_ruler\", # invoke entity_ruler pipe \n                                     \"phone_nlp_rules\", # give a name to the pipe\n                                     config=rules_config)\nphone_nlp_rules.add_patterns(loaded_spacy_patterns) # load patterns to the `phone_nlp_rules` pipe of `phone_nlp` model\n\n\n\n2C. Testing on Sample Phone Sentences\n\n\nCode\nphones_list = [\"+1 (901)-985-4567\", \n               \"+1(901)-985-4567\",\n               \"(901) 985 4567\",\n               \"9019854567\"\n              ]\n\n\n\nsample_phone_sentences = [f\"If you want to talk more. Reach me at {each}\" for each in phones_list]\nsample_phone_sentences\n\n\n['If you want to talk more. Reach me at +1 (901)-985-4567',\n 'If you want to talk more. Reach me at +1(901)-985-4567',\n 'If you want to talk more. Reach me at (901) 985 4567',\n 'If you want to talk more. Reach me at 9019854567']\n\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor i, doc in enumerate(phone_nlp.pipe(sample_phone_sentences,\n             as_tuples=False\n            )):\n    print(f\"\\033[1mProcessing Sentence {i} \\033[1m\")\n    displacy.render(doc,\n                    style='ent',\n                   )\n\n\nProcessing Sentence 0 \nProcessing Sentence 1 \nProcessing Sentence 2 \nProcessing Sentence 3 \n\n\nIf you want to talk more. Reach me at +1 (901)-985-4567\n\n\nIf you want to talk more. Reach me at +1(901)-985-4567\n\n\nIf you want to talk more. Reach me at \n\n    (901) 985 4567\n    PHONE\n\n\n\n\nIf you want to talk more. Reach me at \n\n    9019854567\n    PHONE\n\n\n\n\n\nSome of the phone patterns are not captured using the above patterns, let us add a Phone regex that cuts across multiple tokens\n\n\ntext = \"If you want to talk more. Reach me at +1 (901)-985-4567\"\ndoc = phone_nlp(text)\n[(token.text, token.ent_type_) for token in doc]\n\n[('If', ''),\n ('you', ''),\n ('want', ''),\n ('to', ''),\n ('talk', ''),\n ('more', ''),\n ('.', ''),\n ('Reach', ''),\n ('me', ''),\n ('at', ''),\n ('+1', ''),\n ('(', ''),\n ('901)-985', ''),\n ('-', ''),\n ('4567', '')]"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html#advanced-features-in-spacy-rules-ner",
    "href": "posts/2021-05-09-spacy_rules_ner.html#advanced-features-in-spacy-rules-ner",
    "title": "How to Leverage Spacy Rules NER",
    "section": "3. Advanced Features in Spacy Rules NER",
    "text": "3. Advanced Features in Spacy Rules NER\n\n3A. Adding RegEx patterns as a custom component\n\n\nCode\nimport re \nfrom spacy import Language\nfrom spacy.tokens import Span\n\ndef load_entity_ruler_based_phone_pattern(location_spacy_json):\n    loaded_spacy_patterns = json.load(open(location_spacy_json,'r',encoding='utf-8'))\n    phone_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n    rules_config = {\n        \"validate\": True,\n        \"overwrite_ents\": True,\n    }\n\n    phone_nlp_rules = phone_nlp.add_pipe(\"entity_ruler\", # invoke entity_ruler pipe \n                                         \"phone_nlp_rules\", # give a name to the pipe\n                                         config=rules_config)\n    phone_nlp_rules.add_patterns(loaded_spacy_patterns) # load patterns to the `phone_nlp_rules` pipe of `phone_nlp` model\n    return phone_nlp\n       \nlocation_spacy_json = 'spacy_rules_ner/phone_patterns.json'\n\nphone_nlp = load_entity_ruler_based_phone_pattern(location_spacy_json)\n\nprint(\"Pipeline Components before adding regex custom component:\")\nprint(phone_nlp.pipe_names)\nprint()\nprint(\"Entities tracked in phone_nlp_rules\")\nprint(phone_nlp.pipe_labels['phone_nlp_rules'])\n\nphone_regex_pattern = r\"([+]?[\\d]?[\\d]?.?[(]?\\d{3}[)]?.?\\d{3}.?\\d{4})\"\n\n#noting the token position for every character in a doc\n\ndef generate_chars2tokens_dict(doc):\n    chars_to_tokens = {}\n    for token in doc:\n        for i in range(token.idx, token.idx + len(token.text)):\n            chars_to_tokens[i] = token.i\n    return chars_to_tokens\n\n\n@Language.component(\"phone_multitoken_regex_capture\")\ndef phone_multitoken_regex_capture(doc):\n    original_ents = list(doc.ents)\n    chars_to_tokens = generate_chars2tokens_dict(doc)\n    phone_regex_ents = []\n    for match in re.finditer(phone_regex_pattern, doc.text):\n        start, end = match.span()\n        span = doc.char_span(start, end)\n        if span is not None:\n            phone_regex_ents.append((span.start, span.end, span.text))\n        else:\n            start_token = chars_to_tokens.get(start)\n            end_token = chars_to_tokens.get(end)\n            if start_token is not None and end_token is not None:\n                span = doc[start_token:end_token + 1]\n                phone_regex_ents.append((span.start, span.end, span.text))\n    for regex_ent in phone_regex_ents:\n        start_char, end_char, span_text = regex_ent\n        proper_spacy_ent = Span(doc, start_char, end_char, label=\"PHONE\")\n        original_ents.append(proper_spacy_ent)     \n    filtered = filter_spans(original_ents) #removes overlapping ents\n    doc.ents = filtered\n    return doc\n\n\nphone_nlp.add_pipe(\"phone_multitoken_regex_capture\", after=\"phone_nlp_rules\")\n\nprint(\"Pipeline Components after adding regex custom component:\")\nprint(phone_nlp.pipe_names)\n\n# inspiration for the above code piece: \n# https://spacy.io/usage/rule-based-matching#regex-text\n\n\nPipeline Components before adding regex custom component:\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'phone_nlp_rules']\n\nEntities tracked in phone_nlp_rules\n['PHONE']\nPipeline Components after adding regex custom component:\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'phone_nlp_rules', 'phone_multitoken_regex_capture']\n\n\n\nfor i, doc in enumerate(phone_nlp.pipe(sample_phone_sentences,\n             as_tuples=False\n            )):\n    print(f\"\\033[1mProcessing Sentence {i} \\033[1m\")\n    displacy.render(doc,\n                    style='ent',\n                   )\n\nProcessing Sentence 0 \nProcessing Sentence 1 \nProcessing Sentence 2 \nProcessing Sentence 3 \n\n\nIf you want to talk more. Reach me at \n\n    +1 (901)-985-4567\n    PHONE\n\n\n\n\nIf you want to talk more. Reach me at \n\n    +1(901)-985-4567\n    PHONE\n\n\n\n\nIf you want to talk more. Reach me at \n\n    (901) 985 4567\n    PHONE\n\n\n\n\nIf you want to talk more. Reach me at \n\n    9019854567\n    PHONE\n\n\n\n\n\nBy adding the custom component, we are able to capture the missed out sentences also\n\n\n\n3B. Chaining Spacy NER components\n\nChaining spacy NER components makes the patterns more manageable  It is similar to modular programming but for building a complex spacy NER rules\n\nLet us discuss creation of a first degree NER + second degree NER (chaining NER) written on top of first degree NER entities\nIt is better to train a model for ADDRESS entity. But for the sake of explanation of the Chiaining NER technique, let us build an ADDRESS NER using spacy rules\n\n\nCode\n# Goal: To capture the different types of ADDRESSES in the following ADDRESS SENTENCES\nsample_address_sentences = ['My office is located at 1 American rd, Dearborn, MI 48126, United States',\n 'My office is located at one American Road Dearborb Michigan 48126 United States',\n 'My office is located at 1 American High way, South Dakota 48126, United States',\n 'My office is located at 1 American rd, Dearborn, MI, United States',\n 'My office is located at 717 N 2ND ST, MANKATO, MN 56001 US',\n 'My office is located at 717 N 2ND ST, MANKATO, MN, 56001',\n 'My office is located at 717 N 2ND ST MANKATO MN 56001',\n 'My office is located at Dearborn Michigan',\n 'My office is located at Chennai, TamilNadu',\n 'My office is located at Dearborn, Michigan',\n 'My office is located at PO Box 107050, Albany, NY 12201-7050',\n 'My office is located at PO Box 107050, Albany, NY 12201',\n 'My office is located at P.O. Box 107050, Albany, NY 12201-7050',\n 'My office is located at P.O. Box 107050, Albany, NY 12201']\n\n\n\n\nCode\n# Capture following first degree NER entities\n\n\n# `DOOR_NUM` entity to capture \n# `STREET_NAME` entity\n# `CITY`\n# `STATE`\n# `COUNTRY`\n# `ZIP_CODE`\n# `P_O_BOX`\n\n\n# one or more of the above 1st degree NER entities form the final ADDRESS entity\n\ndoor_num_spacy_pattern = [{\"label\":\"POTENTIAL_DOOR_NUM\",\n                           \"pattern\":[{\"LOWER\":{\"REGEX\":\"\\\\b([0-9]{1,4}|one|two|three|four|five|six|seven|eight|nine|ten)\\\\b\"}}]\n                         }]\n\nstreet_spacy_pattern = [{\"label\":\"POTENTIAL_STREET_NAME\",\n                         \"pattern\":[{\"TEXT\":{\"REGEX\": \"^(N|S|E|W)$\"},\"OP\":\"?\"},\n                                    {\"TEXT\":{\"REGEX\":\"^[A-Z][a-zA-Z]+$|\\d(st|nd|rd|th|ST|ND|RD|TH)|[Ff]irst|[Ss]econd|[Tt]hird|[Ff]ourth|[Ff]ifth|[Ss]ixth|[Ss]eventh|[Ee]ighth|[Nn]inth|[Tt]enth\"},\"OP\":\"+\"},\n                                    {\"LOWER\":{\"REGEX\":\"\\\\b(street|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|park|parkway|pkwy|circle|cir|boulevard|blvd|high|park|way|cross)\\\\b\"},\"OP\":\"+\"}]\n                       }]\n\ncity_or_country_spacy_pattern = [{\"label\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\n                                 \"pattern\":[{\"TEXT\":{\"REGEX\":\"^[A-Z][a-zA-Z]+$\"},\"OP\":\"+\", \"TAG\":{\"REGEX\":\"^NN|HYPH\"},}],\n                               }]\n\nzip_code_spacy_pattern =  [{\"label\":\"ZIP_CODE\",\n                            \"pattern\": [{\"TEXT\":{\"REGEX\":\"\\\\b\\\\d{5}\\\\b\"}},\n                                        {\"ORTH\":\"-\",\"OP\":\"?\"},\n                                        {\"TEXT\":{\"REGEX\":\"\\\\b\\\\d+\\\\b\"},\"OP\":\"?\"} \n                                       ]  \n                                }] \n\np_o_box_pattern = [{\"label\":\"P_O_BOX\",\n                     \"pattern\":[{\"LOWER\":{\"IN\":[\"po\",\"p.o\",\"p.o.\",\"post\"]}},\n                                {\"LOWER\":{\"IN\":[\"office\",\".\"]},\"OP\":\"?\"},\n                                {\"LOWER\":{\"IN\":[\"box\"]}},\n                                {\"TEXT\":{\"REGEX\":\"\\\\b\\\\d+\\\\b\"}}\n                               ]  \n                   }]\n\nfirst_degree_address_patterns = door_num_spacy_pattern + street_spacy_pattern + city_or_country_spacy_pattern + p_o_box_pattern + zip_code_spacy_pattern\n\ndef create_first_degree_address_nlp(first_degree_address_patterns):\n    first_degree_address_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n    rules_config = {\"validate\": True,\n                    \"overwrite_ents\": True,\n                   }\n    first_degree_rules = first_degree_address_nlp.add_pipe(\"entity_ruler\",\n                                                           \"first_degree_rules\",\n                                                           config=rules_config)\n    first_degree_rules.add_patterns(first_degree_address_patterns)\n    return first_degree_address_nlp                                \n\n\n\nfirst_degree_address_nlp = create_first_degree_address_nlp(first_degree_address_patterns)\n\nfor i, doc in enumerate(first_degree_address_nlp.pipe(sample_address_sentences,\n             as_tuples=False\n            )):\n    print(f\"\\033[1mProcessing Sentence {i} \\033[1m\")\n    displacy.render(doc,\n                    style='ent',\n                   )\n\nProcessing Sentence 0 \nProcessing Sentence 1 \nProcessing Sentence 2 \nProcessing Sentence 3 \nProcessing Sentence 4 \nProcessing Sentence 5 \nProcessing Sentence 6 \nProcessing Sentence 7 \nProcessing Sentence 8 \nProcessing Sentence 9 \nProcessing Sentence 10 \nProcessing Sentence 11 \nProcessing Sentence 12 \nProcessing Sentence 13 \n\n\nMy office is located at \n\n    1\n    POTENTIAL_DOOR_NUM\n\n \n\n    American rd\n    POTENTIAL_STREET_NAME\n\n, \n\n    Dearborn\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    MI\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    48126\n    ZIP_CODE\n\n, \n\n    United States\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    one\n    POTENTIAL_DOOR_NUM\n\n \n\n    American Road Dearborb Michigan\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    48126\n    ZIP_CODE\n\n \n\n    United States\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    1\n    POTENTIAL_DOOR_NUM\n\n \n\n    American High way\n    POTENTIAL_STREET_NAME\n\n, \n\n    South Dakota\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    48126\n    ZIP_CODE\n\n, \n\n    United States\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    1\n    POTENTIAL_DOOR_NUM\n\n \n\n    American rd\n    POTENTIAL_STREET_NAME\n\n, \n\n    Dearborn\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    MI\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    United States\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    717\n    POTENTIAL_DOOR_NUM\n\n \n\n    N 2ND ST\n    POTENTIAL_STREET_NAME\n\n, \n\n    MANKATO\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    MN\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    56001\n    ZIP_CODE\n\n \n\n    US\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    717\n    POTENTIAL_DOOR_NUM\n\n \n\n    N 2ND ST\n    POTENTIAL_STREET_NAME\n\n, \n\n    MANKATO\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    MN\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    56001\n    ZIP_CODE\n\n\n\n\nMy office is located at \n\n    717\n    POTENTIAL_DOOR_NUM\n\n \n\n    N 2ND ST\n    POTENTIAL_STREET_NAME\n\n \n\n    MANKATO MN\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    56001\n    ZIP_CODE\n\n\n\n\nMy office is located at \n\n    Dearborn Michigan\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    Chennai\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    TamilNadu\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    Dearborn\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    Michigan\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n\n\n\nMy office is located at \n\n    PO Box 107050\n    P_O_BOX\n\n, \n\n    Albany\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    NY\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    12201-7050\n    ZIP_CODE\n\n\n\n\nMy office is located at \n\n    PO Box 107050\n    P_O_BOX\n\n, \n\n    Albany\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    NY\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    12201\n    ZIP_CODE\n\n\n\n\nMy office is located at \n\n    P.O. Box 107050\n    P_O_BOX\n\n, \n\n    Albany\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    NY\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    12201-7050\n    ZIP_CODE\n\n\n\n\nMy office is located at \n\n    P.O. Box 107050\n    P_O_BOX\n\n, \n\n    Albany\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n, \n\n    NY\n    POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\n\n \n\n    12201\n    ZIP_CODE\n\n\n\n\n\n# pattern for: 1 American rd, Dearborn, MI 48126, United States\n# pattenr for: 1 American High way, South Dakota 48126, United States\n# pattern for: 1 American rd, Dearborn, MI, United States\n# pattern for: 717 N 2ND ST, MANKATO, MN 56001 US\nADDRESS_PATTERN_1 = [{\"label\":\"ADDRESS\",\n                     \"pattern\":[{\"ENT_TYPE\":\"POTENTIAL_DOOR_NUM\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                {\"ENT_TYPE\":\"POTENTIAL_STREET_NAME\",\"OP\":\"+\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"*\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"*\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                {\"ENT_TYPE\":\"ZIP_CODE\",\"OP\":\"*\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"}, \n                                {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"+\"},\n                               ]  \n                   },\n                   ]\n\n# pattern for: one American Road Dearborb Michigan 48126 United States\nADDRESS_PATTERN_2 = [{\"label\":\"ADDRESS\",\n                     \"pattern\":[{\"ENT_TYPE\":\"POTENTIAL_DOOR_NUM\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"+\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                {\"ENT_TYPE\":\"ZIP_CODE\",\"OP\":\"*\"},\n                                {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"}, \n                                {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"+\"},\n                               ]  \n                   },\n                   ]\n\n# 717 N 2ND ST, MANKATO, MN, 56001\n# 717 N 2ND ST MANKATO MN 56001\nADDRESS_PATTERN_3 = [{\"label\": \"ADDRESS\",\n                      \"pattern\":[{\"ENT_TYPE\":\"POTENTIAL_DOOR_NUM\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                 {\"ENT_TYPE\":\"POTENTIAL_STREET_NAME\",\"OP\":\"+\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                 {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"+\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                 {\"ENT_TYPE\":\"ZIP_CODE\",\"OP\":\"+\"},\n                      ]\n                     }\n                    ]\n\n# Chennai, TamilNadu\n# Dearborn Michigan\nADDRESS_PATTERN_4 = [{\"label\": \"ADDRESS\",\n                      \"pattern\":[{\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"+\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                      ]\n                     }\n                    ]\n\n# PO Box 107050, Albany, NY 12201-7050\n# PO Box 107050, Albany, NY 12201\n# PO Box 107050, Albany, NY, US 12201\nADDRESS_PATTERN_5 = [{\"label\": \"ADDRESS\",\n                      \"pattern\":[{\"ENT_TYPE\":\"P_O_BOX\",\"OP\":\"+\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                 {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"+\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                 {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"+\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                 {\"ENT_TYPE\":\"POTENTIAL_CITY_OR_STATE_OR_COUNTRY_NAME\",\"OP\":\"?\"},\n                                 {\"TEXT\":{\"REGEX\":\"\\\\W{1,2}\"},\"OP\":\"?\"},\n                                 {\"ENT_TYPE\":\"ZIP_CODE\",\"OP\":\"+\"},\n                                 \n                      ]\n                     }\n                    ]\n\nsecond_degree_address_patterns = ADDRESS_PATTERN_1 + ADDRESS_PATTERN_2 + ADDRESS_PATTERN_3 + ADDRESS_PATTERN_4 + ADDRESS_PATTERN_5\n\ndef create_second_degree_address_nlp(first_degree_address_patterns,\n                                          second_degree_address_patterns\n                                         ):\n    second_degree_address_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n    rules_config = {\"validate\": True,\n                    \"overwrite_ents\": True,\n                   }\n    first_degree_rules = second_degree_address_nlp.add_pipe(\"entity_ruler\",\n                                                           \"first_degree_rules\",\n                                                           config=rules_config)\n    first_degree_rules.add_patterns(first_degree_address_patterns)\n    \n    second_degree_rules = second_degree_address_nlp.add_pipe(\"entity_ruler\",\n                                                           \"second_degree_rules\",\n                                                           config=rules_config,\n                                                           after='first_degree_rules')\n    second_degree_rules.add_patterns(second_degree_address_patterns)\n    return second_degree_address_nlp\n\n\nsecond_degree_address_nlp = create_second_degree_address_nlp(first_degree_address_patterns, second_degree_address_patterns)\n\nfor i, doc in enumerate(second_degree_address_nlp.pipe(sample_address_sentences,\n             as_tuples=False\n            )):\n    print(f\"\\033[1mProcessing Sentence {i} \\033[1m\")\n    displacy.render(doc,\n                    style='ent',\n                   )\n\nProcessing Sentence 0 \nProcessing Sentence 1 \nProcessing Sentence 2 \nProcessing Sentence 3 \nProcessing Sentence 4 \nProcessing Sentence 5 \nProcessing Sentence 6 \nProcessing Sentence 7 \nProcessing Sentence 8 \nProcessing Sentence 9 \nProcessing Sentence 10 \nProcessing Sentence 11 \nProcessing Sentence 12 \nProcessing Sentence 13 \n\n\nMy office is located at \n\n    1 American rd, Dearborn, MI 48126, United States\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    one American Road Dearborb Michigan 48126 United States\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    1 American High way, South Dakota 48126, United States\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    1 American rd, Dearborn, MI, United States\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    717 N 2ND ST, MANKATO, MN 56001 US\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    717 N 2ND ST, MANKATO, MN\n    ADDRESS\n\n, \n\n    56001\n    ZIP_CODE\n\n\n\n\nMy office is located at \n\n    717 N 2ND ST MANKATO MN 56001\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    Dearborn Michigan\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    Chennai,\n    ADDRESS\n\n \n\n    TamilNadu\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    Dearborn,\n    ADDRESS\n\n \n\n    Michigan\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    PO Box 107050, Albany, NY 12201-7050\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    PO Box 107050, Albany, NY 12201\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    P.O. Box 107050, Albany, NY 12201-7050\n    ADDRESS\n\n\n\n\nMy office is located at \n\n    P.O. Box 107050, Albany, NY 12201\n    ADDRESS"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html#how-to-save-rules-ner-as-a-package",
    "href": "posts/2021-05-09-spacy_rules_ner.html#how-to-save-rules-ner-as-a-package",
    "title": "How to Leverage Spacy Rules NER",
    "section": "4. How to save Rules NER as a package",
    "text": "4. How to save Rules NER as a package\n\nSave the rules ner model phone_nlp to a physical location using nlp.to_disk\nSave the custom components in a py file spacy_rules_ner/phone_functions.py\nuse python -m spacy package input_dir output_dir --code location/to/custom_components.py --name new_model_name to generate .tar.gz format package\nPip install the tar.gz file using pip install location/to/tar.gz\nspacy.load('new_model_name') will load your package with custom components\n\n\n!mkdir -p spacy_rules_ner/phone_nlp\n\n\n\nCode\n# Let us save the `phone_nlp` and the custom component pipeline \n\nphone_nlp.to_disk('spacy_rules_ner/phone_nlp')\n\n\n\n!ls spacy_rules_ner/phone_nlp\n\nattribute_ruler  lemmatizer  ner     phone_nlp_rules  tagger   tokenizer\nconfig.cfg   meta.json   parser  senter       tok2vec  vocab\n\n\n\n%%writefile spacy_rules_ner/phone_functions.py\n\nimport json\nimport spacy\nimport re \nfrom spacy import Language\nfrom spacy.tokens import Span\nfrom spacy.util import filter_spans\n\ndef load_entity_ruler_based_phone_pattern(location_spacy_json):\n    loaded_spacy_patterns = json.load(open(location_spacy_json,'r',encoding='utf-8'))\n    phone_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n    rules_config = {\n        \"validate\": True,\n        \"overwrite_ents\": True,\n    }\n\n    phone_nlp_rules = phone_nlp.add_pipe(\"entity_ruler\", # invoke entity_ruler pipe \n                                         \"phone_nlp_rules\", # give a name to the pipe\n                                         config=rules_config)\n    phone_nlp_rules.add_patterns(loaded_spacy_patterns) # load patterns to the `phone_nlp_rules` pipe of `phone_nlp` model\n    return phone_nlp\n       \nlocation_spacy_json = 'spacy_rules_ner/phone_patterns.json'\n\nphone_nlp = load_entity_ruler_based_phone_pattern(location_spacy_json)\n\nprint(\"Pipeline Components before adding regex custom component:\")\nprint(phone_nlp.pipe_names)\nprint()\nprint(\"Entities tracked in phone_nlp_rules\")\nprint(phone_nlp.pipe_labels['phone_nlp_rules'])\n\nphone_regex_pattern = r\"([+]?[\\d]?[\\d]?.?[(]?\\d{3}[)]?.?\\d{3}.?\\d{4})\"\n\n#noting the token position for every character in a doc\n\ndef generate_chars2tokens_dict(doc):\n    chars_to_tokens = {}\n    for token in doc:\n        for i in range(token.idx, token.idx + len(token.text)):\n            chars_to_tokens[i] = token.i\n    return chars_to_tokens\n\n\n@Language.component(\"phone_multitoken_regex_capture\")\ndef phone_multitoken_regex_capture(doc):\n    original_ents = list(doc.ents)\n    chars_to_tokens = generate_chars2tokens_dict(doc)\n    phone_regex_ents = []\n    for match in re.finditer(phone_regex_pattern, doc.text):\n        start, end = match.span()\n        span = doc.char_span(start, end)\n        if span is not None:\n            phone_regex_ents.append((span.start, span.end, span.text))\n        else:\n            start_token = chars_to_tokens.get(start)\n            end_token = chars_to_tokens.get(end)\n            if start_token is not None and end_token is not None:\n                span = doc[start_token:end_token + 1]\n                phone_regex_ents.append((span.start, span.end, span.text))\n    for regex_ent in phone_regex_ents:\n        start_char, end_char, span_text = regex_ent\n        proper_spacy_ent = Span(doc, start_char, end_char, label=\"PHONE\")\n        original_ents.append(proper_spacy_ent)     \n    filtered = filter_spans(original_ents) #removes overlapping ents\n    doc.ents = filtered\n    return doc\n\n\nphone_nlp.add_pipe(\"phone_multitoken_regex_capture\", after=\"phone_nlp_rules\")\n\nprint(\"Pipeline Components after adding regex custom component:\")\nprint(phone_nlp.pipe_names)\n\n# inspiration for the above code piece: \n# https://spacy.io/usage/rule-based-matching#regex-text\n\nOverwriting spacy_rules_ner/phone_functions.py\n\n\n\n# create the output_dir\n!mkdir -p ./spacy_rules_ner/packaged_phone_nlp\n\n\n# now let us package the `phone_nlp`\n\n!python3 -m spacy package ./spacy_rules_ner/phone_nlp ./spacy_rules_ner/packaged_phone_nlp --code spacy_rules_ner/phone_functions.py --name phone_nlp_2\n\nℹ Building package artifacts: sdist\nPipeline Components before adding regex custom component:\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'phone_nlp_rules']\n\nEntities tracked in phone_nlp_rules\n['PHONE']\nPipeline Components after adding regex custom component:\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'phone_nlp_rules', 'phone_multitoken_regex_capture']\n✔ Including 1 Python module(s) with custom code\n✔ Loaded meta.json from file\nspacy_rules_ner/phone_nlp/meta.json\n✔ Generated README.md from meta.json\n✔ Successfully created package 'en_phone_nlp_2-3.1.0'\nspacy_rules_ner/packaged_phone_nlp/en_phone_nlp_2-3.1.0\nrunning sdist\nrunning egg_info\ncreating en_phone_nlp_2.egg-info\nwriting en_phone_nlp_2.egg-info/PKG-INFO\nwriting dependency_links to en_phone_nlp_2.egg-info/dependency_links.txt\nwriting entry points to en_phone_nlp_2.egg-info/entry_points.txt\nwriting requirements to en_phone_nlp_2.egg-info/requires.txt\nwriting top-level names to en_phone_nlp_2.egg-info/top_level.txt\nwriting manifest file 'en_phone_nlp_2.egg-info/SOURCES.txt'\nreading manifest file 'en_phone_nlp_2.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwarning: no files found matching 'LICENSE'\nwarning: no files found matching 'LICENSES_SOURCES'\nwriting manifest file 'en_phone_nlp_2.egg-info/SOURCES.txt'\nrunning check\ncreating en_phone_nlp_2-3.1.0\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/attribute_ruler\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/lemmatizer\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/lemmatizer/lookups\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/ner\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/parser\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/phone_nlp_rules\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/senter\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/tagger\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/tok2vec\ncreating en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab\ncopying files to en_phone_nlp_2-3.1.0...\ncopying MANIFEST.in -&gt; en_phone_nlp_2-3.1.0\ncopying README.md -&gt; en_phone_nlp_2-3.1.0\ncopying meta.json -&gt; en_phone_nlp_2-3.1.0\ncopying setup.py -&gt; en_phone_nlp_2-3.1.0\ncopying en_phone_nlp_2/__init__.py -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2\ncopying en_phone_nlp_2/meta.json -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2\ncopying en_phone_nlp_2/phone_functions.py -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2\ncopying en_phone_nlp_2.egg-info/PKG-INFO -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncopying en_phone_nlp_2.egg-info/SOURCES.txt -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncopying en_phone_nlp_2.egg-info/dependency_links.txt -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncopying en_phone_nlp_2.egg-info/entry_points.txt -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncopying en_phone_nlp_2.egg-info/not-zip-safe -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncopying en_phone_nlp_2.egg-info/requires.txt -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncopying en_phone_nlp_2.egg-info/top_level.txt -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2.egg-info\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/README.md -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/config.cfg -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/meta.json -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/tokenizer -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/attribute_ruler/patterns -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/attribute_ruler\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/lemmatizer/lookups/lookups.bin -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/lemmatizer/lookups\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/ner/cfg -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/ner\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/ner/model -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/ner\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/ner/moves -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/ner\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/parser/cfg -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/parser\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/parser/model -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/parser\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/parser/moves -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/parser\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/phone_nlp_rules/cfg -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/phone_nlp_rules\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/phone_nlp_rules/patterns.jsonl -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/phone_nlp_rules\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/senter/cfg -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/senter\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/senter/model -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/senter\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/tagger/cfg -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/tagger\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/tagger/model -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/tagger\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/tok2vec/cfg -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/tok2vec\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/tok2vec/model -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/tok2vec\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab/key2row -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab/lookups.bin -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab/strings.json -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab\ncopying en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab/vectors -&gt; en_phone_nlp_2-3.1.0/en_phone_nlp_2/en_phone_nlp_2-3.1.0/vocab\nWriting en_phone_nlp_2-3.1.0/setup.cfg\ncreating dist\nCreating tar archive\nremoving 'en_phone_nlp_2-3.1.0' (and everything under it)\n✔ Successfully created zipped Python package\nspacy_rules_ner/packaged_phone_nlp/en_phone_nlp_2-3.1.0/dist/en_phone_nlp_2-3.1.0.tar.gz\n\n\n\nThe generated tar.gz files can be shared and pip installed\n\n\n!pip install spacy_rules_ner/packaged_phone_nlp/en_phone_nlp_2-3.1.0/dist/en_phone_nlp_2-3.1.0.tar.gz\n\nDefaulting to user installation because normal site-packages is not writeable\nProcessing ./spacy_rules_ner/packaged_phone_nlp/en_phone_nlp_2-3.1.0/dist/en_phone_nlp_2-3.1.0.tar.gz\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: spacy&lt;3.2.0,&gt;=3.1.3 in /usr/local/lib/python3.7/dist-packages (from en-phone-nlp-2==3.1.0) (3.1.3)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.26.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (3.0.5)\nRequirement already satisfied: pathy&gt;=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (0.6.0)\nRequirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (0.4.0)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.4.1)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.0.6)\nRequirement already satisfied: typing-extensions&lt;4.0.0.0,&gt;=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (3.10.0.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (1.8.2)\nRequirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (8.0.10)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (4.62.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (3.0.1)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (0.7.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (59.1.1)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (21.0)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.0.5)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (3.0.8)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (1.0.5)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (0.8.2)\nRequirement already satisfied: numpy&gt;=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (1.21.2)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;2.1.0,&gt;=2.0.6-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (3.5.0)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.4.7)\nRequirement already satisfied: smart-open&lt;6.0.0,&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (5.2.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python3/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2019.11.28)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.0.7)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/lib/python3/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.8)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/lib/python3/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (1.25.8)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (7.1.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;spacy&lt;3.2.0,&gt;=3.1.3-&gt;en-phone-nlp-2==3.1.0) (2.0.1)\nBuilding wheels for collected packages: en-phone-nlp-2\n  Building wheel for en-phone-nlp-2 (setup.py) ... done\n  Created wheel for en-phone-nlp-2: filename=en_phone_nlp_2-3.1.0-py3-none-any.whl size=13618384 sha256=86d51e0f37b26c932184654dd4b82ad7475f0cf41d1e32499263b505a4ea151a\n  Stored in directory: /path/to/dir/.cache/pip/wheels/04/04/8d/81eaf26a25f7dfa433e1be3ad7e6524e81e9a4172a3d9d0d06\nSuccessfully built en-phone-nlp-2\nInstalling collected packages: en-phone-nlp-2\nSuccessfully installed en-phone-nlp-2-3.1.0\nWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\nYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\n\nphone_nlp_new = spacy.load('en_phone_nlp_2')\n\nPipeline Components before adding regex custom component:\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'phone_nlp_rules']\n\nEntities tracked in phone_nlp_rules\n['PHONE']\nPipeline Components after adding regex custom component:\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'phone_nlp_rules', 'phone_multitoken_regex_capture']\n\n\n\nfor i, doc in enumerate(phone_nlp_new.pipe(sample_phone_sentences,\n             as_tuples=False\n            )):\n    print(f\"\\033[1mProcessing Sentence {i} \\033[1m\")\n    displacy.render(doc,\n                    style='ent',\n                   )\n\nProcessing Sentence 0 \nProcessing Sentence 1 \nProcessing Sentence 2 \nProcessing Sentence 3 \n\n\nIf you want to talk more. Reach me at \n\n    +1 (901)-985-4567\n    PHONE\n\n\n\n\nIf you want to talk more. Reach me at \n\n    +1(901)-985-4567\n    PHONE\n\n\n\n\nIf you want to talk more. Reach me at \n\n    (901) 985 4567\n    PHONE\n\n\n\n\nIf you want to talk more. Reach me at \n\n    9019854567\n    PHONE"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html#conclusion",
    "href": "posts/2021-05-09-spacy_rules_ner.html#conclusion",
    "title": "How to Leverage Spacy Rules NER",
    "section": "5. Conclusion",
    "text": "5. Conclusion\n\nBy building ner rules models for MODEL_NAMES, PHONE and ADDRESS entities we discussed the following concepts:\n\nSpacy’s Token Matcher, Phrase Matcher and our own custom component Regex Matcher\nHow to load Spacy patterns from a json file\nHow to chain NER entities\nHow to save and load ner pipeline with custome component"
  },
  {
    "objectID": "posts/2021-05-09-spacy_rules_ner.html#references",
    "href": "posts/2021-05-09-spacy_rules_ner.html#references",
    "title": "How to Leverage Spacy Rules NER",
    "section": "References",
    "text": "References\n\nSpacy Rules based Matching | link"
  },
  {
    "objectID": "posts/2021-09-15-pytorch_for_nlp_part_ii.html",
    "href": "posts/2021-09-15-pytorch_for_nlp_part_ii.html",
    "title": "PyTorch Fundamentals for NLP - Part 2",
    "section": "",
    "text": "In this blog piece, let us cover how we can build a - text classification application using an embedding + fc layer"
  },
  {
    "objectID": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#introduction",
    "href": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#introduction",
    "title": "PyTorch Fundamentals for NLP - Part 2",
    "section": "",
    "text": "In this blog piece, let us cover how we can build a - text classification application using an embedding + fc layer"
  },
  {
    "objectID": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#representing-text-as-tensors---a-quick-introduction",
    "href": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#representing-text-as-tensors---a-quick-introduction",
    "title": "PyTorch Fundamentals for NLP - Part 2",
    "section": "2.Representing Text as Tensors - A Quick Introduction",
    "text": "2.Representing Text as Tensors - A Quick Introduction\nHow do computers represent text? - Using encodings such as ASCII values to represent each character\n\nSource: github.com/MicrosoftDocs/pytorchfundamentals\n\nStill computers cannot interpret the meaning of the words , they just represent text as ascii numbers in the above image\n\nHow is text converted into embeddings? \n\nTwo types of representations to convert text into numbers\n\nCharacter-level representation\nWord-level representation\nToken or sub-word level representation\n\nWhile Character-level and Word-level representations are self explanatory, Token-level representation is a combination of the above two approaches.\n\nSome important terms: \n\nTokenization (sentence/text –&gt; tokens): In the case sub-word level representations, for example, unfriendly will be tokenized as un, #friend, #ly where # indicates the token is a continuation of previous token.\nThis way of tokenization can make the model learnt/trained representations for friend and unfriendly to be closer to each other in the vector spacy\nNumericalization (tokens –&gt; numericals): This is the step where we convert tokens into integers.\nVectorization (numericals –&gt; vectors): This is the process of creating vectors (typically sparse and equal to the length of the vocabulary of the corpus analyzed)\nEmbedding (numericals –&gt; embeddings): For text data, embedding is a lower dimensional equivalent of a higher dimensional sparse vector. Embeddings are typically dense. Vectors are sparse.\n\n\nTypical Process of Embedding Creation  - text_data &gt;&gt; tokens &gt;&gt; numericals &gt;&gt; sparse vectors or dense embeddings"
  },
  {
    "objectID": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#difference-between-nn.embeddingbag-vs-nn.embedding",
    "href": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#difference-between-nn.embeddingbag-vs-nn.embedding",
    "title": "PyTorch Fundamentals for NLP - Part 2",
    "section": "3. Difference between nn.EmbeddingBag vs nn.Embedding",
    "text": "3. Difference between nn.EmbeddingBag vs nn.Embedding\n\nnn.Embedding: A simple lookup table that looks up embeddings in a fixed dictionary and size.\nnn.EmbeddingBag: Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.\n\nSource: PyTorch Official Documentation\n\nnn.Embedding Explanation: - In the above pic, we can see that the encoding of men write code being embedded as [(0.312,0.385), (0.543, 0.481), (0.203, 0.404)] where embed_dim=2. - Looking closer, men is embedded as (0.312,0.385) and the trailing &lt;pad&gt; token is embedded as (0.203, 0.404)\nnn.EmbeddingBag Explanation:  - In here, there is no padding token. The sentences in a batch are connnected together and saved with their offsets array - Instead of each word being represented by an embedding vector, each sentence is embedded into a embedding vector - This above process of “computing a single vector for an entire sentence” is possible also from nn.Embedding followed by torch.mean(dim=1) or torch.sum(dim=1) or torch.max(dim=1)\nSo, when to use nn.EmbeddingBag? - nn.EmbeddingBag works better when sequential information of words is not needed. - Hence can be used with simple Feed forward NN and not with LSTMs or Transformers (all the embedded words are sent at once and they are sequentially processed either from both directions or unidirectional)\nSources:  - nn.EmbeddingBag vs nn.Embedding | link - nn.Emedding followed by torch.mean(dim=1) | link"
  },
  {
    "objectID": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#a-text-classification-pipeline-using-nn.embeddingbag-nn.linear-layer",
    "href": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#a-text-classification-pipeline-using-nn.embeddingbag-nn.linear-layer",
    "title": "PyTorch Fundamentals for NLP - Part 2",
    "section": "4. A Text Classification Pipeline using nn.EmbeddingBag + nn.linear Layer",
    "text": "4. A Text Classification Pipeline using nn.EmbeddingBag + nn.linear Layer\n\nDataset considered: AG_NEWS dataset that consists of 4 classes - World, Sports, Business and Sci/Tech\n\n┣━━ 1.Loading dataset  ┃ ┣━━ torch.data.utils.datasets.AG_NEWS  ┣━━ 2.Load Tokenization  ┃ ┣━━ torchtext.data.utils.get_tokenizer('basic_english')  ┣━━ 3.Build vocabulary  ┃ ┣━━ torchtext.vocab.build_vocab_from_iterator(train_iterator)  ┣━━ 4.Create EmbeddingsBag ┃ ┣━━ Create collate_fn to create triplets of label-feature-offsets tensors for every minibatch  ┣━━ 5.Create train, validation and test DataLoaders ┣━━ 6.Define Model_Architecture ┣━━ 7.define training_loop and testing_loop functions ┣━━ 8.Train the model and Evaluate on Test Data ┣━━ 9.Test the model on sample text\nImporting basic modules\n\n\nCode\nimport torch\nimport torchtext\nimport os\nimport collections\nimport random\nimport numpy as np\n\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\n\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n4.1. Loading dataset\n\n\nCode\ndef load_dataset(ngrams=1):\n    print(\"Loading dataset ...\")\n    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n    train_dataset = list(train_dataset)\n    test_dataset = list(test_dataset)\n    return train_dataset, test_dataset\n\n\n\ntrain_dataset, test_dataset = load_dataset()\n\nLoading dataset ...\n\n\n\nclasses = ['World', 'Sports', 'Business', 'Sci/Tech']\n\n\n\n4.2. Loading Tokenizer\n\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n\n\n\n4.3. Building Vocabulary\n\n\nCode\ndef _yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n\ndef create_vocab(train_dataset):\n    print(\"Building vocabulary ..\")\n    vocab = build_vocab_from_iterator(_yield_tokens(train_dataset),\n                                      min_freq=1,\n                                      specials=['&lt;unk&gt;']\n                                     )\n    vocab.set_default_index(vocab['&lt;unk&gt;'])\n    return vocab\n\n\n\nvocab = create_vocab(train_dataset)\n\nBuilding vocabulary ..\n\n\n\nvocab_size = len(vocab)\nprint(\"Vocab size =\", vocab_size)\n\nVocab size = 95811\n\n\n\nvocab(['this', 'is', 'a', 'sports', 'article','&lt;unk&gt;'])\n\n[52, 21, 5, 262, 4229, 0]\n\n\nLooking at some sample data\n\nfor label, text in random.sample(train_dataset, 3):\n    print(label,classes[label-1])\n    print(text)\n    print(\"******\")\n\n1 World\nEU-25 among least corrupt in global index Corruption is rampant in sixty countries of the world and the public sector continues to be plagued by bribery, says a report by a respected global corruption watchdog.\n******\n4 Sci/Tech\nIDC Raises '04 PC Growth View, Trims '05 (Reuters) Reuters - Shipments of personal computers\\this year will be higher than previously anticipated, boosted\\by the strongest demand from businesses in five years, research\\firm IDC said on Monday.\n******\n2 Sports\nThe not-so-great cover-up A crisis, they say, is the best way to test the efficiency of a system. At the Wankhede Stadium, there was a crisis on the first morning when unseasonal showers showed up on the first morning of the final Test.\n******\n\n\n\n\n4.4. Creating EmbeddingsBag related pipelines\n\nThe text pipeline purpose is to convert text into tokens\nthe label pipeline is to have labels from 0 to 3\n\n\n_text_pipeline = lambda x: vocab(tokenizer(x))\n_label_pipeline = lambda x: int(x) - 1\n\n\n_text_pipeline(\"this is a sports article\")\n\n[52, 21, 5, 262, 4229]\n\n\n\n_label_pipeline('3')\n\n2\n\n\n\n\n4.4.1 Exploring arguments for nn.EmbeddingBag\nnn.EmbeddingBag()(input_tensor, offsets)\n\nfrom torch import nn \n\ninput_tensor = torch.tensor([0, 1, 2, 3, 4, 3, 2, 1], dtype=torch.int64)\noffsets = torch.tensor([0, 5], dtype=torch.long)\nembedding_layer = nn.EmbeddingBag(num_embeddings=10,embedding_dim=3,sparse=True)\nembedding_layer(input_tensor,offsets)\n\ntensor([[ 0.0383,  0.0984, -0.4766],\n        [-0.5284,  0.3360, -0.5838]], grad_fn=&lt;EmbeddingBagBackward&gt;)\n\n\n\n4.4.2 Create Collate Function\n\n\n# create collate batch function \n# to club labels, tokenized_text_converted_into_numbers and token_offsets\ndef collate_batch(batch):\n    label_list, text_list, offsets = [], [], [0]\n    for (_label, _text) in batch:\n        label_list.append(_label_pipeline(_label))\n        processed_text = torch.tensor(_text_pipeline(_text),\n                                      dtype=torch.int64\n                                     )\n        text_list.append(processed_text)\n        offsets.append(processed_text.size(0))\n    label_list_overall = torch.tensor(label_list, dtype=torch.int64)\n    text_list_overall = torch.cat(text_list)\n    offsets_overall = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    return label_list_overall.to(device), text_list_overall.to(device), offsets_overall.to(device) \n\n\n\n\n4.5. Prepare DataLoaders\n\nBATCH_SIZE = 4\nfrom torch.utils.data.dataset import random_split\n\nnum_train = int(len(train_dataset) * 0.95)\n\n\nnum_train\n\n114000\n\n\n\nsplit_train_, split_valid_ = random_split(train_dataset, \n                                          [num_train, len(train_dataset) - num_train]\n                                         )\n\n\ntype(split_train_)\n\ntorch.utils.data.dataset.Subset\n\n\n\nsplit_train_.indices[0:5]\n\n[1565, 113376, 44093, 96738, 56856]\n\n\n\ntrain_dataloader = DataLoader(split_train_,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              collate_fn=collate_batch\n                             ) \n\nvalid_dataloader = DataLoader(split_valid_,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              collate_fn=collate_batch\n                             )\n\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=BATCH_SIZE,\n                             shuffle=True,\n                             collate_fn=collate_batch\n                            )\n\n\n\n4.6. Model Architecture\n\nfrom torch import nn\n\nclass LinearTextClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class=4):\n        super(LinearTextClassifier,self).__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size,\n                                         embed_dim,\n                                         sparse=True\n                                        )\n        # fully connected layer\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n        \n    def init_weights(self):\n        initrange = 0.5\n        # initializing embedding weights as a uniform distribution\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        \n        # initializing linear layer weights as a uniform distribution\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        \n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)\n\nInitializing model and embedding dimension\n\nnum_classes = len(set([label for (label, text) in train_dataset]))\n\n\nnum_classes\n\n4\n\n\n\nvocab_size = len(vocab)\nembedding_dim = 64\n\n# instantiating the class and pass on to device\nmodel = LinearTextClassifier(vocab_size,\n                             embedding_dim,\n                             num_classes\n                            ).to(device)\n\n\nprint(model)\n\nLinearTextClassifier(\n  (embedding): EmbeddingBag(95811, 64, mode=mean)\n  (fc): Linear(in_features=64, out_features=4, bias=True)\n)\n\n\n\n\n4.7. Define train_loop and test_loop functions\n\n\nCode\n# setting hyperparameters\nlr = 3\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nepoch_size = 10 # setting a low number to see time consumption\n\n\n\ndef train_loop(model, \n               train_dataloader,\n               validation_dataloader,\n               epoch,\n               lr=lr,\n               optimizer=optimizer,\n               loss_fn=loss_fn,\n              ):\n    train_size = len(train_dataloader.dataset)\n    validation_size = len(validation_dataloader.dataset)\n    training_loss_per_epoch = 0\n    validation_loss_per_epoch = 0\n    for batch_number, (labels, features, offsets) in enumerate(train_dataloader):\n        if batch_number %100 == 0:\n            print(f\"In epoch {epoch}, training of {batch_number} batches are over\")\n        # following two lines are used while for testing if the fns are accurate\n        # if batch_number %10 == 0:\n        #    break\n        labels, features, offsets = labels.to(device), features.to(device), offsets.to(device)\n        # labels = labels.clone().detach().requires_grad_(True).long().to(device)        # compute prediction and prediction error\n        pred = model(features, offsets)\n        \n        # print(pred.dtype, pred.shape)\n        loss = loss_fn(pred, labels)\n        # print(loss.dtype)\n        \n        # backpropagation steps\n        # key optimizer steps\n        # by default, gradients add up in PyTorch\n        # we zero out in every iteration\n        optimizer.zero_grad()\n        \n        # performs the gradient computation steps (across the DAG)\n        loss.backward()\n        \n        # adjust the weights\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n        optimizer.step()\n        training_loss_per_epoch += loss.item()\n        \n    for batch_number, (labels, features, offsets) in enumerate(validation_dataloader):\n        labels, features, offsets = labels.to(device), features.to(device), offsets.to(device)\n        # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n\n        # compute prediction error\n        pred = model(features, offsets)\n        loss = loss_fn(pred, labels)\n        \n        validation_loss_per_epoch += loss.item()\n    \n    avg_training_loss = training_loss_per_epoch / train_size\n    avg_validation_loss = validation_loss_per_epoch / validation_size\n    print(f\"Average Training Loss of {epoch}: {avg_training_loss}\")\n    print(f\"Average Validation Loss of {epoch}: {avg_validation_loss}\")\n\n\ndef test_loop(model,test_dataloader, epoch, loss_fn=loss_fn):\n    test_size = len(test_dataloader.dataset)\n    # Failing to do eval can yield inconsistent inference results\n    model.eval()\n    test_loss_per_epoch, accuracy_per_epoch = 0, 0\n    # disabling gradient tracking while inference\n    with torch.no_grad():\n        for labels, features, offsets in test_dataloader:\n            labels, features, offsets = labels.to(device), features.to(device), offsets.to(device)\n            # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n\n            # labels = torch.tensor(labels, dtype=torch.float32)\n            pred = model(features, offsets)\n            loss = loss_fn(pred, labels)\n            test_loss_per_epoch += loss.item()\n            accuracy_per_epoch += (pred.argmax(1)==labels).type(torch.float).sum().item()\n    # following two lines are used only while testing if the fns are accurate\n    # print(f\"Last Prediction \\n 1. {pred}, \\n 2.{pred.argmax()}, \\n 3.{pred.argmax(1)}, \\n 4.{pred.argmax(1)==labels}\")\n    # print(f\"Last predicted label: \\n {labels}\")\n    print(f\"Average Test Loss of {epoch}: {test_loss_per_epoch/test_size}\")\n    print(f\"Average Accuracy of {epoch}: {accuracy_per_epoch/test_size}\")\n\n\n\n4.8 Training the Model\n\n# checking for 1 epoch, testing for 1 epoch\nepoch = 1\ntrain_loop(model,\n           train_dataloader, \n           valid_dataloader,\n           epoch\n          )\n\ntest_loop(model, \n          test_dataloader,\n          epoch)\n\n\nepoch_size\n\n10\n\n\n\n%%time\n# it takes time to run this model\nfor epoch in range(epoch_size):\n    print(f\"Epoch Number: {epoch} \\n---------------------\")\n    train_loop(model, \n               train_dataloader, \n               valid_dataloader,\n               epoch\n              )\n    test_loop(model, \n              test_dataloader,\n              epoch)\n\nEpoch Number: 0 \n---------------------\nIn epoch 0, training of 0 batches are over\nIn epoch 0, training of 100 batches are over\nIn epoch 0, training of 200 batches are over\nIn epoch 0, training of 300 batches are over\nIn epoch 0, training of 400 batches are over\nIn epoch 0, training of 500 batches are over\nIn epoch 0, training of 600 batches are over\nIn epoch 0, training of 700 batches are over\nIn epoch 0, training of 800 batches are over\nIn epoch 0, training of 900 batches are over\nIn epoch 0, training of 1000 batches are over\n.....\nIn epoch 4, training of 28000 batches are over\nIn epoch 4, training of 28100 batches are over\nIn epoch 4, training of 28200 batches are over\nIn epoch 4, training of 28300 batches are over\nIn epoch 4, training of 28400 batches are over\nAverage Training Loss of 4: 0.06850743169194017\nAverage Validation Loss of 4: 0.10275975785597817\nAverage Test Loss of 4: 0.10988466973246012\nAverage Accuracy of 4: 0.9142105263157895\nEpoch Number: 5 \n---------------------\nIn epoch 5, training of 0 batches are over\n.....\nIn epoch 9, training of 28200 batches are over\nIn epoch 9, training of 28300 batches are over\nIn epoch 9, training of 28400 batches are over\nAverage Training Loss of 9: 0.05302847929670939\nAverage Validation Loss of 9: 0.12290485648680452\nAverage Test Loss of 9: 0.1306164133289306\nAverage Accuracy of 9: 0.9111842105263158\nCPU times: user 5min 56s, sys: 25.8 s, total: 6min 22s\nWall time: 6min 13s\n\n\n\n\n4.9.Test the model on sample text\n\nag_news_label = {1: \"World\",\n                 2: \"Sports\",\n                 3: \"Business\",\n                 4: \"Sci/Tec\"}\n\ndef predict(text, model):\n    with torch.no_grad():\n        tokenized_numericalized_vector = torch.tensor(_text_pipeline(text))\n        offsets = torch.tensor([0])\n        output = model(tokenized_numericalized_vector, \n                       offsets)\n        output_label = ag_news_label[output.argmax(1).item() + 1]\n        return output_label\n    \nsample_string = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n    enduring the season’s worst weather conditions on Sunday at The \\\n    Open on his way to a closing 75 at Royal Portrush, which \\\n    considering the wind and the rain was a respectable showing. \\\n    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n    was another story. With temperatures in the mid-80s and hardly any \\\n    wind, the Spaniard was 13 strokes better in a flawless round. \\\n    Thanks to his best putting performance on the PGA Tour, Rahm \\\n    finished with an 8-under 62 for a three-stroke lead, which \\\n    was even more impressive considering he’d never played the \\\n    front nine at TPC Southwind.\"\n\ncpu_model = model.to(\"cpu\")\n\nprint(f\"This is a {predict(sample_string, model=cpu_model)} news\")\n\nThis is a Sports news"
  },
  {
    "objectID": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#a-text-classification-pipeline-using-nn.embedding-nn.linear-layer",
    "href": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#a-text-classification-pipeline-using-nn.embedding-nn.linear-layer",
    "title": "PyTorch Fundamentals for NLP - Part 2",
    "section": "5. A Text Classification Pipeline using nn.Embedding + nn.linear Layer",
    "text": "5. A Text Classification Pipeline using nn.Embedding + nn.linear Layer\n\nDataset considered: AG_NEWS dataset that consists of 4 classes - World, Sports, Business and Sci/Tech\n\n(same architecture as previous one, except the change in step 4)\n┣━━ 1.Loading dataset  ┃ ┣━━ torch.data.utils.datasets.AG_NEWS  ┣━━ 2.Load Tokenization  ┃ ┣━━ torchtext.data.utils.get_tokenizer('basic_english')  ┣━━ 3.Build vocabulary  ┃ ┣━━ torchtext.vocab.build_vocab_from_iterator(train_iterator)  ┣━━ 4.Create Embedding layer ┃ ┣━━ Create collate_fn (padify) to create pairs of label-feature tensors for every minibatch  ┣━━ 5.Create train, validation and test DataLoaders ┣━━ 6.Define Model_Architecture ┣━━ 7.define training_loop and testing_loop functions ┣━━ 8.Train the model and Evaluate on Test Data ┣━━ 9.Test the model on sample text\nImporting basic modules\n\n\nCode\nimport torch\nimport torchtext\nimport os\nimport collections\nimport random\nimport numpy as np\n\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\n\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n5.1. Loading dataset\n\n\nCode\ndef load_dataset(ngrams=1):\n    print(\"Loading dataset ...\")\n    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n    train_dataset = list(train_dataset)\n    test_dataset = list(test_dataset)\n    return train_dataset, test_dataset\n\n\n\ntrain_dataset, test_dataset = load_dataset()\n\nLoading dataset ...\n\n\n\nclasses = ['World', 'Sports', 'Business', 'Sci/Tech']\n\n5.2. Loading Tokenizer\n\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n\n5.3. Building Vocabulary\n\n\nCode\ndef _yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n\ndef create_vocab(train_dataset):\n    print(\"Building vocabulary ..\")\n    vocab = build_vocab_from_iterator(_yield_tokens(train_dataset),\n                                      min_freq=1,\n                                      specials=['&lt;unk&gt;']\n                                     )\n    vocab.set_default_index(vocab['&lt;unk&gt;'])\n    return vocab\n\n\n\nvocab = create_vocab(train_dataset)\n\nBuilding vocabulary ..\n\n\n\nvocab_size = len(vocab)\nprint(\"Vocab size =\", vocab_size)\n\nVocab size = 95811\n\n\n\nvocab(['this', 'is', 'a', 'sports', 'article','&lt;unk&gt;'])\n\n[52, 21, 5, 262, 4229, 0]\n\n\n5.4. Creating nn.Embedding related pipelines\n\nThe text pipeline purpose is to convert text into tokens\nthe label pipeline is to have labels from 0 to 3\n\n\n_text_pipeline = lambda x: vocab(tokenizer(x))\n_label_pipeline = lambda x: int(x) - 1\n\n\n_text_pipeline(\"this is a sports article\")\n\n[52, 21, 5, 262, 4229]\n\n\n\n_label_pipeline('3')\n\n2\n\n\n5.4.1 Exploring arguments for nn.Embedding\n\nnn.Embedding: A simple lookup table that stores embeddings of a fixed dictionary and size.\nLet us create an Embedding module containing 10 tensors of size 3\n\n\nfrom torch import nn\nembedding = nn.Embedding(5, 3)\nfor i in range(5):\n    print(embedding(torch.tensor([i])))\n\ntensor([[ 1.2225,  0.7789, -1.1441]], grad_fn=&lt;EmbeddingBackward&gt;)\ntensor([[1.3428, 1.2356, 0.6745]], grad_fn=&lt;EmbeddingBackward&gt;)\ntensor([[-0.6605, -1.5354, -0.4195]], grad_fn=&lt;EmbeddingBackward&gt;)\ntensor([[-0.9991,  1.7851, -1.6268]], grad_fn=&lt;EmbeddingBackward&gt;)\ntensor([[0.7723, 2.0980, 0.3080]], grad_fn=&lt;EmbeddingBackward&gt;)\n\n\n\nan_array_input = torch.tensor([[1,2,4,3]])\nembedding(an_array_input)\n\ntensor([[[ 1.3428,  1.2356,  0.6745],\n         [-0.6605, -1.5354, -0.4195],\n         [ 0.7723,  2.0980,  0.3080],\n         [-0.9991,  1.7851, -1.6268]]], grad_fn=&lt;EmbeddingBackward&gt;)\n\n\n5.4.2. Create Collate Function\nDealing with Variable Sequence Size\n\nEvery data point in a text corpus could have different number of tokens\nFor maintaining uniform number of input tokens in texts, we padify the text\ntorch.nn.functional.pad on a tokenized dataset can padify the dataset\n\n Source: Microsoft PyTorch Docs\n\ndef padify(batch):\n    # batch is a list of (label, text) pair of tuples\n    label_list, text_list = [], []\n    for (_label, _text) in batch:\n        label_list.append(_label_pipeline(_label))\n        tokenized_numericalized_text = torch.tensor(_text_pipeline(_text),\n                                                    dtype=torch.int64 \n                                                   )\n        text_list.append(tokenized_numericalized_text)\n    # compute max length of a sequence in this minibatch\n    max_length = max(map(len,text_list))\n    label_list_overall = torch.tensor(label_list, \n                                      dtype=torch.int64\n                                     )\n    text_list_overall = torch.stack([torch.nn.functional.pad(torch.tensor(t),\n                                                             (0,max_length - len(t)),\n                                                             mode='constant',\n                                                             value=0) for t in text_list\n                                    ])\n    return label_list_overall.to(device), text_list_overall.to(device)\n\n5.5. Prepare DataLoaders\n\nBATCH_SIZE = 4\nfrom torch.utils.data.dataset import random_split\n\nnum_train = int(len(train_dataset) * 0.95)\n\n\nsplit_train_, split_valid_ = random_split(train_dataset, \n                                          [num_train, len(train_dataset) - num_train]\n                                         )\nsplit_train_.indices[0:5]\n\n[13748, 32598, 23674, 26304, 9007]\n\n\n\ntrain_dataloader = DataLoader(split_train_,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              collate_fn=padify\n                             ) \n\nvalid_dataloader = DataLoader(split_valid_,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              collate_fn=padify\n                             )\n\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=BATCH_SIZE,\n                             shuffle=True,\n                             collate_fn=padify\n                            )\n\n\nfor i, (labels, features) in enumerate(test_dataloader):\n    print(f\"Tracking batch {i}\")\n    print(labels.shape)\n    print(features.shape)\n    if i == 3:\n        break\n    print(\"****\")\n\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\nTracking batch 0\ntorch.Size([4])\ntorch.Size([4, 52])\n****\nTracking batch 1\ntorch.Size([4])\ntorch.Size([4, 62])\n****\nTracking batch 2\ntorch.Size([4])\ntorch.Size([4, 50])\n****\nTracking batch 3\ntorch.Size([4])\ntorch.Size([4, 47])\n\n\n5.6 Model Architecture\n\nfrom torch import nn\n\nclass LinearTextClassifier_2(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class=4):\n        super(LinearTextClassifier_2,self).__init__()\n        self.embedding = nn.Embedding(vocab_size,\n                                         embed_dim,\n                                     )\n        # fully connected layer\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n        \n    def init_weights(self):\n        initrange = 0.5\n        # initializing embedding weights as a uniform distribution\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        \n        # initializing linear layer weights as a uniform distribution\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        \n        self.fc.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.mean(x, dim=1)\n        return self.fc(x)\n\nInitializing the model hyperaparameters\n\nnum_classes = len(set([label for (label, text) in train_dataset]))\n\n\nvocab_size = len(vocab)\nembedding_dim = 64\n\n# instantiating the class and pass on to device\nmodel_2 = LinearTextClassifier_2(vocab_size,\n                               embedding_dim,\n                               num_classes\n                              ).to(device)\n\n\nprint(model_2)\n\nLinearTextClassifier_2(\n  (embedding): Embedding(95811, 64)\n  (fc): Linear(in_features=64, out_features=4, bias=True)\n)\n\n\n\n\nCode\n# setting hyperparameters\nlr = 0.001\noptimizer = torch.optim.Adam(model_2.parameters(), lr=lr)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nepoch_size = 3 # setting a low number to see time consumption\n\n\n5.7. Define train_loop and test_loop functions\n\ndef train_loop_2(model_2, \n               train_dataloader,\n               validation_dataloader,\n               epoch,\n               lr=lr,\n               optimizer=optimizer,\n               loss_fn=loss_fn,\n              ):\n    train_size = len(train_dataloader.dataset)\n    validation_size = len(validation_dataloader.dataset)\n    training_loss_per_epoch = 0\n    validation_loss_per_epoch = 0\n    for batch_number, (labels, features) in enumerate(train_dataloader):\n        if batch_number %1000 == 0:\n            print(f\"In epoch {epoch}, training of {batch_number} batches are over\")\n        # following two lines are used while for testing if the fns are accurate\n        # if batch_number %10 == 0:\n        #    break\n        labels, features = labels.to(device), features.to(device)\n        # labels = labels.clone().detach().requires_grad_(True).long().to(device)        # compute prediction and prediction error\n        pred = model_2(features)\n        \n        # print(pred.dtype, pred.shape)\n        loss = loss_fn(pred, labels)\n        # print(loss.dtype)\n        \n        # backpropagation steps\n        # key optimizer steps\n        # by default, gradients add up in PyTorch\n        # we zero out in every iteration\n        optimizer.zero_grad()\n        \n        # performs the gradient computation steps (across the DAG)\n        loss.backward()\n        \n        # adjust the weights\n        # torch.nn.utils.clip_grad_norm_(model_2.parameters(), 0.1)\n        optimizer.step()\n        training_loss_per_epoch += loss.item()\n        \n    for batch_number, (labels, features) in enumerate(validation_dataloader):\n        labels, features = labels.to(device), features.to(device)\n        # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n\n        # compute prediction error\n        pred = model_2(features)\n        loss = loss_fn(pred, labels)\n        \n        validation_loss_per_epoch += loss.item()\n    \n    avg_training_loss = training_loss_per_epoch / train_size\n    avg_validation_loss = validation_loss_per_epoch / validation_size\n    print(f\"Average Training Loss of {epoch}: {avg_training_loss}\")\n    print(f\"Average Validation Loss of {epoch}: {avg_validation_loss}\")\n\n\ndef test_loop_2(model_2,test_dataloader, epoch, loss_fn=loss_fn):\n    test_size = len(test_dataloader.dataset)\n    # Failing to do eval can yield inconsistent inference results\n    model_2.eval()\n    test_loss_per_epoch, accuracy_per_epoch = 0, 0\n    # disabling gradient tracking while inference\n    with torch.no_grad():\n        for labels, features in test_dataloader:\n            labels, features = labels.to(device), features.to(device)\n            # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n\n            # labels = torch.tensor(labels, dtype=torch.float32)\n            pred = model_2(features)\n            loss = loss_fn(pred, labels)\n            test_loss_per_epoch += loss.item()\n            accuracy_per_epoch += (pred.argmax(1)==labels).type(torch.float).sum().item()\n    # following two lines are used only while testing if the fns are accurate\n    # print(f\"Last Prediction \\n 1. {pred}, \\n 2.{pred.argmax()}, \\n 3.{pred.argmax(1)}, \\n 4.{pred.argmax(1)==labels}\")\n    # print(f\"Last predicted label: \\n {labels}\")\n    print(f\"Average Test Loss of {epoch}: {test_loss_per_epoch/test_size}\")\n    print(f\"Average Accuracy of {epoch}: {accuracy_per_epoch/test_size}\")\n\n5.8 Training the model\n\n# checking for 1 epoch, testing for 1 epoch\nepoch = 1\ntrain_loop_2(model_2,\n           train_dataloader, \n           valid_dataloader,\n           epoch\n          )\n\ntest_loop_2(model_2, \n          test_dataloader,\n          epoch)\n\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\nIn epoch 1, training of 0 batches are over\nIn epoch 1, training of 1000 batches are over\nIn epoch 1, training of 2000 batches are over\nIn epoch 1, training of 3000 batches are over\n.....\nIn epoch 1, training of 26000 batches are over\nIn epoch 1, training of 27000 batches are over\nIn epoch 1, training of 28000 batches are over\nAverage Training Loss of 1: 0.08683817907764081\nAverage Validation Loss of 1: 0.0605684169218495\nAverage Test Loss of 1: 0.06346218769094585\nAverage Accuracy of 1: 0.9201315789473684\n\n\n\n\nCode\n# it takes time to run this model\nfor epoch in range(epoch_size):\n    print(f\"Epoch Number: {epoch} \\n---------------------\")\n    train_loop_2(model_2, \n               train_dataloader, \n               valid_dataloader,\n               epoch\n              )\n    test_loop_2(model_2, \n              test_dataloader,\n              epoch)\n\n\n5.9. Test the model on sample text\n\nag_news_label = {1: \"World\",\n                 2: \"Sports\",\n                 3: \"Business\",\n                 4: \"Sci/Tec\"}\n\n\ndef predict_2(text, model):\n    batch = [(torch.tensor([0]),\n              text\n             )\n            ]\n    with torch.no_grad():\n        _, padded_sequence = padify(batch)\n        padded_sequence = padded_sequence.to(\"cpu\")\n        # tokenized_numericalized_vector = torch.tensor(_text_pipeline(text))\n        output = model_2(padded_sequence)\n        output_label = ag_news_label[output.argmax(1).item() + 1]\n        return output_label\n    \nsample_string = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n    enduring the season’s worst weather conditions on Sunday at The \\\n    Open on his way to a closing 75 at Royal Portrush, which \\\n    considering the wind and the rain was a respectable showing. \\\n    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n    was another story. With temperatures in the mid-80s and hardly any \\\n    wind, the Spaniard was 13 strokes better in a flawless round. \\\n    Thanks to his best putting performance on the PGA Tour, Rahm \\\n    finished with an 8-under 62 for a three-stroke lead, which \\\n    was even more impressive considering he’d never played the \\\n    front nine at TPC Southwind.\"\n\ncpu_model_2 = model_2.to(\"cpu\")\n\nprint(f\"This is a {predict_2(sample_string, model=cpu_model_2)} news\")\n\nThis is a Sports news\n\n\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)."
  },
  {
    "objectID": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#conclusion",
    "href": "posts/2021-09-15-pytorch_for_nlp_part_ii.html#conclusion",
    "title": "PyTorch Fundamentals for NLP - Part 2",
    "section": "6. Conclusion",
    "text": "6. Conclusion\n\nIn this blog piece, we looked at how we can build linear classifiers (without non-linear activation functions) over nn.EmbeddingBag and nn.Embedding modules\nIn the nn.EmbeddingBag method of embedding creation, we did not create padding tokens but have to track offsets for every minibatch.\nIn the nn.Embedding method of creating embeddings, we used torch.nn.functional.pad function to ensure all text sequences have fixed length\n\nSources \n\nMSFT PyTorch NLP Course | link\nOfficial PyTorch Tutorial on Text Classification using nn.EmbeddingBag | link\nMSFT PyTorch Text Classification using nn.Embedding | link"
  },
  {
    "objectID": "posts/2022-02-03-Primer-on-Statistics.html",
    "href": "posts/2022-02-03-Primer-on-Statistics.html",
    "title": "Part 2 - A Primer on Statistics",
    "section": "",
    "text": "1. What is the starting point to unravel a data story?\n\nLook for the middle (mean, median and mode)\n\n\n\n2. How spread out is the data?\nAlong with “Middle” point, look for variability - Range: (Max - Min) \nTelling stories with mean and median is still limited. With `Range` it becomes better\n\n|        | Value |\n|--------|-------|\n| Mean   | 60    |\n| Median | 58    |\n| Range  | 70    |\n\nStandard Deviation:\n\nApprox Definition: Average of all data point’s distances from the mean\nProper Definition: Square Root of { the mean of {Square of the difference between each data point and the mean of the data}}\nStd Deviation of the population:  σ = √( Σ(X - μ)2/ N );\nX = Value in the data distribution  μ = Mean of the population  N = Number of data points\nStd Deviatin of Sample:  s = √ (  Σ(X - x̄ )2  /  (n-1) )\nWhy the denominator is n - 1 in Sample Std Deviation? \n\nx̄ is the mean of the sample\nBy empirical evidence (observed in many datasets),  Σi=1N (xi - x̄)2 &lt;&lt; Σi=1N (xi - μ)2 \nHence dividing the sample std deviation by (n-1) makes it “unbiased” and more towards population std deviation \nAnother Explanation: There are only (n-1) degrees of freedom in the calculation of (xi - x̄)\n\n\nZ-Score:\n\nA particular datapoint’s distance from the mean measured in standard deviations Z-score = ( X - μ /  σ )\n= (231 - 139) / 41 = 2.24 = 231 is 2.24 std deviations from the mean = 112 is -0.66 std deviations from the mean\n\nInteresting points:\n\nStd deviations of two different datasets cannot be compared (e.g.: Salaries of Data Scientists and Consumption of Fuel in cars)\n\n\n\n\n3. Empirical Rule (or the 68-95-99.7 rule)\n\nMost of the datapoints (68%, 95%, 99.7% ) fall within some std deviations (1,2, and 3 respectively) from the mean\nIn other words, 99.7% of the data that is normally distributed will lie 3 standard deviations from the mean.\nWhat is normal distribution?  The dataset distribution mimics a bell curve\nApplication of the Empirical Rule:\n\nUnderstanding if a particular data point being an outlier or not\n\n\n\n\n4. Central Limit Theorem\n\nGiven a population of unknown distribution with mean μ and finite variance σ2,\n\nIf we keep sampling n values from the the distribution, and compute sample mean as  X̄n ~= (  X1 + X1 + Xn  / n)\nAs n-&gt; ∞, the distribution of the sample means tend to be normal or gaussian (following the bell curve)\n\nIn simple words, \n\nIf you have a population with unknown distribution but with a mean of μ and std deviation of σ and take sufficiently large number of samples n (with replacement), the distribution of means will be approximately normally distributed  \n\nWith the help of CLT, we need not wait for the entire population’s data (and the subsequent identification of the population’s unknown distribution), we can apply normal distribution principles (like the empirical rule and many more statistical techniques) on the sample means and draw a conclusion about the population\n\nMore about CLT with an example:  - Central Limit Theorem’s super power - “You don’t need to know the population distribution”\n\n\n\n5. Outlier:\n\nOutlier is a relative term. There is no absolute definition (like if a datapoint is 2 or 3 σ away from the mean)\nHow to investigate outliers:  (one should not simply ignore/remove it)\n\nIs this really an outlier?\nHow did this happen?\nWhat can we learn?\nWhat needs to change (to make it fit into the distribution)?\n\n\nSource: - LinkedIn Courses - Statistics Foundations: Probability and Statistics Foundations: The Basics | refer"
  },
  {
    "objectID": "posts/2020-09-17-ptw_lda_vs_use_k_means.html",
    "href": "posts/2020-09-17-ptw_lda_vs_use_k_means.html",
    "title": "Comparing Two Unsupervised Clustering Algorithms for Text Data",
    "section": "",
    "text": "Prior-topic words guided Latent Dirichlet Allocation and\nUniversal Sentence Encoder (USE) powered K-Means\n\n\n\n\n\nGiven that USE can encode complex semantic information, is LDA worthwhile?\nCan LDA be used for at least some type of data, where it can produce better/on-par results compared to USE-K-Means?\n\nBefore diving into the study, let us understand how USE-KMeans and PTW-guided LDA works"
  },
  {
    "objectID": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#i.-introduction",
    "href": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#i.-introduction",
    "title": "Comparing Two Unsupervised Clustering Algorithms for Text Data",
    "section": "",
    "text": "Prior-topic words guided Latent Dirichlet Allocation and\nUniversal Sentence Encoder (USE) powered K-Means\n\n\n\n\n\nGiven that USE can encode complex semantic information, is LDA worthwhile?\nCan LDA be used for at least some type of data, where it can produce better/on-par results compared to USE-K-Means?\n\nBefore diving into the study, let us understand how USE-KMeans and PTW-guided LDA works"
  },
  {
    "objectID": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#ii.-about-use",
    "href": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#ii.-about-use",
    "title": "Comparing Two Unsupervised Clustering Algorithms for Text Data",
    "section": "II. About USE",
    "text": "II. About USE\n\nIIA. How USE works?\nUSE converts sentences into 512 embeddings \n\nSemantic Similarity Correlation Matrix \n\nDespite many common words, semantically different sentences will have dissimilar embeddings \n\n\nIIB. USE Architecture\nThere are two variants\nVariant 1: Transformer Encoder:  \nsource: https://amitness.com/2020/06/universal-sentence-encoder/ \nWhat does a Transformer Encoder Layer comprise? - Self Attention Layer - Feed Forward Network Layer \nVariant 2: Deep Averaging Network: \n\n\nIIC. Pre-trained Tasks in USE\nOverall Pipeline of how USE is trained:  \n\nA. Skip-thought prediction:  - Use Central Sentence to predict both the Previous Sentence and Next Sentence\n\nB. Response Prediction:  - Train the USE architecture to do smart reply prediction\n\nC. Natural Language Inference:  - Do NLI task prediction, where given a premise and a hypothesis, the model is trained to predict whether the hypothesis is an entailment, contradition or neutral to the premise"
  },
  {
    "objectID": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#iii.-about-lda",
    "href": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#iii.-about-lda",
    "title": "Comparing Two Unsupervised Clustering Algorithms for Text Data",
    "section": "III. About LDA",
    "text": "III. About LDA\n\nLatent: \n\nTopic structures in a document are latent/hidden in the text\n\nDirichlet: \n\nThe Dirichlet distribution determines the mixture proportions of  - the topics in the documents and - the words in each topic.\n\nAllocation: \n\nAllocation of words to a given topic and allocation of topics to a document\n\n\n\nIIIA. Intuitive understanding of Dirichlet Distribution\n\nA Normal/Gaussian distribution is a continuous probability distribution over all the real numbers\n\nIt is described by a mean and a variance.\n\n\n\n\nA Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval\n\n\n\nThe Poisson distribution is specified by one parameter: lambda (λ). As lambda increases to sufficiently large values, the normal distribution (λ, λ) may be used to approximate the Poisson distribution Source:https://en.wikipedia.org/wiki/Poisson_distribution\n\n\n\nNow, what is Dirichlet Distribution? \n\nThe dirichlet distribution is a probability distribution as well\nbut it is not sampling from the space of real numbers. Instead it is sampling over a probability simplex \n\n\n (0.6, 0.4)\n (0.1, 0.1, 0.8)\n (0.05, 0.2, 0.15, 0.1, 0.3, 0.2)\n\n\nHow Dirichlet Distribution varies w.r.t dirichlet prior: \n\nThe below image shows Dir(α) \nAs α increases from 0.05 (1/20) to 0.1, 0.2, 0.4 respectively in plots from left to right & top to down, you can see the distribution becoming more uniform.  \n\n\n\n\nIIIB. How LDA Works?\n\nLDA is a generative model \nLDA processes documents as ‘bag of words’ – ordering of words is not important\n\n\nIn principle, LDA generates a document based on dirichlet distribution (dd) of topics over documents and dd of words over topics\n\n \nBut we inverse the generative process for statistical inference\n\n\n\nIIIC. Hyperparameters of LDA\n\nD = Total No. of Documents  N = No. of Words = Vocab Size  T = No. of Topics   θd = Topic Distribution for a particular document d\n\n Φt= Word Distribution for a topic t. Here for topic 1 and 2.\n\n(colored books represent words/tokens)\nZd,n = Topic Distribution for n th word in document d\nWd,n = nth word in dth document\nα= parameter that sets the dircihlet prior on the per-document topic distribution (θ)\n= parameter that represents the doc-topic density\n= determines the no. of topics in each doc\n= (Default) = 1/num_of_topics (in sklearn and gensim)\ndecreasing alpha results in less number of topics per document\n \nβ= parameter that sets the dirichlet prior on the per-topic word distribution(ϕ)\n= parameter that represents the topic-word density\n= determines the no. of words per each topic\n= (Default) = 1/num_of_topics (in sklearn and gensim)\ndecreasing beta results in just a few major words in topics\n\nm = base measure for per-document topic distribution; a simplex vector/array (m1, m2, …, mT) ; sums to one\nα = Concentration parameter α (positive scalar) \n \n α * m = (α1, α2, …., αT)\n \n \nn = base measure for per-topic word distribution; a simplex vector/array (n1, n2, …, nN); sums to one\nβ = Concentration parameter β (positive scalar) \n \n β * n = (β1, β2, …., βT)\n \n \nThere is also another hyper parameter η - topic coherence or perplexity - which can be used to determine the number of topics \n\n\nIIID. Now, How does PTW-LDA work?\n\nNudge the regular LDA to converge faster and better with human-reviewed words list for each topic\n\n\n\nHow the topics are seeded with some seed words  Source:Freecodecamp.org \nChaotic LDA and Clear PTW_LDA outputs… \nLDA might need several hyperparameter tuning attempts to get to the desired splits\nDefault initialization with uniform word topic distribution \n\n\n\nSeeded Initialization \nThe seeded words are guided towards seeded topics for converging faster \n\n\n\nIIID. Pre-processing and Hyper-parameters\nPreprocessing: \n\n\n\n\n\n\n\nPTW-LDA\nUSE + Clustering\n\n\n\n\n1. Stop words removed\nNo pre-processing; Comments were used as is\n\n\n2. Lemmatized\n\n\n\n3. Top 20 words per (ground truth) label was extracted\n\n\n\n4. Human-reviewed list of 20 word lists (each corresponding to 1 topic) were chosen as prior topic words input (SAT is useful when there are prior topic words fed; otherwise it works as normal LDA)\n\n\n\n\nHyperparameter Tuning\n\n\n\n\n\n\n\nPTW-LDA (best possible based on heuristics and limited # of experimentations)\nUSE + Clustering\n\n\n\n\nNo. Of Topics\nNo. Of clusters\n\n\nMax Iterations\nMax Iterations (for K-Means)\n\n\n(default) Doc_topic_prior = alpha = 1/ no_of_topics\n\n\n\n(default) topic_word_prior = beta = 1/ no_of_topics\n\n\n\nLearning_method: “batch” (whole dataset is used)\n\n\n\n(alternative is ‘online’ which uses only batch size no. Of comments; similar to mini_batch_kmeans)\n\n\n\nSeeded_words_list\n\n\n\nSeed_coefficent/seed confidence (how much to nudge the seeded words)"
  },
  {
    "objectID": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#iv.-comparison-study",
    "href": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#iv.-comparison-study",
    "title": "Comparing Two Unsupervised Clustering Algorithms for Text Data",
    "section": "IV. Comparison Study",
    "text": "IV. Comparison Study\n\nData I - 20 Newsgroups - Supervised Evaluation \n\n\nSource for Adjusted Mutual Information Score\n\nData II -ABC (Australian Broadcast Corporation) Corpus - Unsupervised Evaluation\n\n\n\n\n\n\n\n\n\nMetric\nPTW-LDA\nUSE-Clustering\n\n\n\n\nWord Embedding based Coherence Score  (more the coherence score, better is the clustering output)\nCoherence Score for 20 topics: 0.091  \nCoherence Score for 20 clusters : 0.159\n\n\nMethodology of computing the above metric\nTake the top 10 words that constitute each of the 20 topics (each topic comprises of a probability simplex of the words; select the top 10 highly probable words in that topic)   For our case, the top 10 words used for coherence computation in the 10 topics are:   [[‘police’, ‘baghdad’, ‘war’, ‘probe’, ‘protest’, ‘anti’, ‘missing’, ‘man’, ‘fight’, ‘coalition’],  [‘report’, ‘pm’, ‘death’, ‘korea’, ‘claim’, ‘war’, ‘north’, ‘nt’, ‘toll’, ‘protesters’],  [‘win’, ‘govt’, ‘set’, ‘community’, ‘end’, ‘wins’, ‘vic’, ‘indigenous’, ‘road’, ‘help’],  [‘world’, ‘cup’, ‘australia’, ‘found’, ‘ban’, ‘plans’, ‘lead’, ‘gets’, ‘expected’, ‘match’],  [‘un’, ‘coast’, ‘title’, ‘takes’, ‘peace’, ‘iraq’, ‘gold’, ‘defence’, ‘residents’, ‘play’],  [‘iraq’, ‘iraqi’, ‘war’, ‘says’, ‘troops’, ‘killed’, ‘dead’, ‘hospital’, ‘clash’, ‘forces’],  [‘council’, ‘boost’, ‘mp’, ‘fire’, ‘group’, ‘qld’, ‘minister’, ‘defends’, ‘land’, ‘welcomes’],  [‘man’, ‘court’, ‘charged’, ‘face’, ‘plan’, ‘open’, ‘murder’, ‘urged’, ‘case’, ‘charges’],  [‘new’, ‘oil’, ‘dies’, ‘security’, ‘crash’, ‘sars’, ‘high’, ‘year’, ‘house’, ‘car’],  [‘water’, ‘rain’, ‘claims’, ‘wa’, ‘nsw’, ‘farmers’, ‘drought’, ‘howard’, ‘centre’, ‘union’]]   For each topic, taking 2 out of the top 10 words at a time, compute cosine similarity using pre-trained W2V embedding  Overall Coherence is sum of similarity scores of all possible pairs of words for each topic, normalized by the no. of topics\nTake the top 10 words that constitute each of the 20 clusters (top 10 words (from stop-words removed set) are computed based on their TF-IDF weighted scores in that cluster)   For each cluster, taking 2 out of the top 10 words at a time, compute similarity using pre-trained W2V embedding   For our case, the top 10 words used for coherence computation in the 10 clusters are:   [[‘win’, ‘cup’, ‘final’, ‘wins’, ‘world’, ‘afl’, ‘coach’, ‘england’, ‘season’, ‘day’],  [‘council’, ‘plan’, ‘market’, ‘funding’, ‘boost’, ‘housing’, ‘water’, ‘funds’, ‘budget’, ‘rise’],  [‘crash’, ‘man’, ‘killed’, ‘death’, ‘dies’, ‘dead’, ‘injured’, ‘woman’, ‘car’, ‘sydney’],  [‘interview’, ‘michael’, ‘business’, ‘abc’, ‘news’, ‘market’, ‘analysis’, ‘david’, ‘extended’, ‘andrew’],  [‘australia’, ‘australian’, ‘aussie’, ‘sydney’, ‘australians’, ‘day’, ‘aussies’, ‘australias’, ‘melbourne’, ‘south’],  [‘abc’, ‘country’, ‘hour’, ‘news’, ‘weather’, ‘grandstand’, ‘friday’, ‘nsw’, ‘drum’, ‘monday’],  [‘govt’, ‘election’, ‘council’, ‘government’, ‘minister’, ‘pm’, ‘parliament’, ‘nsw’, ‘anti’, ‘trump’],  [‘police’, ‘man’, ‘court’, ‘murder’, ‘charged’, ‘accused’, ‘death’, ‘guilty’, ‘charges’, ‘assault’],  [‘farmers’, ‘water’, ‘drought’, ‘industry’, ‘farm’, ‘coal’, ‘green’, ‘cattle’, ‘mining’, ‘nsw’],  [‘health’, ‘hospital’, ‘flu’, ‘mental’, ‘doctors’, ‘treatment’, ‘cancer’, ‘drug’, ‘service’, ‘care’]]    Overall Coherence is sum of similarity scores of all possible pairs of words for each cluster, normalized by the no. of clusters"
  },
  {
    "objectID": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#conclusion",
    "href": "posts/2020-09-17-ptw_lda_vs_use_k_means.html#conclusion",
    "title": "Comparing Two Unsupervised Clustering Algorithms for Text Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe have used two News corpus of varying text length. One dataset has ground truth labels and the other doesn’t have labels\nIn our comparison of PTW-LDA vs USE-Clustering\n\nusing both ‘Supervised’ (Adjusted Mutual Information Score) and ‘Unsupervised’ evaluation metrics, USE-clustering performs far superior to PTW-LDA despite repeated attempts at different set of hyper-parameters for PTW-LDA\n\n\nImportant References: USE:  - https://amitness.com/2020/06/universal-sentence-encoder/  - USE Paper: https://arxiv.org/abs/1803.11175\nLDA:  - The original paper of LDA by David Blei - https://www.slideshare.net/hustwj/nicolas-loeff-lda - http://videolectures.net/mlss09uk_blei_tm/ - http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf - https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution (it compares normal distribution with Dirichlet Distribution) - https://ldabook.com/lda-as-a-generative-model.html - https://ldabook.com/index.html"
  },
  {
    "objectID": "posts/2022-09-15-PostgreSQL.ipynb.html",
    "href": "posts/2022-09-15-PostgreSQL.ipynb.html",
    "title": "Commonly used PostgreSQL Commands",
    "section": "",
    "text": "CREATE TABLE IF NOT EXISTS table_name(\n    primary_key VARCHAR PRIMARY KEY NOT NULL,\n    text_column_name  TEXT,\n    boolean_column_name BOOL,\n    float_column_name FLOAT8,\n    date_column_name DATE,\n\n\n\nALTER TABLE table_name \n    ADD column_name COLUMN_DATA_TYPE;\n\n\n\nALTER TABLE table_name \n    DROP column_name COLUMN_DATA_TYPE;\n\n\n\nUPDATE table_name_1\nSET column_name = t2.column_name\nFROM table_name_2 t2\nWHERE table_name_1.column_name = t2.column_name;\n\n\n\nUPDATE table_name\nSET column_name_1 = 'some_value'\nWHERE column_name_2 = 'someother value'\n\n\n\nALTER TABLE table_name\n  ALTER COLUMN column_name TYPE column_definition;\n\n\n\nALTER TABLE table_name\n  RENAME COLUMN old_name TO new_name;\n\n\n\nALTER TABLE table_name\n  RENAME TO new_table_name;\n\n\n\nUPDATE public.table_name \nSET agent_name = NULL\nWHERE agent_name = 'NaN';\n\n\n\nselect *\nfrom table_name\nwhere false\n\n\n\nDELETE FROM table_name where some_column ~ 'pattern'\n\n\n\n~ CASE SENSITIVE pattern matching\n~* CASE INSENSITIVE pattern matching\n!~ CASE SENSITIVE “unmatched” pattern (or NOT regex)\n!~* CASE INSENSITIVE “unmatched” pattern (or NOT regex)\n\n\n\n\n\nFor matching all values in some_column which starts with capital S\n\nSELECT * FROM table_name where some_column ~ '^S'\n\nFor matching all values in some_column which end with capital or small case S\n\nSELECT * FROM table_name where some_column ~* 'S$'\n\nFor matching all values in some_column where there or 2 or more numbers in the Text\n\nSELECT * FROM table_name where some_column ~ '[0-9][0-9]'\n\nTo look into more about PostgreSQL functions like REGEXP_REPLACE, REGEXP_MATCHES or Regex in SUBSTRING function, refer this url"
  },
  {
    "objectID": "posts/2022-09-15-PostgreSQL.ipynb.html#sql-couplets-to-update-rds-tables",
    "href": "posts/2022-09-15-PostgreSQL.ipynb.html#sql-couplets-to-update-rds-tables",
    "title": "Commonly used PostgreSQL Commands",
    "section": "",
    "text": "CREATE TABLE IF NOT EXISTS table_name(\n    primary_key VARCHAR PRIMARY KEY NOT NULL,\n    text_column_name  TEXT,\n    boolean_column_name BOOL,\n    float_column_name FLOAT8,\n    date_column_name DATE,\n\n\n\nALTER TABLE table_name \n    ADD column_name COLUMN_DATA_TYPE;\n\n\n\nALTER TABLE table_name \n    DROP column_name COLUMN_DATA_TYPE;\n\n\n\nUPDATE table_name_1\nSET column_name = t2.column_name\nFROM table_name_2 t2\nWHERE table_name_1.column_name = t2.column_name;\n\n\n\nUPDATE table_name\nSET column_name_1 = 'some_value'\nWHERE column_name_2 = 'someother value'\n\n\n\nALTER TABLE table_name\n  ALTER COLUMN column_name TYPE column_definition;\n\n\n\nALTER TABLE table_name\n  RENAME COLUMN old_name TO new_name;\n\n\n\nALTER TABLE table_name\n  RENAME TO new_table_name;\n\n\n\nUPDATE public.table_name \nSET agent_name = NULL\nWHERE agent_name = 'NaN';\n\n\n\nselect *\nfrom table_name\nwhere false\n\n\n\nDELETE FROM table_name where some_column ~ 'pattern'\n\n\n\n~ CASE SENSITIVE pattern matching\n~* CASE INSENSITIVE pattern matching\n!~ CASE SENSITIVE “unmatched” pattern (or NOT regex)\n!~* CASE INSENSITIVE “unmatched” pattern (or NOT regex)\n\n\n\n\n\nFor matching all values in some_column which starts with capital S\n\nSELECT * FROM table_name where some_column ~ '^S'\n\nFor matching all values in some_column which end with capital or small case S\n\nSELECT * FROM table_name where some_column ~* 'S$'\n\nFor matching all values in some_column where there or 2 or more numbers in the Text\n\nSELECT * FROM table_name where some_column ~ '[0-9][0-9]'\n\nTo look into more about PostgreSQL functions like REGEXP_REPLACE, REGEXP_MATCHES or Regex in SUBSTRING function, refer this url"
  },
  {
    "objectID": "posts/2022-09-15-PostgreSQL.ipynb.html#python-postgresql-using-psycopg2",
    "href": "posts/2022-09-15-PostgreSQL.ipynb.html#python-postgresql-using-psycopg2",
    "title": "Commonly used PostgreSQL Commands",
    "section": "python + PostgreSQL using Psycopg2",
    "text": "python + PostgreSQL using Psycopg2\n\nUpdate a particular cell recursively from values in a list of lists\nlol = [[value1A,value1B], [value2A, value2B]]\nfor i,every_list in enumerate(lol):\n    print(f\"Update value to a particular cell in the table ...\")\n    sql_query = f\"UPDATE public.table_name SET date_column='{every_list[0]}' WHERE file_key='{every_list[1]}'\"\n    print(\"*********\")\n    cur.execute(sql_query)\nconn.commit()\n\n\nInsert rows of values from a list of lists\nlol = [[value1A,value1B], [value2A, value2B]]\nsql_query = f\"INSERT INTO table_name (column_name1, column_name2) VALUES (%s, %s)\"\nfor every_list in lol:\n    print(f\"Updating the last row in a table ...\")\n    cur.execute(sql_query,every_list)\nconn.commit()\n\n\nCopy a table to CSV and export the csv to AWS S3 bucket using Pandas\nselect_query = f\"SELECT * FROM public.{table_name} LIMIT 100\"\nsql = f\"\"\"\ncopy (\n    {select_query}\n) to stdout\n\"\"\"\noutputquery = \"COPY ({0}) TO STDOUT WITH CSV HEADER\".format(select_query)\nprint(sql)\nfile = io.StringIO()\ncur.copy_expert(outputquery,file)\nconn.commit()\nfile.seek(0)\ndf = pd.read_csv(file, encoding='utf-8')\n# in case your csv has foreign lang or non-ascii characters\ncsv_buffer = io.BytesIO()\ndf.to_csv(csv_buffer, index=False, encoding='utf_8_sig')\ncsv_buffer.seek(0)\ns3_obj = boto3.client('s3')\nresponse = s3_obj.put_object(\n    Bucket=bucket_name,\n    Key=f'{prefix}/{table_name}.csv',\n    Body=csv_buffer\n)"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html",
    "href": "posts/2023-01-12-jmespath-querying.html",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "",
    "text": "A better version of this blog piece is published in Toyota Connected India Medium Link"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#what-is-a-json",
    "href": "posts/2023-01-12-jmespath-querying.html#what-is-a-json",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "What is a JSON?",
    "text": "What is a JSON?\n\nJSON (JavaScript Object Notation) is used everywhere. Typically they are seen/used\n\nas data interchange formats while developing\nas Logs in JSON format\nas Configurations in JSON Format\nwhile transfering data in Cloud Serverless Services"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#what-are-the-different-types-of-json",
    "href": "posts/2023-01-12-jmespath-querying.html#what-are-the-different-types-of-json",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "What are the different types of JSON",
    "text": "What are the different types of JSON\n\nJSON (JavaScript Object Notation) represents structured data in key, value pairs\nTypes of JSON\n\nString: {\"name\":\"Senthil\"}\ndictionary\nlist\nfloat or int\nboolean\nnull\n\n\n\nExamples of Valid Jsons\n\nA typical dictionary type\n\n{\"name\":\"Senthil\"}\n\nA nested dictionary with list data type values\n\n{\n  \"Android Phones\": [\n    [\n      {\n        \"name\": \"Samsung Galaxy\",\n        \"price\": 899\n      }\n    ]\n  ]\n}\n\nA dictionary with null value\n\n{\"name\":null}\nChecking the above is a valid json\necho \"{\\\"name\\\":null}\" &gt; string_json.json \npython -c \"import json; dict_list = json.load(open('string_json.json','r')); print(dict_list)\"\n{'name': None}\n\nA json containing only a list of values is also\n\n[\n  \"iPhone\",\n  \"Samsung Galaxy\",\n  \"Google Pixel\"\n]\n\nChecking the above is a valid json\n\n&gt;&gt; cat list_json.json\n[\n  \"iPhone\",\n  \"Samsung Galaxy\",\n  \"Google Pixel\"\n]\n&gt;&gt; python -c \"import json; dict_list = json.load(open('list_json.json','r')); print(dict_list)\"\n['iPhone', 'Samsung Galaxy', 'Google Pixel']"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#what-is-jmespath",
    "href": "posts/2023-01-12-jmespath-querying.html#what-is-jmespath",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "What is JMESPath?",
    "text": "What is JMESPath?\n\nJMESPath stands for JSON Matching Expression Paths source\nJMESPath is a query expression language for searching in JSON documents"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#simple-retrieval-of-keys",
    "href": "posts/2023-01-12-jmespath-querying.html#simple-retrieval-of-keys",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "1. Simple Retrieval of Keys",
    "text": "1. Simple Retrieval of Keys\necho '{\"field_1\":30, \"field_2\":50}' | jp 'field_2'\n50\necho '{\"field\":{\"sub_field\":30}}' | jp 'field.sub_field'\n30\n\nIn the above examples, we have extracted a specific key by using . operator field.sub_field"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#slicing-array-or-list-type-field",
    "href": "posts/2023-01-12-jmespath-querying.html#slicing-array-or-list-type-field",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "2. Slicing array or list type field",
    "text": "2. Slicing array or list type field\necho '{\"field_1\":30, \"field_2\":50, \"field_3\":[1,2,3]}' | jp 'field_3[*]'\n[\n  1,\n  2,\n  3\n]\necho '{\"field_1\":30, \"field_2\":50, \"field_3\":[1,2,3]}' | jp 'field_3[0]'\n1\necho '{\"field_1\":30, \"field_2\":50, \"field_3\":[1,2,3]}' | jp '[field_3[0], field_3[2]]'\n[\n  1,\n  3\n]\n\nIn the above examples, we have used syntax such as\n\n[*] to extract all elements in an array\n[field[index], field[another_index]] to extract specific indices of an array"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#slicing-an-array-of-dictionaries-products...-to-fetch-one-of-the-keys-name",
    "href": "posts/2023-01-12-jmespath-querying.html#slicing-an-array-of-dictionaries-products...-to-fetch-one-of-the-keys-name",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "3. Slicing an array of dictionaries products[{...}] to fetch one of the keys name",
    "text": "3. Slicing an array of dictionaries products[{...}] to fetch one of the keys name\ncat data.json\n{\n  \"products\": [\n    {\n      \"name\": \"iPhone\",\n      \"price\": 999\n    },\n    {\n      \"name\": \"Samsung Galaxy\",\n      \"price\": 899\n    },\n    {\n      \"name\": \"Google Pixel\",\n      \"price\": 799\n    },\n    {\n      \"name\": \"OnePlus\",\n      \"price\": 699\n    }\n  ]\n}\njp -f data.json 'products[*].name'\n[\n  \"iPhone\",\n  \"Samsung Galaxy\",\n  \"Google Pixel\",\n  \"OnePlus\"\n]\n\nIn the above example, we have used [*] to look into all values in an array and then show only one field name"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#filtering-based-on-condition",
    "href": "posts/2023-01-12-jmespath-querying.html#filtering-based-on-condition",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "4. Filtering based on condition",
    "text": "4. Filtering based on condition\n\nRetrieve all values from a specific key name in an array prodcts where price greater than a specified value\n\njp -f data.json 'products[?price &gt;= `799`].name'\n[\n  \"iPhone\",\n  \"Samsung Galaxy\",\n  \"Google Pixel\"\n]\n\nIn the above example, we have used a condition on a field price to retrieve from an array products and then display only field name"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#retrieve-multiple-values-and-make-a-new-json",
    "href": "posts/2023-01-12-jmespath-querying.html#retrieve-multiple-values-and-make-a-new-json",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "5. Retrieve multiple values and make a new json",
    "text": "5. Retrieve multiple values and make a new json\njp -f data.json '{\"AndroidPhones\":products[?name != `\"iPhone\"`].[{\"android_phone_name\":name, \"price\":price}]}' &gt; android_phones_data.json && cat android_phones_data.json\n{\n  \"AndroidPhones\": [\n    [\n      {\n        \"android_phone_name\": \"Samsung Galaxy\",\n        \"price\": 899\n      }\n    ],\n    [\n      {\n        \"android_phone_name\": \"Google Pixel\",\n        \"price\": 799\n      }\n    ],\n    [\n      {\n        \"android_phone_name\": \"OnePlus\",\n        \"price\": 699\n      }\n    ]\n  ]\n}"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#pipe-expressions",
    "href": "posts/2023-01-12-jmespath-querying.html#pipe-expressions",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "6. Pipe Expressions",
    "text": "6. Pipe Expressions\nThe above result can be made with Pipes (which give a sense of modularized expressions)\njp -f data.json '{\"Android Phones\":products[?name != `\"iPhone\"`]} | \"Android Phones\"[*]'\n\n[\n  {\n    \"name\": \"Samsung Galaxy\",\n    \"price\": 899\n  },\n  {\n    \"name\": \"Google Pixel\",\n    \"price\": 799\n  },\n  {\n    \"name\": \"OnePlus\",\n    \"price\": 699\n  }\n]"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#built-in-functions",
    "href": "posts/2023-01-12-jmespath-querying.html#built-in-functions",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "7. Built-in Functions",
    "text": "7. Built-in Functions\nThere are so many built-in jmespath functions (refer here). Let us cover some of them. The rest of them should follow similar template.\n\nA. sort_by, min_by, max_by\nSort an array in ascending order\njp -f data.json 'products[*] | sort_by(@,&price)' \n[\n  {\n    \"name\": \"OnePlus\",\n    \"price\": 699\n  },\n  {\n    \"name\": \"Google Pixel\",\n    \"price\": 799\n  },\n  {\n    \"name\": \"Samsung Galaxy\",\n    \"price\": 899\n  },\n  {\n    \"name\": \"iPhone\",\n    \"price\": 999\n  }\n]\nNote:  - The &key_name is critical to refer to the variable inside a built-in function\n\nSort an array in descending order\n\njp -f data.json 'products | sort_by(@,&price) | reverse(@)'\n\n[\n  {\n    \"name\": \"iPhone\",\n    \"price\": 999\n  },\n  {\n    \"name\": \"Samsung Galaxy\",\n    \"price\": 899\n  },\n  {\n    \"name\": \"Google Pixel\",\n    \"price\": 799\n  },\n  {\n    \"name\": \"OnePlus\",\n    \"price\": 699\n  }\n]\nNote: - The use of @ sympbolizing the output from previous portion of the pipe to be used to the next stage\n\nMaximum Element in an array\n\njp -f data.json 'products | max_by(@,&price)'\n\n{\n  \"name\": \"iPhone\",\n  \"price\": 999\n}\n\nMinimum Element in an array\n\njp -f data.json 'products | min_by(@,&price) | name'\n\n\"OnePlus\"\njp -u -f data.json 'products | min_by(@,&price) | name'\n\nOnePlus\n\nThe Pipe expressions are modularized and easy to handle.\nNote the argument -u (unquoted) to get string without quotes\n\n\n\nB. contains official docs\njp -u -f data.json 'products | contains([].name,`\"OnePlus\"`)'\n\ntrue\n\ncontains gives out true or false; simplest example contains('foobar','bar') will give true\n\njp -u -f data.json 'products[?contains(name, `\"Plus\"`)]'\n\n[\n  {\n    \"name\": \"OnePlus\",\n    \"price\": 699\n  }\n]\n\nWe can use contains to match a portion of text in a variable inside an array\n\n\n\nC. join official docs\n&gt;&gt; jp -f data.json 'products[*].name'\n\n[\n  \"iPhone\",\n  \"Samsung Galaxy\",\n  \"Google Pixel\",\n  \"OnePlus\"\n]\n\n&gt;&gt; jp -f data.json 'join(`\",\"`,products[*].name)'\n\"iPhone,Samsung Galaxy,Google Pixel,OnePlus\"\n\n&gt;&gt; jp -u -f data.json 'join(`\",\"`,products[*].name)'\niPhone,Samsung Galaxy,Google Pixel,OnePlus\n\nYou can use the -u argument when you want the output to be displayed as plain, unquoted strings instead of valid JSON\n\n\n\nD. keys official docs\n&gt;&gt; jp -f data.json 'keys(@)'\n\n[\n  \"products\"\n]\n\n&gt;&gt; jp -f data.json 'products[0] | keys(@)'\n\n[\n  \"name\",\n  \"price\"\n]"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#logical-or-and",
    "href": "posts/2023-01-12-jmespath-querying.html#logical-or-and",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "8. Logical OR and &&",
    "text": "8. Logical OR and &&\n\n\n\njp -f data.json 'products[?(price &gt;  `699` && price &lt; `999`)]'\n\n[\n  {\n    \"name\": \"Samsung Galaxy\",\n    \"price\": 899\n  },\n  {\n    \"name\": \"Google Pixel\",\n    \"price\": 799\n  }\n]\njp -f data.json 'products[?(contains(name, `\"Sam\"`) || price &lt; `899`)]'\n\n[\n  {\n    \"name\": \"Samsung Galaxy\",\n    \"price\": 899\n  },\n  {\n    \"name\": \"Google Pixel\",\n    \"price\": 799\n  },\n  {\n    \"name\": \"OnePlus\",\n    \"price\": 699\n  }\n]"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#let-us-analyze-an-example-of-an-aws-cli-output-json",
    "href": "posts/2023-01-12-jmespath-querying.html#let-us-analyze-an-example-of-an-aws-cli-output-json",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "1. Let us analyze an example of an AWS cli output json",
    "text": "1. Let us analyze an example of an AWS cli output json\naws lambda list-functions --output json &gt;&gt; aws_example.json && cat aws_example.json\n\n{\n  \"Functions\": [\n    {\n      \"FunctionName\": \"my-function-1\",\n      \"FunctionArn\": \"arn:aws:lambda:us-east-1:1234567890:function:my-function-1\",\n      \"Runtime\": \"nodejs12.x\",\n      \"MemorySize\": 128,\n      \"Timeout\": 3,\n      \"LastModified\": \"2023-06-18T10:15:00Z\"\n    },\n    {\n      \"FunctionName\": \"my-function-2\",\n      \"FunctionArn\": \"arn:aws:lambda:us-east-1:1234567890:function:my-function-2\",\n      \"Runtime\": \"python3.8\",\n      \"MemorySize\": 256,\n      \"Timeout\": 5,\n      \"LastModified\": \"2023-06-17T14:30:00Z\"\n    },\n    {\n      \"FunctionName\": \"my-function-3\",\n      \"FunctionArn\": \"arn:aws:lambda:us-east-1:1234567890:function:my-function-3\",\n      \"Runtime\": \"java11\",\n      \"MemorySize\": 512,\n      \"Timeout\": 10,\n      \"LastModified\": \"2023-06-16T09:45:00Z\"\n    }\n  ]\n}\n\nQ1. Query all lambda functions running python\n\nBased on how you want to parse the output, you can have it as a list or just the first element by accessing [0]\n\njp -f aws_example.json 'Functions[?starts_with(Runtime,`\"python\"`)]'\n[\n  {\n    \"FunctionArn\": \"arn:aws:lambda:us-east-1:1234567890:function:my-function-2\",\n    \"FunctionName\": \"my-function-2\",\n    \"LastModified\": \"2023-06-17T14:30:00Z\",\n    \"MemorySize\": 256,\n    \"Runtime\": \"python3.8\",\n    \"Timeout\": 5\n  }\n]\n\njp -f aws_example.json 'Functions[?starts_with(Runtime,`\"python\"`)] | [0]' \n\n{\n  \"FunctionArn\": \"arn:aws:lambda:us-east-1:1234567890:function:my-function-2\",\n  \"FunctionName\": \"my-function-2\",\n  \"LastModified\": \"2023-06-17T14:30:00Z\",\n  \"MemorySize\": 256,\n  \"Runtime\": \"python3.8\",\n  \"Timeout\": 5\n}\n\n\nQ2. Query all lambda functions using memory more than 128 MB\njp -f aws_example.json 'Functions[?MemorySize &gt; `128`]'\n\n[\n  {\n    \"FunctionArn\": \"arn:aws:lambda:us-east-1:1234567890:function:my-function-2\",\n    \"FunctionName\": \"my-function-2\",\n    \"LastModified\": \"2023-06-17T14:30:00Z\",\n    \"MemorySize\": 256,\n    \"Runtime\": \"python3.8\",\n    \"Timeout\": 5\n  },\n  {\n    \"FunctionArn\": \"arn:aws:lambda:us-east-1:1234567890:function:my-function-3\",\n    \"FunctionName\": \"my-function-3\",\n    \"LastModified\": \"2023-06-16T09:45:00Z\",\n    \"MemorySize\": 512,\n    \"Runtime\": \"java11\",\n    \"Timeout\": 10\n  }\n]"
  },
  {
    "objectID": "posts/2023-01-12-jmespath-querying.html#let-us-analyze-a-more-complicated-example-from-official-jmespath-tutorial.",
    "href": "posts/2023-01-12-jmespath-querying.html#let-us-analyze-a-more-complicated-example-from-official-jmespath-tutorial.",
    "title": "A Gentle Introduction to JMESPath - an intuitive way to parse JSON documents",
    "section": "2. Let us analyze a more complicated example from official jmespath tutorial.",
    "text": "2. Let us analyze a more complicated example from official jmespath tutorial.\n\nIt looks like the state of EC2 instances\n\ncat official_example_for_nested.json\n{\n  \"reservations\": [\n    {\n      \"instances\": [\n        {\"type\": \"small\",\n         \"state\": {\"name\": \"running\"},\n         \"tags\": [{\"Key\": \"Name\",\n                   \"Values\": [\"Web\"]},\n                  {\"Key\": \"version\",\n                   \"Values\": [\"1\"]}]},\n        {\"type\": \"large\",\n         \"state\": {\"name\": \"stopped\"},\n         \"tags\": [{\"Key\": \"Name\",\n                   \"Values\": [\"Web\"]},\n                  {\"Key\": \"version\",\n                   \"Values\": [\"1\"]}]}\n      ]\n    }, {\n      \"instances\": [\n        {\"type\": \"medium\",\n         \"state\": {\"name\": \"terminated\"},\n         \"tags\": [{\"Key\": \"Name\",\n                   \"Values\": [\"Web\"]},\n                  {\"Key\": \"version\",\n                   \"Values\": [\"1\"]}]},\n        {\"type\": \"xlarge\",\n         \"state\": {\"name\": \"running\"},\n         \"tags\": [{\"Key\": \"Name\",\n                   \"Values\": [\"DB\"]},\n                  {\"Key\": \"version\",\n                   \"Values\": [\"1\"]}]}\n      ]\n    }\n  ]\n}\n\nQ1. Find all instances that are running and give me a count of them\njp -f official_example_for_nested.json 'reservations[].instances[?state.name == `\"running\"`][]'  \n[\n  {\n    \"state\": {\n      \"name\": \"running\"\n    },\n    \"tags\": [\n      {\n        \"Key\": \"Name\",\n        \"Values\": [\n          \"Web\"\n        ]\n      },\n      {\n        \"Key\": \"version\",\n        \"Values\": [\n          \"1\"\n        ]\n      }\n    ],\n    \"type\": \"small\"\n  },\n  {\n    \"state\": {\n      \"name\": \"running\"\n    },\n    \"tags\": [\n      {\n        \"Key\": \"Name\",\n        \"Values\": [\n          \"DB\"\n        ]\n      },\n      {\n        \"Key\": \"version\",\n        \"Values\": [\n          \"1\"\n        ]\n      }\n    ],\n    \"type\": \"xlarge\"\n  }\n]\njp -f official_example_for_nested.json 'length(reservations[].instances[?state.name == `\"running\"`][])'  \n\n2\n\nTwo instances are running\nNote the [] in the end to flatten the list. A simpler example below:\n\necho \"[[0,1],2,3,[4,5,6]]\" | jp '[]'\n\n[\n  0,\n  1,\n  2,\n  3,\n  4,\n  5,\n  6\n]\n\n\nQ2. Find the status of instances of type large or xlarge\njp -f official_example_for_nested.json 'reservations[].instances[?(type==`\"xlarge\"` || type==`\"large\"`)][]'\n\n[\n  {\n    \"state\": {\n      \"name\": \"stopped\"\n    },\n    \"tags\": [\n      {\n        \"Key\": \"Name\",\n        \"Values\": [\n          \"Web\"\n        ]\n      },\n      {\n        \"Key\": \"version\",\n        \"Values\": [\n          \"1\"\n        ]\n      }\n    ],\n    \"type\": \"large\"\n  },\n  {\n    \"state\": {\n      \"name\": \"running\"\n    },\n    \"tags\": [\n      {\n        \"Key\": \"Name\",\n        \"Values\": [\n          \"DB\"\n        ]\n      },\n      {\n        \"Key\": \"version\",\n        \"Values\": [\n          \"1\"\n        ]\n      }\n    ],\n    \"type\": \"xlarge\"\n  }\n]\n\n\nQ3. When the instance is used for DB (tags.Values == [\"DB\"]), what is the instance type and give its type and state details alone\n\nWell, If you happen to get a good answer in jp, let me know :)\nI would rather use json module from python for more complex things\n\nimport json\ninstances_dict = json.load(open('official_example_for_nested.json', 'r'))\n\nfor element in instances_dict[\"reservations\"]:\n    for instance in element[\"instances\"]:\n        for tag in instance[\"tags\"]:\n            for value in tag[\"Values\"]:\n                if value == \"DB\":\n                    print(f'Instance Type: {instance[\"type\"]}')\n                    print(f'Instance State: {instance[\"state\"]}')\n                    print(f'Instance Tags: {instance[\"tags\"]}')\nInstance Type: xlarge\nInstance State: {'name': 'running'}\nInstance Tags: [{'Key': 'Name', 'Values': ['DB']}, {'Key': 'version', 'Values': ['1']}]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn by Blogging",
    "section": "",
    "text": "Hi there 👋 I’m Senthil Kumar 👨‍💻\n\n\n\nI am an Applied ML Engineer specialized in building Natural Language Processing applications using ML and DL.  This is my blog Learn by Blogging\n\n\nI’m here to pen my learnings in Natural Language Processing, Machine Learning and Software Engineering. The range of my topics include various NLP topics, some mathematics fundamentals, a lot of coding concepts, Cloud related topics, tool tips and more. Predominantly, I write more code-first learning approaches to different topics.\n\n\nI understand what I write here could just be a drop in an ocean of good materials out there. This is my humble attempt to keep learning, unlearning and relearning as much as possible by writing about them.\n\n\nHit me up in LinkedIn if what I do interests you !\n\n\nHow I Learn by Blogging\n\n\n\n\n\nWant to sample any of my blog post?\n\n\n\n\n\n\n\nMy Collection of Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Blog Post 1 in Toyota Connected India Medium Space\n\n\n\n\n\n\n\n\nTCIN Blog by Senthil Kumar\n\n\n\n\n\n\n\n\n\n\n\nMy Blog Post 2 in Toyota Connected India Medium Space\n\n\n\n\n\n\n\n\nTCIN Blog by Senthil Kumar"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Learn by Blogging",
    "section": "",
    "text": "The Mental Model for Leveraging LLMs in Cloud\n\n\n\n\n\n\n\nLLM\n\n\nAWS\n\n\nServerless\n\n\n\n\nIn the blog post, I explore the compute environments for the plethora of OpenSource LLMs in the market\n\n\n\n\n\n\nJun 19, 2024\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Frugal Open Source LLM Applications using Serverless Cloud\n\n\n\n\n\n\n\nLLM\n\n\nServerless\n\n\nAWS\n\n\nNLP\n\n\n\n\nThis blog covers 4 Serverless LLM Recipes useful from the point of view of learning and building PoCs\n\n\n\n\n\n\nMay 16, 2024\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nAll You Need to Know about the Golang-based Task Runner Build Tool\n\n\n\n\n\n\n\nBash Scripting\n\n\nTaskfiles\n\n\nAWS\n\n\nAWS CLI\n\n\nDevOps\n\n\n\n\nThis blog covers how to use the amazingly simple Task runner in the most productive way in 10 easy-to-understand sections\n\n\n\n\n\n\nDec 14, 2023\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nBuild a Replicable Serverless Python NLP App from Scratch\n\n\n\n\n\n\n\nAWS\n\n\nAWS Serverless\n\n\nAWS CLI\n\n\nDevOps\n\n\n\n\nThis blog was originally presented in PyCon India 2023 Open Spaces Talk on behalf of Toyota Connected India\n\n\n\n\n\n\nSep 30, 2023\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nBoost Your Productivity with Bash Script Recipes\n\n\n\n\n\n\n\nCoding\n\n\nBash Scripting\n\n\n\n\nThe bash script recipes discussed here can be used as-is like a mini-cookbook or for understanding the fundamentals of bash scripting\n\n\n\n\n\n\nAug 16, 2023\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to JMESPath - an intuitive way to parse JSON documents\n\n\n\n\n\n\n\nCoding\n\n\nBash Scripting\n\n\n\n\nI discovered jmespath when using aws cli --query. This post is an earnest attempt to spread the word about JMESPath expressions\n\n\n\n\n\n\nJan 12, 2023\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying the basics of Encoding and Decoding in Python\n\n\n\n\n\n\n\nCoding\n\n\nPython\n\n\n\n\nA short blog on my experiences dealing with non-ASCII characters in Python 3\n\n\n\n\n\n\nDec 9, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nAn Ode to the Ubiquitous Regex\n\n\n\n\n\n\n\nCoding\n\n\nPython\n\n\n\n\nA selected list of confusing Regex Patterns that helped me learn its working better\n\n\n\n\n\n\nNov 8, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nSQL Fundamentals - Learnings from Kaggle Learn Google BigQuery SQL Course\n\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\n\n\nThis blog holds my SQL notes from Kaggle.com/learn course on SQL Fundamentals\n\n\n\n\n\n\nOct 12, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nCommonly used PostgreSQL Commands\n\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\n\n\nLet us dig into the PostgreSQL Couplets to update RDS & common Python (Psycopg2) scripts to access PostgreSQL RDS\n\n\n\n\n\n\nSep 15, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nA Practical Guide to Bash Scripting\n\n\n\n\n\n\n\nCoding\n\n\nBash Scripting\n\n\n\n\nA quick guide manual to level up your understanding and use of Bash Scripting\n\n\n\n\n\n\nAug 3, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nThe Evolution of Transfer Learning until the Advent of the First Tranformers in NLP\n\n\n\n\n\n\n\nNLP\n\n\nTransformers\n\n\n\n\nAn overview covering important NLP milestones from pre-transfer learning era until the era of BERT (before GPT)\n\n\n\n\n\n\nApr 11, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Machine Learning Fundamentals - from a coder’s viewpoint\n\n\n\n\n\n\n\nCoding\n\n\nML\n\n\nPython\n\n\n\n\nThis blog is inspired from my notes on Kaggle Learn Course on ML. It has code snippets in Scikit-learn and Pandas to put concepts into practices\n\n\n\n\n\n\nMar 4, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Deep Learning Fundamentals - from a coder’s viewpoint\n\n\n\n\n\n\n\nCoding\n\n\nDL\n\n\nPython\n\n\n\n\nThis blog is inspired from my notes on Kaggle Learn Course on DL. It has code snippets in Keras and Pytorch to put concepts into practices\n\n\n\n\n\n\nMar 4, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nPart 2 - A Primer on Statistics\n\n\n\n\n\n\n\nStatistics\n\n\n\n\nMy notes from a LinkedIn Course on Statistics\n\n\n\n\n\n\nFeb 27, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nPart 1 - A Primer on Probability\n\n\n\n\n\n\n\nStatistics\n\n\n\n\nMy notes from a LinkedIn Course on Probability\n\n\n\n\n\n\nFeb 3, 2022\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch Fundamentals for NLP - Part 2\n\n\n\n\n\n\n\nNLP\n\n\nCoding\n\n\n\n\nThis blog post explains how to build Linear Text Classifiers using PyTorch’s modules such as nn.EmbeddingBag and nn.Embedding functions to convert tokenized text into embeddings\n\n\n\n\n\n\nSep 15, 2021\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch Fundamentals for NLP - Part 1\n\n\n\n\n\n\n\nNLP\n\n\nCoding\n\n\n\n\nThis blog post explains the use of PyTorch for building a bow-based Text Classifier\n\n\n\n\n\n\nAug 28, 2021\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to PyTorch Fundamentals for Training DL Models\n\n\n\n\n\n\n\nCoding\n\n\nDL\n\n\n\n\nThis blog post explains the basics of PyTorch Tensors, the workflow to train a 2 layer Neural Network for a vision dataset and track the progress in a Tensorboard\n\n\n\n\n\n\nAug 15, 2021\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nHow to train a Spacy NER Model\n\n\n\n\n\n\n\nNLP\n\n\nDL\n\n\nCoding\n\n\nPython\n\n\n\n\nIn this blog post, I cover the process of creating trained ML NER model from Unlabeled data\n\n\n\n\n\n\nJun 25, 2021\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nHow to Leverage Spacy Rules NER\n\n\n\n\n\n\n\nNLP\n\n\nCoding\n\n\nPython\n\n\n\n\nThis blog post outlines the important features in Spacy Rules NER\n\n\n\n\n\n\nMay 9, 2021\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nAn In-depth Technical View on BERT\n\n\n\n\n\n\n\nNLP\n\n\nTransformers\n\n\n\n\nPart 2 - A deeper technical view of the BERT architecture\n\n\n\n\n\n\nDec 15, 2020\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nA Bird’s Eye View of BERT\n\n\n\n\n\n\n\nNLP\n\n\nTransformers\n\n\n\n\nPart 1 - My notes on how BERT works at a high level view\n\n\n\n\n\n\nNov 15, 2020\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nComparing Two Unsupervised Clustering Algorithms for Text Data\n\n\n\n\n\n\n\nML\n\n\nNLP\n\n\n\n\nA Short Study comparing PTW_LDA and Transfer Learning powered KMeans on Text Data\n\n\n\n\n\n\nSep 17, 2020\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nEvolution of RNN Architectures for Transfer Learning\n\n\n\n\n\n\n\nNLP\n\n\nDL\n\n\nML\n\n\n\n\nHighlighting the non-mathematical essentials in the evolution of RNN architectures in Transfer Learning (before Transformer-based models came to the fore)\n\n\n\n\n\n\nJul 17, 2020\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nThe Theory of Word2Vec Algorithm\n\n\n\n\n\n\n\nNLP\n\n\n\n\nThis blog post attempts to explain one of the seminal algorithms in NLP for embedding creation\n\n\n\n\n\n\nMay 3, 2020\n\n\nSenthil Kumar\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Statistical Language Modeling\n\n\n\n\n\n\n\nStatistics\n\n\n\n\nA gentle introduction to understand the ABCs of NLP in the era of Transformer LMs generating poems.\n\n\n\n\n\n\nMar 17, 2020\n\n\nSenthil Kumar\n\n\n\n\n\n\nNo matching items"
  }
]