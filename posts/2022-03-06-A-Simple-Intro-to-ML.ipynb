{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ad53a55c",
   "metadata": {},
   "source": [
    "---\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- Coding\n",
    "- ML\n",
    "- Python\n",
    "date: '2022-03-04'\n",
    "description: This blog is inspired from my notes on Kaggle Learn Course on ML. It has code snippets in Scikit-learn and Pandas to put concepts into practices\n",
    "output-file: 2022-03-04-Intro-to-ML.html\n",
    "title: Understanding Machine Learning Fundamentals - from a coder's viewpoint\n",
    "image: images/ML_fundamentals/ML.png\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99a60b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd1f2d",
   "metadata": {},
   "source": [
    "## Key Learnings from `Intro to ML Course` in Kaggle Learn\n",
    "- How to `train_test_split` the data\n",
    "- Briefly discussed the concept of underfitting and overfitting (Loss vs model complexity curve)\n",
    "- How to train a typical scikit-learn model like `DecisionTreeRegressor` or `RandomForestRegressor` \n",
    "    - both need no scaling of continuous or discrete data;\n",
    "    - for sklearn might have to convert categorical data into encoded values\n",
    "- After finding out the best parameters, one should train with the identified hyperparameters on the whole data \n",
    "    - (so that model will learn a bit more from held out data too) \n",
    "       \n",
    "## Key Learnings from `Intermediary ML Course` in Kaggle Learn\n",
    "\n",
    "### Missing Value Treatment:\n",
    "- Remove the Null Rows OR Columns (by column meaning the whole feature containing the missing value)\n",
    "- Impute (by some strategy like Mean, Median, some regression like KNN)\n",
    "- Impute + Add a boolean variable for every column imputed (so as to make the model hopefully treat the imputed row differently)\n",
    "- Do removing missing values help or imputing missing values help more for the model accuracy?\n",
    "- Opinion shared by the Author: SimpleImputer works as effectively as a complex imputing algorithm when used inside sophisticated ML models\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Categorical Column Treatment:\n",
    "    - `Drop Categorical columns` (worst approach)\n",
    "    - `OrdinalEncoder` \n",
    "    - `OneHotEncoder` (most cases, the best approach)\n",
    "- Learnt the concept of \"good_category_cols\" and \"bad_category_columns\" <br> (if a particular class occurs new in the unseen dataset; `handle_unknown` argument in \"OneHotEncoder\" is possible)\n",
    "- Think twice before applying onehot encoding because of \"high cardinality columns\"\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Data Leakage\n",
    "- An example of Data Leakage: \n",
    "    - Doing Inputer before train_test_split. Validation data then would have \"seen\" training data  \n",
    "\n",
    "**Example 1 - `Nike`**: <br>\n",
    "\n",
    "- Objective: How much shoelace material will be used?\n",
    "- Situation: With the feature `Leather used this month` , the prediction accuracy is 98%+. Without this featur, the accuracy is just ~80%\n",
    "- IsDataLeakage?: `Depends` ! \n",
    "    - ❌ `Leather used this month` is a bad feature if the number is populated during the month (which makes it not available to predict the amount of shoe lace material needed)\n",
    "    - ✔️ `Leather used this month` is a okay feature to use if the number determined during the beginning of the month (making it available during predition time on unseen data)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example 2 - `Nike`**: <br>\n",
    "\n",
    "- Objective: How much shoelace material will be used?\n",
    "- Situation: Can we use the feature `Leather order this month`?\n",
    "- IsDataLeakage? `Most likely no, however ...`\n",
    "    - ❌ If `Shoelaces ordered` (our Target Variable) is determined first and then only `Leather Ordered` is planned, <br>\n",
    "   then we won't have `Leather Ordered` during the time of prediction of unseen data\n",
    "    - ✔️ If `Leather Ordered` is determined before `Shoelaces Ordered`, then it is a useful feature\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example 3 - `Cryptocurrency`**: <br>\n",
    "\n",
    "- Objective: Predicting tomo's crypto price with a error of <$1\n",
    "- Situation: Are the following features susceptible to leakage?\n",
    "    - `Current price of Crypto`\n",
    "    - `Change in the price of crypto from 1 hour ago`\n",
    "    - `Avg Price of crypto in the largest 24 h0urs`\n",
    "    - `Macro-economic Features`\n",
    "    - `Tweets in the last 24 hours `\n",
    "- IsDataLeakage? `No`, none of the features seem to cause leakage.\n",
    "- However, more useful Target Variable `Change in Price (pos/neg) the next day`. If this can be consistently predicted higher, then it is a useful model\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example 4 - `Surgeon's Infection Rate Performance`**: <br>\n",
    "\n",
    "- Objective: How to predict if a patient who has undergone a surgery will get infected post surgery?\n",
    "- Situation: How can information about each surgeon's infection rate performance be carefully utilized while training?\n",
    "    - The independent features are strictly data points collected until the surgery had taken place\n",
    "    - The dependent variable - whether infected or not - should be post surgery measurement \n",
    "- IsDataLeakage? `Depends` on the what are the features used.\n",
    "    - If a surgeon's infection rate is used as a feature while training the model (that predicts whether a patient will be infected post surgery), that will lead to data leakage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f82b3c",
   "metadata": {},
   "source": [
    "## Key Learnings from `Feature Engineering` Course in Kaggle Learn\n",
    "\n",
    "- Key Topics of this course: \n",
    "    - Mutual Information\n",
    "    - Inventing New Features (like `apparent temparature` = {Air Temparature + Humidity + Wind Speed})\n",
    "    - Segmentation Features (using K-Means Clustering)\n",
    "    - Variance in the Dataset based features (using Principal Component Analysis)\n",
    "    - Encode (high cardinality) category variables using `Target Encoding`\n",
    "- Why Feature Engineering?\n",
    "    - To improve model performance\n",
    "    - To reduce computational complexity by combining many features into a few\n",
    "    - To improve interpretability of results\n",
    "- Wherever the model cannot identify a proper relationship between a dependent and a particular independent variable, <br>\n",
    "    - we can engineer/transform 1 or more of the independent variables \n",
    "    - so as to let model learn a better relationship between the engineered features and dependent variable \n",
    "- E.g.: In `compressive_strength` prediction in `cement` data, synthetic feature - ratio of Water to Cement helps\n",
    "\n",
    "### Mutual Information\n",
    "- Mutual information is similar to correlation but correlation only looks for linear relationship whereas Mutual information can talk about any relationship\n",
    "- `Mutual Information` decribes relationship between two variables in terms of uncertainty (or certainty)\n",
    "    - For e.g.: Knowing `ExteriorQuality` of a house (one of 4 values - Fair, Typical, Good and Excellent) can help one reduce uncertainty over `SalePrice`. Better the ExteriorQuality, more the SalesPrice\n",
    "    - Typical values: If two variables have a MI score of 0.0 - they are totally indepndent. \n",
    "    - Mutual Information is a logarithmic quantity. So it increases slowly\n",
    "    - Mutual Information is a univariate metric. MI can't detect interactions between features Meaning, if multiple features together make sense to a dependent variable but not independently, then MI cannot determine that. Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects\n",
    "- Parallel Read for MI like metrics: \n",
    "    - [Feature Importances from fitted attribute](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)  \n",
    "    - [Recursive Feature Elimination](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "\n",
    "### Types of New Features:\n",
    "- Mathematical Transformations (Ratio, Log)\n",
    "- Grouping Columns Features\n",
    "      - `df['New_Group_Feature'] = df[list_of_boolean_features].sum(axis=1)`\n",
    "      - `df['New_Group_Feature'] = df[list_of_numerical_features].gt(0).sum(axis=1)` gt -- greater than \n",
    "- Grouping `numerical` Rows Features\n",
    "      - `customer['Avg_Income_by_State'] = customer.groupby('State')['Income'].transform('mean')`\n",
    "- Grouping `categorical` columns features\n",
    "      - `customer['StateFreq'] = customer.groupby('State')['State'].transform('count')/customer.State.count()`\n",
    "- Split Features\n",
    "      - df[['Type', 'Count']] = df['some_var'].str.split(\" \",expand=True)\n",
    "- Combine Features\n",
    "      - `df['new_feture'] = df['var1'] + \"_\" + df['var2']` \n",
    "       \n",
    "### Useful Tips on Feature Engineering:\n",
    "- Linear models learn sum and differences naturally\n",
    "- Neural Networks work better with scaled features\n",
    "- Ratios are difficult for many models, so can yeild better results when incorporated as additional feature\n",
    "- Tree models do not have the ability to factor in cound feature\n",
    "- Clustering as `feature discovery` tool (add a categorical feature based on clustering of a subset of features)\n",
    "\n",
    "### Principal Component Analysis\n",
    "- PCA is like partitioning of the variation in data\n",
    "- Instead of describing the data with the original features,\n",
    "- you do an orthogonl transformation of the features and compute \"principal components\" \n",
    "- which are used to explain the variation in the data. \n",
    "- Convert the correlated variables into mutually orthogonal (uncorrelated) `principal components`\n",
    "- Principal components can be more informative than the original features\n",
    "- Advantages of PCA:\n",
    "      - Dimensionality Reducton\n",
    "      - Anamoly Detection\n",
    "      - Boosting signal to noise ratio\n",
    "      - Decorrelation\n",
    "- PCA works only for numeric variables; works best for scaled data  \n",
    "- **Pipeline for PCA**: original_features --> Scaled_features --> PCA Features --> MI_computed_on_PCA_features\n",
    "\n",
    "### Target Encoding\n",
    "- `Target Encoding`: A Supervised Feature Engineering technique for encoding categorical variables by including the target labels\n",
    "- Target Encoding is basically assigning a number to a categorical variable where in the number is derived from target variable\n",
    "      - `autos['target_encoded_make'] =  autos.groupby('make')['Price'].transform('mean')`\n",
    "- Disadvantages of Target Encoding:\n",
    "      - Overfits for low volume (rare) classes\n",
    "      - What if there are missing values  \n",
    "- Where is Target Encoding most suitable?\n",
    "      - For High cardinality features\n",
    "      - Domain-motivated features (features that could have been scored poorly using feature importance metric function, we can unearth its real usefulness using target encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df336f",
   "metadata": {},
   "source": [
    "## ML Coding Tips\n",
    "- **sklearn**: \n",
    "    - `Pipleine`: Bundles together `preprocessing` and `modeling` steps | makes codebase easier for productionalizing\n",
    "    - `ColumnTransformer`: Bundles together different preprocessing steps\n",
    "    - Sklearn classes used often:\n",
    "    - **Model**\n",
    "        - `from sklearn.tree import DecisionTreeRegressor` AND `from sklearn.ensemble import RandomForestRegressor`\n",
    "        - `from sklearn.model_selection import train_test_split`\n",
    "        - `from sklearn.metrics import mean_absolute_error`\n",
    "        - `from sklearn.model_selection import cross_val_score`\n",
    "             - cross_val_score(my_pipeline, X, y, scorung='neg_mean_absolute_error')\n",
    "        - `from xgboost import XGBRegressor`\n",
    "             - `n_estimators`: Number of estimators is same as the number of cycles the data is processed by the model (`100-1000`)\n",
    "             - `early_stopping_rounds`: Early stopping stops the iteration when the validation score stops improving \n",
    "             - `learning_rate`\n",
    "                   - xgboost_model = XGBRegressor(n_estimators=500)\n",
    "                   - xgboost_model.fit(X_train,y_train,early_stopping_rounds=5,eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "    - **Preprocessing & Feature Engineering**\n",
    "        - `from sklearn.feature_selection import mutual_info_regression, mutual_info_classif`\n",
    "        - `from sklearn.decomposition import PCA`\n",
    "        - `from sklearn.impute import SimpleImputer, KNNImputer`\n",
    "        - `from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    " \n",
    "    \n",
    "- **Pandas**: \n",
    "    - X = df.copy() y = X.pop('TheDependentVariable') # remove the dependent variable from the X (features) and save in y)\n",
    "    - `df[encoded_colname], unique_values = df[colname].factorize()` # for converting a categorical list of values into encoded numbers using pandas\n",
    "    - `df[list_of_oh_encoded_col_name_values] = pd.get_dummies(df[colname])` # for converting a categorical variable into a list of oh-encoded-values using pandas\n",
    "    - Exclude all categorical columns at once:\n",
    "        - `df = df.select_dtypes(exclude=['object'])`\n",
    "    - Creating a new column just to show the model if which row of a particular column have null values\n",
    "        - df[col + '__ismissing'] = df[col].isnull() \n",
    "    - Isolate all categorical columns: \n",
    "        - `object_cols = [col for col in df.columns if df[col].dtype == \"object\"]`\n",
    "    - Segregate good and bad object columns (defined by the presence of \"unknown\" or new categories in validation or test dataset)\n",
    "        - `good_object_cols = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col])]`\n",
    "        - `bad_object_cols = list(set(good_object_cols) - set(bad_object_cols))`\n",
    "    - Getting number of unique entries (`cardinality`) across `object` or categorical columns\n",
    "        - `num_of_uniques_in_object_cols = list(map(lambda col: df[col].nunique(), object_cols))`  \n",
    "        - `sorted(list(zip(object_cols, num_of_uniques_in_object_cols)), key=lambda x: x[1], reverse=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b31c6",
   "metadata": {},
   "source": [
    "**Source**: <br>\n",
    "- Kaggle.com/learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_macos",
   "language": "python",
   "name": "tf_macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
