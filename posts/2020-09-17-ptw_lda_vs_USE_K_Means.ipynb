{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /Topic_Modeling/LDA/Seeded_LDA/Universal_Sentence_Encoder/KMeans/2020/09/17/ptw_lda_vs_USE_K_Means\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- ML\n",
    "- NLP\n",
    "date: '2020-09-17'\n",
    "description: A Short Study comparing PTW_LDA and Transfer Learning powered KMeans\n",
    "  on Text Data\n",
    "hide: false\n",
    "image: images/LDA_vs_KMeans/data1_evaluation_complete.PNG\n",
    "output-file: 2020-09-17-ptw_lda_vs_use_k_means.html\n",
    "title: Comparing Two Unsupervised Clustering Algorithms for Text Data\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac1f42",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684c916-fe04-498b-b1a0-e2b9545e6755",
   "metadata": {},
   "source": [
    "### What algorithms are covered here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3b866-be90-4dbe-84a2-5bd1f6682421",
   "metadata": {},
   "source": [
    "- **Prior-topic words** guided *Latent Dirichlet Allocation* and \n",
    "- **Universal Sentence Encoder** (USE) powered *K-Means*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff1440-a6cb-4429-9800-b81c3bc26e9f",
   "metadata": {},
   "source": [
    "### What questions am I answering here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb1a1f-bd99-41a6-85e1-8a218eb7d8c6",
   "metadata": {},
   "source": [
    "- Given that USE can encode complex semantic information, **is LDA worthwhile?** \n",
    "\n",
    "- **Can LDA be used for at least some type of data**, where it can produce better/on-par results compared to USE-K-Means? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f643b-37e5-433a-a686-b4480e790679",
   "metadata": {},
   "source": [
    "Before diving into the study, let us understand how **USE-KMeans** and **PTW-guided LDA** works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06d61f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbb50e-c514-4cf6-a590-6d0902785c0f",
   "metadata": {},
   "source": [
    "## II. About USE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e41d97-2538-4b0e-94ab-dacffd77ba37",
   "metadata": {},
   "source": [
    "### IIA. How USE works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32864a-671a-4586-943e-add07301c0e5",
   "metadata": {},
   "source": [
    "**USE** converts sentences into 512 embeddings\n",
    "![](https://www.learnopencv.com/wp-content/uploads/2018/11/Universal-Sentence-Encoder.png)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Semantic Similarity** Correlation Matrix\n",
    "![](https://2.bp.blogspot.com/-9Qk1fubLpzg/Wv2QGgKVVmI/AAAAAAAACvs/Gm-XF3prXVIIvaIkrTmkcIcYz-4qSxLKwCLcBGAs/s400/image2.png)\n",
    "\n",
    "<hr>\n",
    "\n",
    "Despite many common words, semantically different sentences will have **dissimilar embeddings** \n",
    "![](https://1.bp.blogspot.com/-w2kAi39zPrE/Wv2OPHTwDgI/AAAAAAAACvY/aQzvBcaIqYkw8McCBcXlTx0pj9FbILH0ACLcBGAs/s400/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9748f",
   "metadata": {},
   "source": [
    "### IIB. USE Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72816d62",
   "metadata": {},
   "source": [
    "There are two variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936befc",
   "metadata": {},
   "source": [
    "**Variant 1: Transformer Encoder**: <br>\n",
    "![](https://amitness.com/images/use-transformer-variant.png)\n",
    "\n",
    "<small>source: https://amitness.com/2020/06/universal-sentence-encoder/ </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24e26b",
   "metadata": {},
   "source": [
    "What does a **Transformer Encoder** Layer comprise?\n",
    "- Self Attention Layer \n",
    "- Feed Forward Network Layer\n",
    "![](https://amitness.com/images/use-transformer-one-layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd0106",
   "metadata": {},
   "source": [
    "**Variant 2: Deep Averaging Network**:<br>\n",
    "![](https://amitness.com/images/use-deep-averaging-network-variant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c122d",
   "metadata": {},
   "source": [
    "### IIC. Pre-trained Tasks in USE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d3efd",
   "metadata": {},
   "source": [
    "Overall Pipeline of how **USE** is trained: <br>\n",
    "![](https://amitness.com/images/use-overall-pipeline.png)\n",
    "\n",
    "<hr>\n",
    "\n",
    "A. Skip-thought prediction: <br>\n",
    "- Use `Central Sentence` to predict both the `Previous Sentence` and `Next Sentence`\n",
    "\n",
    "![](https://amitness.com/images/nlp-ssl-neighbor-sentence.gif)\n",
    "\n",
    "B. Response Prediction: <br>\n",
    "- Train the USE architecture to do `smart reply` prediction\n",
    "\n",
    "![](https://3.bp.blogspot.com/-qcqYQcxfLS0/Wv2Pxmm945I/AAAAAAAACvk/decC5VtlRGUdD4NqCui3HgNd3LXdjEvlgCLcBGAs/s640/image3.gif)\n",
    "\n",
    "C. Natural Language Inference: <br>\n",
    "- Do `NLI` task prediction, where given a premise and a hypothesis, the model is trained to predict whether the hypothesis is an `entailment`, `contradition` or `neutral` to the premise\n",
    "\n",
    "![](images/LDA_vs_KMeans/markdown_table1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233ba8e",
   "metadata": {},
   "source": [
    "## III. About LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7382a",
   "metadata": {},
   "source": [
    "- **Latent**: <br>\n",
    "    - Topic structures in a document are **latent/hidden** in the text\n",
    "\n",
    "\n",
    "- **Dirichlet**: <br>\n",
    "    - The **Dirichlet** distribution determines the mixture proportions of <br> \n",
    "            - the topics in the documents and\n",
    "            - the words in each topic.\n",
    "- **Allocation**: <br>\n",
    "    - **Allocation** of words to a given topic and allocation of topics to a document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726f2f9",
   "metadata": {},
   "source": [
    "### IIIA. Intuitive understanding of Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b98ea7",
   "metadata": {},
   "source": [
    "- A **Normal/Gaussian** distribution is a continuous probability distribution over all the real numbers\n",
    "    - It is described by a mean and a variance.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg)\n",
    "\n",
    "- A **Poisson** distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/16/Poisson_pmf.svg)\n",
    "\n",
    "> The Poisson distribution is specified by one parameter: lambda (λ). As lambda increases to sufficiently large values, the normal distribution (λ, λ) may be used to approximate the Poisson distribution\n",
    "> Source:https://en.wikipedia.org/wiki/Poisson_distribution \n",
    "\n",
    "<hr>\n",
    "\n",
    "- Now, what is **Dirichlet Distribution**? <br>\n",
    "    - The dirichlet distribution is a probability distribution as well\n",
    "    - but it is not sampling from the space of real numbers. Instead it is sampling over a probability simplex <br>\n",
    "\n",
    "```\n",
    " (0.6, 0.4)\n",
    " (0.1, 0.1, 0.8)\n",
    " (0.05, 0.2, 0.15, 0.1, 0.3, 0.2)\n",
    "```\n",
    "\n",
    "![](images/LDA_vs_KMeans/dirichlet_distribution_of_words_topics.PNG)\n",
    "\n",
    "- How Dirichlet Distribution varies w.r.t dirichlet prior: <br>\n",
    "    - The below image shows Dir(α) <br>\n",
    "    - As α increases from 0.05 (1/20) to 0.1, 0.2, 0.4 respectively in plots from left to right & top to down, you can see the distribution becoming more uniform. <br>\n",
    "![](images/LDA_vs_KMeans/dirichlet_distribution_with_prior.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9cfa25-c771-4b25-b24b-c1e5a1cbf9c5",
   "metadata": {},
   "source": [
    "### IIIB. How LDA Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92cc34-dc47-4768-b4e3-3f1e7463c85a",
   "metadata": {},
   "source": [
    "- LDA is a generative model <br>\n",
    "- LDA processes documents as 'bag of words' -- ordering of words is not important\n",
    "\n",
    "![](images/LDA_vs_KMeans/generative_process_1.png)\n",
    "\n",
    "In principle, LDA generates a document based on **dirichlet distribution (dd) of topics over documents** and **dd of words over topics**\n",
    "\n",
    "![](images/LDA_vs_KMeans/generative_process_2.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "But we inverse the generative process for statistical inference \n",
    "\n",
    "![](images/LDA_vs_KMeans/generative_process_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79322d",
   "metadata": {},
   "source": [
    "### IIIC. Hyperparameters of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654945f6",
   "metadata": {},
   "source": [
    "![](LDA_vs_KMeans/plate_notation_LDA.png)\n",
    "\n",
    "D = Total No. of Documents <br>\n",
    "N = No. of Words = Vocab Size <br>\n",
    "T = No. of Topics <br>\n",
    "<br>\n",
    "θd = Topic Distribution for a particular document d\n",
    "\n",
    "![](images/LDA_vs_KMeans/plate_notation_2.png)\n",
    "\n",
    "<br>\n",
    "Φt= Word Distribution for a topic t. Here for topic 1 and 2. \n",
    "\n",
    "![](images/LDA_vs_KMeans/plate_notation_3.png)\n",
    "\n",
    "(colored books represent words/tokens)\n",
    "\n",
    "```\n",
    "Zd,n = Topic Distribution for n th word in document d\n",
    "Wd,n = nth word in dth document\n",
    "```\n",
    "\n",
    "```\n",
    "α= parameter that sets the dircihlet prior on the per-document topic distribution (θ)\n",
    "= parameter that represents the doc-topic density\n",
    "= determines the no. of topics in each doc\n",
    "= (Default) = 1/num_of_topics (in sklearn and gensim)\n",
    "decreasing alpha results in less number of topics per document\n",
    " \n",
    "β= parameter that sets the dirichlet prior on the per-topic word distribution(ϕ)\n",
    "= parameter that represents the topic-word density\n",
    "= determines the no. of words per each topic\n",
    "= (Default) = 1/num_of_topics (in sklearn and gensim)\n",
    "decreasing beta results in just a few major words in topics\n",
    "\n",
    "m = base measure for per-document topic distribution; a simplex vector/array (m1, m2, …, mT) ; sums to one\n",
    "α = Concentration parameter α (positive scalar) \n",
    " \n",
    " α * m = (α1, α2, …., αT)\n",
    " \n",
    " \n",
    "n = base measure for per-topic word distribution; a simplex vector/array (n1, n2, …, nN); sums to one\n",
    "β = Concentration parameter β (positive scalar) \n",
    " \n",
    " β * n = (β1, β2, …., βT)\n",
    " \n",
    " \n",
    "There is also another hyper parameter η - topic coherence or perplexity - which can be used to determine the number of topics \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772af71",
   "metadata": {},
   "source": [
    "### IIID. Now, How does PTW-LDA work? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a5e31",
   "metadata": {},
   "source": [
    "- Nudge the regular LDA to converge faster and better with human-reviewed words list for each topic\n",
    "\n",
    "![](images/LDA_vs_KMeans/seededlda_1.png)\n",
    "\n",
    "- How the topics are seeded with some seed words\n",
    "![](images/LDA_vs_KMeans/seededlda_3.png)\n",
    "Source:Freecodecamp.org\n",
    "<br>\n",
    "\n",
    "- Chaotic LDA and Clear PTW_LDA outputs... <br>\n",
    "- LDA might need several hyperparameter tuning attempts to get to the desired splits\n",
    "\n",
    "- Default initialization with uniform word topic distribution <br>\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/1*RojQi7m5yBGHSD0Q0HGGYg.png)\n",
    "\n",
    "- Seeded Initialization\n",
    "![](https://cdn-media-1.freecodecamp.org/images/1*kMxQ47DFkpxIDBCyXeff6w.png)\n",
    "\n",
    "- The seeded words are guided towards seeded topics for converging faster\n",
    "![](images/LDA_vs_KMeans/simplistic_guided_LDA_notation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443dbc4",
   "metadata": {},
   "source": [
    "### IIID. Pre-processing and Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6494846",
   "metadata": {},
   "source": [
    "**Preprocessing**: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174edbb",
   "metadata": {},
   "source": [
    "| PTW-LDA                                                                                                                                                                                                     | USE +   Clustering                            |\n",
    "|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| 1. Stop words removed                                                                                                                                                                                       | No   pre-processing; Comments were used as is |\n",
    "| 2.   Lemmatized                                                                                                                                                                                             |\n",
    "| 3. Top   20 words per (ground truth) label was extracted                                                                                                                                                    |\n",
    "| 4.   Human-reviewed list of 20 word lists (each corresponding to 1 topic) were   chosen as prior topic words input (SAT is useful when there are prior topic   words fed; otherwise it works as normal LDA) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b20253",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b7c75",
   "metadata": {},
   "source": [
    "| PTW-LDA   <br>(best possible based on heuristics and <br>limited # of experimentations)                      | USE +   Clustering           |\n",
    "|------------------------------------------------------------------------------------------------------|------------------------------|\n",
    "| No. Of Topics                                                                                        | No. Of clusters              |\n",
    "| Max Iterations                                                                                       | Max Iterations (for K-Means) |\n",
    "| (default) Doc_topic_prior =   alpha = 1/ no_of_topics                                                |\n",
    "| (default) topic_word_prior =   beta = 1/ no_of_topics                                                |\n",
    "| Learning_method:   \"batch\" (whole dataset is used)                                                   |\n",
    "| (alternative is 'online'   which uses only batch size no. Of comments; similar to mini_batch_kmeans) |\n",
    "| Seeded_words_list                                                                                    |\n",
    "| Seed_coefficent/seed   confidence (how much to nudge the seeded words)                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b6bca",
   "metadata": {},
   "source": [
    "## IV. Comparison Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b160ca",
   "metadata": {},
   "source": [
    "- Data I - `20 Newsgroups` - Supervised Evaluation <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2258d",
   "metadata": {},
   "source": [
    "![](images/LDA_vs_KMeans/data1_evaluation_complete.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcb1dc",
   "metadata": {},
   "source": [
    "[Source for Adjusted Mutual Information Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6541096",
   "metadata": {},
   "source": [
    "- Data II -`ABC (Australian Broadcast Corporation) Corpus` - Unsupervised Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196e349-269b-45f7-94ef-e224894388f3",
   "metadata": {},
   "source": [
    "|     Metric    |     PTW-LDA    |     USE-Clustering    |\n",
    "|-|-|-|\n",
    "|     Word Embedding   based Coherence Score <br> (more the coherence score, better is the clustering output)          |     Coherence   Score for 20 topics: 0.091 <br>       <br>    |     Coherence   Score for 20 clusters : 0.159          |\n",
    "|     Methodology of   computing the above metric    |                Take the top 10         words that constitute each of the 20 topics          (each topic    comprises of a probability simplex of the words; select the top 10 highly    probable words in that topic)      <br>      <br>      For our case, the    top 10 words used for coherence computation in the 10 topics are:       <br>      <br>      [['police',  'baghdad',     'war',  'probe',  'protest',     'anti',  'missing',  'man',     'fight',  'coalition'],    <br>      ['report',  'pm',     'death',  'korea',  'claim',     'war',  'north',  'nt',     'toll',  'protesters'],    <br>      ['win',  'govt',     'set',  'community',  'end',     'wins',  'vic',  'indigenous',  'road',     'help'], <br>      ['world',  'cup',     'australia',  'found',  'ban',     'plans',  'lead',  'gets',     'expected',  'match'],    <br>      ['un',  'coast',     'title',  'takes',  'peace',     'iraq',  'gold',  'defence',     'residents',  'play'],    <br>      ['iraq',  'iraqi',     'war',  'says',  'troops',     'killed',  'dead',  'hospital',  'clash',     'forces'], <br>      ['council',  'boost',     'mp',  'fire',  'group',     'qld',  'minister',  'defends',     'land',  'welcomes'],    <br>      ['man',  'court',     'charged',  'face',  'plan',     'open',  'murder',  'urged',     'case',  'charges'], <br>      ['new',  'oil',     'dies',  'security',  'crash',     'sars',  'high',  'year',     'house',  'car'], <br>      ['water',  'rain',     'claims',  'wa',  'nsw',     'farmers',  'drought',  'howard',     'centre',  'union']] <br>      <br>                   For each topic,         taking 2 out of the top 10 words at a time,  compute cosine similarity using         pre-trained W2V embedding          <br>            Overall         Coherence is sum of similarity scores of all possible pairs of words         for each topic, normalized by the          no. of topics                  |                Take the top 10         words that constitute each of the 20 clusters          (top 10 words    (from stop-words removed set) are computed based on their TF-IDF weighted    scores in that cluster)      <br>      <br>            For each         cluster, taking 2 out of the top 10 words at a time,  compute similarity using pre-trained         W2V embedding          <br>      <br>      For our case, the    top 10 words used for coherence computation in the 10 clusters are:   <br>      <br>     [['win',  'cup',     'final',  'wins',  'world',     'afl',  'coach',  'england',     'season',  'day'], <br>       ['council',     'plan',  'market',  'funding',     'boost',  'housing',  'water',     'funds',  'budget',  'rise'],     <br>       ['crash',     'man',  'killed',  'death',     'dies',  'dead',  'injured',     'woman',  'car',  'sydney'], <br>       ['interview',  'michael',     'business',  'abc',  'news',     'market',  'analysis',  'david',     'extended',  'andrew'],  <br>       ['australia',  'australian',  'aussie',     'sydney',  'australians',  'day',     'aussies',  'australias',  'melbourne',  'south'], <br>       ['abc',     'country',  'hour',  'news',     'weather',  'grandstand',  'friday',     'nsw',  'drum',  'monday'], <br>       ['govt',     'election',  'council',  'government',  'minister',  'pm',     'parliament',  'nsw',  'anti',     'trump'], <br>       ['police',     'man',  'court',  'murder',     'charged',  'accused',  'death',     'guilty',  'charges',  'assault'], <br>       ['farmers',     'water',  'drought',  'industry',  'farm',     'coal',  'green',  'cattle',     'mining',  'nsw'], <br>       ['health',     'hospital',  'flu',  'mental',     'doctors',  'treatment',  'cancer',     'drug',  'service',  'care']] <br>      <br>      <br>            Overall         Coherence is sum of similarity scores of all possible pairs of words         for each cluster, normalized by the          no. of clusters          <br>       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c81828-7e89-4c8d-8d8a-a31c7b7418b3",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81ec72-ff56-4c93-8f15-bbfe5a7f0f44",
   "metadata": {},
   "source": [
    "- We have used two News corpus of varying text length. One dataset has ground truth labels and the other doesn't have labels\n",
    "\n",
    "\n",
    "- In our comparison of **PTW-LDA vs USE-Clustering** \n",
    "    - using both 'Supervised' (Adjusted Mutual Information Score) and 'Unsupervised' evaluation metrics, USE-clustering performs far superior to PTW-LDA despite repeated attempts at different set of hyper-parameters for PTW-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9933e6cc",
   "metadata": {},
   "source": [
    "**Important References**:<br>\n",
    "**USE**: <br> \n",
    "- https://amitness.com/2020/06/universal-sentence-encoder/ <br>\n",
    "- USE Paper: https://arxiv.org/abs/1803.11175    \n",
    "\n",
    "**LDA**: <br>\n",
    "- The original paper of LDA by David Blei \n",
    "- https://www.slideshare.net/hustwj/nicolas-loeff-lda\n",
    "- http://videolectures.net/mlss09uk_blei_tm/\n",
    "- http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf\n",
    "- https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution (it compares \n",
    "normal distribution with Dirichlet Distribution)\n",
    "- https://ldabook.com/lda-as-a-generative-model.html\n",
    "- https://ldabook.com/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98c3ded5f4c982d767ead9cded27e95b53d0df25404a508cedfb98865b9710c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
