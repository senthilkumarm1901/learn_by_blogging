{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /pytorch_for_nlp/2021/08/28/Pytorch_for_NLP_PART_I\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- NLP\n",
    "- Coding\n",
    "date: '2021-08-28'\n",
    "description: This blog post explains the use of PyTorch for building a bow-based Text\n",
    "  Classifier\n",
    "hide: false\n",
    "image: images/pytorch_nn/linear_bag_of_words_based_text_classifier.png\n",
    "output-file: 2021-08-28-pytorch_for_nlp_part_i.html\n",
    "title: PyTorch Fundamentals for NLP - Part 1\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a417bf",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e61933",
   "metadata": {},
   "source": [
    "**Why `NLP` has grown in recent years?**\n",
    "- Because of the improvement in the ability of Language Models (such as `BERT` or `GPT-3`) to accurately `understand` human language\n",
    "- Easy to train these LMs as they learn from performing `unsupervised pretraining` tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1816c7c",
   "metadata": {},
   "source": [
    "**What are the common types of NLP Applications for which NNs are built?**\n",
    "- `Text Classification` | E.g.: Email Spam classification, Intent Classification of incomming messages in Chatbots\n",
    "- `Sentiment Analysis` | A regression task (outputs a number from most negative `-1` to most positive `+1` | Note: Training data needs to have outputs in range too)\n",
    "- `NER` | a component of `Information Retrieval` | We classify every token (typically tokens that are proper nouns) a pre-defined entity which is then used for some downstream\n",
    "- `NER` and `Intent Classification` can be used together with intent classification\n",
    "    - E.g.: \"*Ok Google, Search apartments in Thoraipakam*\"\n",
    "    - **Intent**: Search | Entity_1 (search_entity) `apartments` | Entity_2 (search_filter_location) `Thoraipakkam` \n",
    "- `Text Summarization`\n",
    "- `Question-Answer` Systems | Typicall Closed domain system where in the answer to a question is in the context\n",
    "    - **Context**: \"*Joe Biden became US President in 2021 succedding Donald Trump*\"\n",
    "    - **Query**: \"*Who was the President of the US before Joe Biden*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603947d1",
   "metadata": {},
   "source": [
    "In this blog piece, let us cover \n",
    "- `text classification` task using a `bow` based vectorizer + `nn.Linear` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196b171",
   "metadata": {},
   "source": [
    "## 2.Representing Text as Tensors - A Quick Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417206e",
   "metadata": {},
   "source": [
    "**How do computers represent text?**\n",
    "- Using encodings such as ASCII values to represent each character\n",
    "\n",
    "![](https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/images/ascii-character-map.png)\n",
    "\n",
    "Source: github.com/MicrosoftDocs/pytorchfundamentals\n",
    "\n",
    "\n",
    "> Still computers cannot `interpret` the meaning of the words , they just `represent` text as ascii numbers in the above image\n",
    "\n",
    "\n",
    "**How is text converted into embeddings?** <br>\n",
    "\n",
    "- Two types of representations to convert text into numbers\n",
    "\n",
    "    - Character-level representation\n",
    "    - Word-level representation\n",
    "    - Token or sub-word level representation\n",
    "    \n",
    "- While Character-level and Word-level representations are self explanatory, Token-level representation is a combination of the above two approaches. \n",
    "\n",
    "<u>Some important terms</u>: <br>\n",
    "\n",
    "- **Tokenization** (sentence/text --> tokens): In the case sub-word level representations, for example, `unfriendly` will be **tokenized** as `un, #friend, #ly` where `#` indicates the token is a continuation of previous token. \n",
    "- This way of tokenization can make the model learnt/trained representations for `friend` and `unfriendly` to be closer to each other in the vector spacy\n",
    "\n",
    "- **Numericalization** (tokens --> numericals): This is the step where we convert tokens into integers.\n",
    "\n",
    "- **Vectorization** (numericals --> vectors): This is the process of creating vectors (typically sparse and equal to the length of the vocabulary of the corpus analyzed)\n",
    "\n",
    "- **Embedding** (numericals --> embeddings): For text data, embedding is a lower dimensional equivalent of a higher dimensional sparse vector. Embeddings are typically dense. Vectors are sparse. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Typical Process of Embedding Creation** <br>\n",
    "- `text_data` >> `tokens` >> `numericals` >> sparse `vectors` or dense `embeddings` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f681a",
   "metadata": {},
   "source": [
    "## 3. A Text Classification Pipeline to build BoW Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a55b51",
   "metadata": {},
   "source": [
    "- Dataset considered: **AG_NEWS** dataset that consists of 4 classes - `World, Sports, Business and Sci/Tech`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221fade",
   "metadata": {},
   "source": [
    "┣━━ **1.Loading dataset** <br>\n",
    "┃   ┣━━ `torch.data.utils.datasets.AG_NEWS` <br>\n",
    "┣━━ **2.Load Tokenization** <br>\n",
    "┃   ┣━━ `torchtext.data.utils.get_tokenizer('basic_english')` <br>\n",
    "┣━━ **3.Build vocabulary** <br>\n",
    "┃   ┣━━ `torchtext.vocab.build_vocab_from_iterator(train_iterator)` <br>\n",
    "┣━━ **4.Create BoW supporting functions**<br>\n",
    "┃   ┣━━ Convert `text_2_BoW_vector` <br>\n",
    "┃   ┣━━ Create `collate_fn` to create a pair of label-feature tensors for every minibatch <br>\n",
    "┣━━ **5.Create train, validation and test `DataLoaders`**<br>\n",
    "┣━━ **6.Define `Model_Architecture`**<br>\n",
    "┣━━ **7.define `training_loop` and `testing_loop` functions**<br>\n",
    "┣━━ **8.Train the model and Evaluate on Test Data**<br>\n",
    "┣━━ **9.Test the model on sample text**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48576710",
   "metadata": {},
   "source": [
    "Importing basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117285ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3601e",
   "metadata": {},
   "source": [
    "#### 3.1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79c6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "def load_dataset(ngrams=1):\n",
    "    print(\"Loading dataset ...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e00a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36af1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d3fa7",
   "metadata": {},
   "source": [
    "#### 3.2. Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f33ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e84897",
   "metadata": {},
   "source": [
    "#### 3.3. Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88199ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "def _yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def create_vocab(train_dataset):\n",
    "    print(\"Building vocabulary ..\")\n",
    "    vocab = build_vocab_from_iterator(_yield_tokens(train_dataset),\n",
    "                                      min_freq=1,\n",
    "                                      specials=['<unk>']\n",
    "                                     )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd36531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary ..\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36ac45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 95811\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size =\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d93b5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 21, 5, 262, 4229, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['this', 'is', 'a', 'sports', 'article','<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ee8fc",
   "metadata": {},
   "source": [
    "Looking at some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727839c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 World\n",
      "Burgers for the Health Professional Even as obesity and its consequences are increasingly taxing the health care system, fast food places are serving as hospital cafeterias.\n",
      "******\n",
      "4 Sci/Tech\n",
      "Climate Talks Bring Bush #39;s Policy to Fore  Glaciers in the Antarctic and in Greenland are melting much faster than expected, and the fastest moving glacier in the world has doubled its speed.\n",
      "******\n",
      "3 Business\n",
      "Bush Health Savings Accounts Slow to Gain Acceptance So far employers and their workers have been slow to accept health savings accounts as an alternative to conventional health insurance.\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "for label, text in random.sample(train_dataset, 3):\n",
    "    print(label,classes[label-1])\n",
    "    print(text)\n",
    "    print(\"******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae250a76",
   "metadata": {},
   "source": [
    "#### 3.4. Creating BoW related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dddf6a3",
   "metadata": {},
   "source": [
    "- The text pipeline purpose is `to convert text into tokens`\n",
    "- the label pipeline is to have labels from 0 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d58766f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "_label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f468b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 21, 5, 262, 4229]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text_pipeline(\"this is a sports article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6030ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label_pipeline('3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f783b53",
   "metadata": {},
   "source": [
    "In **Bag of Words (BOW)** representation, <br> \n",
    "- each word is linked to a vector index \n",
    "- where the vector value in that index is the frequency of occurrence of the word in the given document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22361911",
   "metadata": {},
   "source": [
    "![](https://github.com/MicrosoftDocs/pytorchfundamentals/blob/main/nlp-pytorch/images/bag-of-words-example.png?raw=true)\n",
    "\n",
    "Source: Microsoft Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb572d",
   "metadata": {},
   "source": [
    "##### 3.4.1 Creating `text_2_bow_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037a6e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text:\n",
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "BoW vector:\n",
      "tensor([0., 2., 1.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def to_bow(text,\n",
    "           bow_vocab_size=vocab_size\n",
    "          ):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in _text_pipeline(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(f\"sample text:\\n{train_dataset[0][1]}\")\n",
    "print(f\"\\nBoW vector:\\n{to_bow(train_dataset[0][1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5bb5cd",
   "metadata": {},
   "source": [
    "##### 3.4.2 Create Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0030e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the collate function\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.tensor([t[0]-1 for t in b],dtype=torch.float32),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f65d0",
   "metadata": {},
   "source": [
    "#### 3.5. Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47bb127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8486520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a300d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=bowify)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=bowify)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=bowify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbea78e",
   "metadata": {},
   "source": [
    "### 3.6. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e071402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BOW_TextClassification(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        # initialize the layers in the __init__ constructor\n",
    "        super(BOW_TextClassification,self).__init__()\n",
    "        # supercharge the sub-class by inheriting the defaults from parent class\n",
    "        self.simple_linear_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(vocab_size,4),\n",
    "            # torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(512,4), # 4 denotes the number of classes\n",
    "            )\n",
    "        \n",
    "    def forward(self,features):\n",
    "        softmax_values = self.simple_linear_stack(features)\n",
    "        return softmax_values\n",
    "\n",
    "bow_model = BOW_TextClassification(vocab_size).to(device)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61a0150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_TextClassification(\n",
      "  (simple_linear_stack): Sequential(\n",
      "    (0): Linear(in_features=95811, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f153b1",
   "metadata": {},
   "source": [
    "### 3.7. Define `train_loop` and `test_loop` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b56b62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "# setting hyperparameters\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(bow_model.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_size = 1 # just for checking how much time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "775cd091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of training batches\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10709d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "161e6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(bow_model, \n",
    "               train_dataloader,\n",
    "               validation_dataloader,\n",
    "               epoch,\n",
    "               lr=lr,\n",
    "               optimizer=optimizer,\n",
    "               loss_fn=loss_fn,\n",
    "              ):\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "    validation_size = len(validation_dataloader.dataset)\n",
    "    training_loss_per_epoch = 0\n",
    "    validation_loss_per_epoch = 0\n",
    "    for batch_number, (labels, features) in enumerate(train_dataloader):\n",
    "        if batch_number %100 == 0:\n",
    "            print(f\"In epoch {epoch}, training of {batch_number} batches are over\")\n",
    "        if batch_number == 100:\n",
    "            break\n",
    "        labels, features = labels.to(device), features.to(device)\n",
    "        labels = labels.clone().detach().requires_grad_(True).long().to(device)\n",
    "        # labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        # compute prediction and prediction error\n",
    "        pred = bow_model(features)\n",
    "        # print(pred.dtype, pred.shape)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        # print(loss.dtype)\n",
    "        \n",
    "        # backpropagation steps\n",
    "        # key optimizer steps\n",
    "        # by default, gradients add up in PyTorch\n",
    "        # we zero out in every iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # performs the gradient computation steps (across the DAG)\n",
    "        loss.backward()\n",
    "        \n",
    "        # adjust the weights\n",
    "        optimizer.step()\n",
    "        training_loss_per_epoch += loss.item()\n",
    "        \n",
    "    for batch_number, (labels, features) in enumerate(validation_dataloader):\n",
    "        if batch_number == 100:\n",
    "            break\n",
    "        labels, features = labels.to(device), features.to(device)\n",
    "        labels = labels.clone().detach().requires_grad_(True).long().to(device)\n",
    "        #labels, features = labels.to(device), features.to(device)\n",
    "        #labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        # compute prediction error\n",
    "        pred = bow_model(features)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        validation_loss_per_epoch += loss.item()\n",
    "    \n",
    "    avg_training_loss = training_loss_per_epoch / train_size\n",
    "    avg_validation_loss = validation_loss_per_epoch / validation_size\n",
    "    print(f\"Average Training Loss of {epoch}: {avg_training_loss}\")\n",
    "    print(f\"Average Validation Loss of {epoch}: {avg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6f09656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(bow_model,test_dataloader, epoch, loss_fn=loss_fn):\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    # Failing to do eval can yield inconsistent inference results\n",
    "    bow_model.eval()\n",
    "    bow_model.to(device)\n",
    "    test_loss_per_epoch, accuracy_per_epoch = 0, 0\n",
    "    # disabling gradient tracking while inference\n",
    "    with torch.no_grad():\n",
    "        for labels, features in test_dataloader:\n",
    "            labels, features = labels.to(device), features.to(device)\n",
    "            labels = labels.clone().detach().requires_grad_(True).long().to(device)\n",
    "            # labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            # labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            pred = bow_model(features)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            test_loss_per_epoch += loss.item()\n",
    "            accuracy_per_epoch += (pred.argmax(1)==labels).type(torch.float).sum().item()\n",
    "    print(f\"Average Test Loss of {epoch}: {test_loss_per_epoch/test_size}\")\n",
    "    print(f\"Average Accuracy of {epoch}: {accuracy_per_epoch/test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef90a47",
   "metadata": {},
   "source": [
    "### 3.8 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d0ac3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7415a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0 \n",
      "---------------------\n",
      "In epoch 0, training of 0 batches are over\n",
      "In epoch 0, training of 100 batches are over\n",
      "Average Training Loss of 0: 0.0004964731066373357\n",
      "Average Validation Loss of 0: 0.008571766301679114\n",
      "Average Test Loss of 0: 0.12454833071194835\n",
      "Average Accuracy of 0: 0.8268421052631579\n",
      "CPU times: user 3h 22min 19s, sys: 13.6 s, total: 3h 22min 32s\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# it takes a lot of time to run this model\n",
    "# hence running only for 100 batches (of size 4) in 1 epoch\n",
    "for epoch in range(epoch_size):\n",
    "    print(f\"Epoch Number: {epoch} \\n---------------------\")\n",
    "    train_loop(bow_model, \n",
    "               train_dataloader, \n",
    "               valid_dataloader,\n",
    "               epoch\n",
    "              )\n",
    "    test_loop(bow_model, \n",
    "              test_dataloader,\n",
    "              epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5765f",
   "metadata": {},
   "source": [
    "### 3.9.Test the model on sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "785671bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model):\n",
    "    with torch.no_grad():\n",
    "        bow_vector = to_bow(text)\n",
    "        output = bow_model(bow_vector)\n",
    "        output_label = ag_news_label[output.argmax().item() + 1]\n",
    "        return output_label\n",
    "    \n",
    "sample_string = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "cpu_model = bow_model.to(\"cpu\")\n",
    "\n",
    "print(f\"This is a {predict(sample_string, model=cpu_model)} news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc089e06",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "- In this blog piece, we looked at how bow vectorizer was used as input to build a shallow NN (without non-linear activation function) classification. \n",
    "- In the next parts to this Pytorch series, I will cover better ways to build a text classification NN model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a8799",
   "metadata": {},
   "source": [
    "Sources <br>\n",
    "\n",
    "- MSFT PyTorch NLP Course | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/)\n",
    "- MSFT PyTorch Course - BoW Classifier | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/3-bow-tfidf)\n",
    "- Torchtext Tutorial on Text Classification | [link](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611c622",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98c3ded5f4c982d767ead9cded27e95b53d0df25404a508cedfb98865b9710c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
