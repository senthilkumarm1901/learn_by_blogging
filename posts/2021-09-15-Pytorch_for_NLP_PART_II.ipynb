{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /pytorch_for_nlp/2021/09/15/Pytorch_for_NLP_PART_II\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- NLP\n",
    "- Coding\n",
    "date: '2021-09-15'\n",
    "description: This blog post explains how to build Linear Text Classifiers using PyTorch's\n",
    "  modules such as `nn.EmbeddingBag` and `nn.Embedding` functions to convert tokenized\n",
    "  text into embeddings\n",
    "hide: false\n",
    "image: images/pytorch_nn/linear_classifier_w_embedding.png\n",
    "output-file: 2021-09-15-pytorch_for_nlp_part_ii.html\n",
    "title: PyTorch Fundamentals for NLP - Part 2\n",
    "toc: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a417bf",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603947d1",
   "metadata": {},
   "source": [
    "In this blog piece, let us cover how we can build a\n",
    "- `text classification` application using an embedding + fc layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196b171",
   "metadata": {},
   "source": [
    "## 2.Representing Text as Tensors - A Quick Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417206e",
   "metadata": {},
   "source": [
    "**How do computers represent text?**\n",
    "- Using encodings such as ASCII values to represent each character\n",
    "\n",
    "![](https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/images/ascii-character-map.png)\n",
    "\n",
    "Source: github.com/MicrosoftDocs/pytorchfundamentals\n",
    "\n",
    "\n",
    "> Still computers cannot `interpret` the meaning of the words , they just `represent` text as ascii numbers in the above image\n",
    "\n",
    "\n",
    "**How is text converted into embeddings?** <br>\n",
    "\n",
    "- Two types of representations to convert text into numbers\n",
    "\n",
    "    - Character-level representation\n",
    "    - Word-level representation\n",
    "    - Token or sub-word level representation\n",
    "    \n",
    "- While Character-level and Word-level representations are self explanatory, Token-level representation is a combination of the above two approaches. \n",
    "\n",
    "<u>Some important terms</u>: <br>\n",
    "\n",
    "- **Tokenization** (sentence/text --> tokens): In the case sub-word level representations, for example, `unfriendly` will be **tokenized** as `un, #friend, #ly` where `#` indicates the token is a continuation of previous token. \n",
    "- This way of tokenization can make the model learnt/trained representations for `friend` and `unfriendly` to be closer to each other in the vector spacy\n",
    "\n",
    "- **Numericalization** (tokens --> numericals): This is the step where we convert tokens into integers.\n",
    "\n",
    "- **Vectorization** (numericals --> vectors): This is the process of creating vectors (typically sparse and equal to the length of the vocabulary of the corpus analyzed)\n",
    "\n",
    "- **Embedding** (numericals --> embeddings): For text data, embedding is a lower dimensional equivalent of a higher dimensional sparse vector. Embeddings are typically dense. Vectors are sparse. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Typical Process of Embedding Creation** <br>\n",
    "- `text_data` >> `tokens` >> `numericals` >> sparse `vectors` or dense `embeddings` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93856cd",
   "metadata": {},
   "source": [
    "## 3. Difference between `nn.EmbeddingBag` vs `nn.Embedding` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cc2c9",
   "metadata": {},
   "source": [
    "- `nn.Embedding`: A simple lookup table that looks up embeddings in a fixed dictionary and size.\n",
    "\n",
    "- `nn.EmbeddingBag`: Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.\n",
    "\n",
    "Source: PyTorch Official Documentation\n",
    "\n",
    "![](https://jamesmccaffrey.files.wordpress.com/2021/03/regular_embedding_vs_embedding_bag_diagram.jpg?w=640&h=394)\n",
    "\n",
    "\n",
    "**`nn.Embedding` Explanation**: \n",
    "- In the above pic, we can see that the encoding of `men write code` being embedded as `[(0.312,0.385), (0.543, 0.481), (0.203, 0.404)]` where `embed_dim=2`. \n",
    "- Looking closer, `men` is embedded as `(0.312,0.385)` and the trailing `<pad>` token is embedded as ` (0.203, 0.404)`\n",
    "\n",
    "**`nn.EmbeddingBag` Explanation**: <br> \n",
    "- In here, there is no padding token. The sentences in a batch are connnected together and saved with their `offsets` array\n",
    "- Instead of each word being represented by an embedding vector, each sentence is embedded into a embedding vector\n",
    "- This above process of \"computing a single vector for an entire sentence\" is possible also from `nn.Embedding` followed by `torch.mean(dim=1)` or `torch.sum(dim=1)` or `torch.max(dim=1)`\n",
    "\n",
    "\n",
    "**So, when to use `nn.EmbeddingBag`?**\n",
    "- nn.EmbeddingBag works better when sequential information of words is not needed. \n",
    "- Hence can be used with simple Feed forward NN and not with LSTMs or Transformers (all the embedded words are sent at once and they are sequentially processed either from both directions or unidirectional)\n",
    "\n",
    "Sources: <br>\n",
    "- `nn.EmbeddingBag` vs `nn.Embedding` | [link](https://jamesmccaffrey.wordpress.com/2021/04/14/explaining-the-pytorch-embeddingbag-layer/)\n",
    "- `nn.Emedding` followed by `torch.mean(dim=1)` | [link](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f681a",
   "metadata": {},
   "source": [
    "## 4. A Text Classification Pipeline using `nn.EmbeddingBag` + `nn.linear` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a55b51",
   "metadata": {},
   "source": [
    "- Dataset considered: **AG_NEWS** dataset that consists of 4 classes - `World, Sports, Business and Sci/Tech`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221fade",
   "metadata": {},
   "source": [
    "┣━━ **1.Loading dataset** <br>\n",
    "┃   ┣━━ `torch.data.utils.datasets.AG_NEWS` <br>\n",
    "┣━━ **2.Load Tokenization** <br>\n",
    "┃   ┣━━ `torchtext.data.utils.get_tokenizer('basic_english')` <br>\n",
    "┣━━ **3.Build vocabulary** <br>\n",
    "┃   ┣━━ `torchtext.vocab.build_vocab_from_iterator(train_iterator)` <br>\n",
    "┣━━ **4.Create `EmbeddingsBag`**<br>\n",
    "┃   ┣━━ Create `collate_fn` to create triplets of label-feature-offsets tensors for every minibatch <br>\n",
    "┣━━ **5.Create train, validation and test `DataLoaders`**<br>\n",
    "┣━━ **6.Define `Model_Architecture`**<br>\n",
    "┣━━ **7.define `training_loop` and `testing_loop` functions**<br>\n",
    "┣━━ **8.Train the model and Evaluate on Test Data**<br>\n",
    "┣━━ **9.Test the model on sample text**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48576710",
   "metadata": {},
   "source": [
    "Importing basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117285ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3601e",
   "metadata": {},
   "source": [
    "#### 4.1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79c6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "def load_dataset(ngrams=1):\n",
    "    print(\"Loading dataset ...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e00a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36af1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d3fa7",
   "metadata": {},
   "source": [
    "#### 4.2. Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f33ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e84897",
   "metadata": {},
   "source": [
    "#### 4.3. Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88199ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "def _yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def create_vocab(train_dataset):\n",
    "    print(\"Building vocabulary ..\")\n",
    "    vocab = build_vocab_from_iterator(_yield_tokens(train_dataset),\n",
    "                                      min_freq=1,\n",
    "                                      specials=['<unk>']\n",
    "                                     )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd36531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary ..\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36ac45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 95811\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size =\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d93b5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 21, 5, 262, 4229, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['this', 'is', 'a', 'sports', 'article','<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ee8fc",
   "metadata": {},
   "source": [
    "Looking at some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727839c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 World\n",
      "EU-25 among least corrupt in global index Corruption is rampant in sixty countries of the world and the public sector continues to be plagued by bribery, says a report by a respected global corruption watchdog.\n",
      "******\n",
      "4 Sci/Tech\n",
      "IDC Raises '04 PC Growth View, Trims '05 (Reuters) Reuters - Shipments of personal computers\\this year will be higher than previously anticipated, boosted\\by the strongest demand from businesses in five years, research\\firm IDC said on Monday.\n",
      "******\n",
      "2 Sports\n",
      "The not-so-great cover-up A crisis, they say, is the best way to test the efficiency of a system. At the Wankhede Stadium, there was a crisis on the first morning when unseasonal showers showed up on the first morning of the final Test.\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "for label, text in random.sample(train_dataset, 3):\n",
    "    print(label,classes[label-1])\n",
    "    print(text)\n",
    "    print(\"******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae250a76",
   "metadata": {},
   "source": [
    "#### 4.4. Creating `EmbeddingsBag` related pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dddf6a3",
   "metadata": {},
   "source": [
    "- The text pipeline purpose is `to convert text into tokens`\n",
    "- the label pipeline is to have labels from 0 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d58766f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "_label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f468b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 21, 5, 262, 4229]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text_pipeline(\"this is a sports article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6030ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label_pipeline('3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f783b53",
   "metadata": {},
   "source": [
    "#### 4.4.1 Exploring arguments for nn.EmbeddingBag\n",
    "`nn.EmbeddingBag()(input_tensor, offsets)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4139b19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0383,  0.0984, -0.4766],\n",
       "        [-0.5284,  0.3360, -0.5838]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "input_tensor = torch.tensor([0, 1, 2, 3, 4, 3, 2, 1], dtype=torch.int64)\n",
    "offsets = torch.tensor([0, 5], dtype=torch.long)\n",
    "embedding_layer = nn.EmbeddingBag(num_embeddings=10,embedding_dim=3,sparse=True)\n",
    "embedding_layer(input_tensor,offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5bb5cd",
   "metadata": {},
   "source": [
    "##### 4.4.2 Create Collate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb40f1",
   "metadata": {},
   "source": [
    "![](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0030e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create collate batch function \n",
    "# to club labels, tokenized_text_converted_into_numbers and token_offsets\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label_pipeline(_label))\n",
    "        processed_text = torch.tensor(_text_pipeline(_text),\n",
    "                                      dtype=torch.int64\n",
    "                                     )\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list_overall = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list_overall = torch.cat(text_list)\n",
    "    offsets_overall = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    return label_list_overall.to(device), text_list_overall.to(device), offsets_overall.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f65d0",
   "metadata": {},
   "source": [
    "#### 4.5. Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1eb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0bd750e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a35b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_, split_valid_ = random_split(train_dataset, \n",
    "                                          [num_train, len(train_dataset) - num_train]\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eddc72ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(split_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28c28301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1565, 113376, 44093, 96738, 56856]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_train_.indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62a38979",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(split_train_,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_batch\n",
    "                             ) \n",
    "\n",
    "valid_dataloader = DataLoader(split_valid_,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_batch\n",
    "                             )\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             collate_fn=collate_batch\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbea78e",
   "metadata": {},
   "source": [
    "### 4.6. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4fd4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class=4):\n",
    "        super(LinearTextClassifier,self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size,\n",
    "                                         embed_dim,\n",
    "                                         sparse=True\n",
    "                                        )\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        # initializing embedding weights as a uniform distribution\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        # initializing linear layer weights as a uniform distribution\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c081a3e",
   "metadata": {},
   "source": [
    "Initializing model and embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c90c2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set([label for (label, text) in train_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25e1e8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "725a2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "\n",
    "# instantiating the class and pass on to device\n",
    "model = LinearTextClassifier(vocab_size,\n",
    "                             embedding_dim,\n",
    "                             num_classes\n",
    "                            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61a0150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearTextClassifier(\n",
      "  (embedding): EmbeddingBag(95811, 64, mode=mean)\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f153b1",
   "metadata": {},
   "source": [
    "### 4.7. Define `train_loop` and `test_loop` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b56b62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "# setting hyperparameters\n",
    "lr = 3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_size = 10 # setting a low number to see time consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "161e6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, \n",
    "               train_dataloader,\n",
    "               validation_dataloader,\n",
    "               epoch,\n",
    "               lr=lr,\n",
    "               optimizer=optimizer,\n",
    "               loss_fn=loss_fn,\n",
    "              ):\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "    validation_size = len(validation_dataloader.dataset)\n",
    "    training_loss_per_epoch = 0\n",
    "    validation_loss_per_epoch = 0\n",
    "    for batch_number, (labels, features, offsets) in enumerate(train_dataloader):\n",
    "        if batch_number %100 == 0:\n",
    "            print(f\"In epoch {epoch}, training of {batch_number} batches are over\")\n",
    "        # following two lines are used while for testing if the fns are accurate\n",
    "        # if batch_number %10 == 0:\n",
    "        #    break\n",
    "        labels, features, offsets = labels.to(device), features.to(device), offsets.to(device)\n",
    "        # labels = labels.clone().detach().requires_grad_(True).long().to(device)        # compute prediction and prediction error\n",
    "        pred = model(features, offsets)\n",
    "        \n",
    "        # print(pred.dtype, pred.shape)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        # print(loss.dtype)\n",
    "        \n",
    "        # backpropagation steps\n",
    "        # key optimizer steps\n",
    "        # by default, gradients add up in PyTorch\n",
    "        # we zero out in every iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # performs the gradient computation steps (across the DAG)\n",
    "        loss.backward()\n",
    "        \n",
    "        # adjust the weights\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        training_loss_per_epoch += loss.item()\n",
    "        \n",
    "    for batch_number, (labels, features, offsets) in enumerate(validation_dataloader):\n",
    "        labels, features, offsets = labels.to(device), features.to(device), offsets.to(device)\n",
    "        # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n",
    "\n",
    "        # compute prediction error\n",
    "        pred = model(features, offsets)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        validation_loss_per_epoch += loss.item()\n",
    "    \n",
    "    avg_training_loss = training_loss_per_epoch / train_size\n",
    "    avg_validation_loss = validation_loss_per_epoch / validation_size\n",
    "    print(f\"Average Training Loss of {epoch}: {avg_training_loss}\")\n",
    "    print(f\"Average Validation Loss of {epoch}: {avg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e6f09656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model,test_dataloader, epoch, loss_fn=loss_fn):\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    # Failing to do eval can yield inconsistent inference results\n",
    "    model.eval()\n",
    "    test_loss_per_epoch, accuracy_per_epoch = 0, 0\n",
    "    # disabling gradient tracking while inference\n",
    "    with torch.no_grad():\n",
    "        for labels, features, offsets in test_dataloader:\n",
    "            labels, features, offsets = labels.to(device), features.to(device), offsets.to(device)\n",
    "            # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n",
    "\n",
    "            # labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            pred = model(features, offsets)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            test_loss_per_epoch += loss.item()\n",
    "            accuracy_per_epoch += (pred.argmax(1)==labels).type(torch.float).sum().item()\n",
    "    # following two lines are used only while testing if the fns are accurate\n",
    "    # print(f\"Last Prediction \\n 1. {pred}, \\n 2.{pred.argmax()}, \\n 3.{pred.argmax(1)}, \\n 4.{pred.argmax(1)==labels}\")\n",
    "    # print(f\"Last predicted label: \\n {labels}\")\n",
    "    print(f\"Average Test Loss of {epoch}: {test_loss_per_epoch/test_size}\")\n",
    "    print(f\"Average Accuracy of {epoch}: {accuracy_per_epoch/test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef90a47",
   "metadata": {},
   "source": [
    "### 4.8 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for 1 epoch, testing for 1 epoch\n",
    "epoch = 1\n",
    "train_loop(model,\n",
    "           train_dataloader, \n",
    "           valid_dataloader,\n",
    "           epoch\n",
    "          )\n",
    "\n",
    "test_loop(model, \n",
    "          test_dataloader,\n",
    "          epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0beaad7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7415a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0 \n",
      "---------------------\n",
      "In epoch 0, training of 0 batches are over\n",
      "In epoch 0, training of 100 batches are over\n",
      "In epoch 0, training of 200 batches are over\n",
      "In epoch 0, training of 300 batches are over\n",
      "In epoch 0, training of 400 batches are over\n",
      "In epoch 0, training of 500 batches are over\n",
      "In epoch 0, training of 600 batches are over\n",
      "In epoch 0, training of 700 batches are over\n",
      "In epoch 0, training of 800 batches are over\n",
      "In epoch 0, training of 900 batches are over\n",
      "In epoch 0, training of 1000 batches are over\n",
      ".....\n",
      "In epoch 4, training of 28000 batches are over\n",
      "In epoch 4, training of 28100 batches are over\n",
      "In epoch 4, training of 28200 batches are over\n",
      "In epoch 4, training of 28300 batches are over\n",
      "In epoch 4, training of 28400 batches are over\n",
      "Average Training Loss of 4: 0.06850743169194017\n",
      "Average Validation Loss of 4: 0.10275975785597817\n",
      "Average Test Loss of 4: 0.10988466973246012\n",
      "Average Accuracy of 4: 0.9142105263157895\n",
      "Epoch Number: 5 \n",
      "---------------------\n",
      "In epoch 5, training of 0 batches are over\n",
      ".....\n",
      "In epoch 9, training of 28200 batches are over\n",
      "In epoch 9, training of 28300 batches are over\n",
      "In epoch 9, training of 28400 batches are over\n",
      "Average Training Loss of 9: 0.05302847929670939\n",
      "Average Validation Loss of 9: 0.12290485648680452\n",
      "Average Test Loss of 9: 0.1306164133289306\n",
      "Average Accuracy of 9: 0.9111842105263158\n",
      "CPU times: user 5min 56s, sys: 25.8 s, total: 6min 22s\n",
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# it takes time to run this model\n",
    "for epoch in range(epoch_size):\n",
    "    print(f\"Epoch Number: {epoch} \\n---------------------\")\n",
    "    train_loop(model, \n",
    "               train_dataloader, \n",
    "               valid_dataloader,\n",
    "               epoch\n",
    "              )\n",
    "    test_loop(model, \n",
    "              test_dataloader,\n",
    "              epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f954f571",
   "metadata": {},
   "source": [
    "### 4.9.Test the model on sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1fa8266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model):\n",
    "    with torch.no_grad():\n",
    "        tokenized_numericalized_vector = torch.tensor(_text_pipeline(text))\n",
    "        offsets = torch.tensor([0])\n",
    "        output = model(tokenized_numericalized_vector, \n",
    "                       offsets)\n",
    "        output_label = ag_news_label[output.argmax(1).item() + 1]\n",
    "        return output_label\n",
    "    \n",
    "sample_string = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "cpu_model = model.to(\"cpu\")\n",
    "\n",
    "print(f\"This is a {predict(sample_string, model=cpu_model)} news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ad093",
   "metadata": {},
   "source": [
    "## 5. A Text Classification Pipeline using `nn.Embedding` + `nn.linear` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4597f3",
   "metadata": {},
   "source": [
    "- Dataset considered: **AG_NEWS** dataset that consists of 4 classes - `World, Sports, Business and Sci/Tech`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25689599",
   "metadata": {},
   "source": [
    "(same architecture as previous one, except the change in step 4)\n",
    "\n",
    "┣━━ **1.Loading dataset** <br>\n",
    "┃   ┣━━ `torch.data.utils.datasets.AG_NEWS` <br>\n",
    "┣━━ **2.Load Tokenization** <br>\n",
    "┃   ┣━━ `torchtext.data.utils.get_tokenizer('basic_english')` <br>\n",
    "┣━━ **3.Build vocabulary** <br>\n",
    "┃   ┣━━ `torchtext.vocab.build_vocab_from_iterator(train_iterator)` <br>\n",
    "┣━━ **4.Create `Embedding`** layer<br>\n",
    "┃   ┣━━ Create `collate_fn` (`padify`) to create pairs of label-feature tensors for every minibatch <br>\n",
    "┣━━ **5.Create train, validation and test `DataLoaders`**<br>\n",
    "┣━━ **6.Define `Model_Architecture`**<br>\n",
    "┣━━ **7.define `training_loop` and `testing_loop` functions**<br>\n",
    "┣━━ **8.Train the model and Evaluate on Test Data**<br>\n",
    "┣━━ **9.Test the model on sample text**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ab909",
   "metadata": {},
   "source": [
    "Importing basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e84656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b2cbe",
   "metadata": {},
   "source": [
    "**5.1. Loading dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a0597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "def load_dataset(ngrams=1):\n",
    "    print(\"Loading dataset ...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d797629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c48ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b2df6",
   "metadata": {},
   "source": [
    "**5.2. Loading Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa821388",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f40a09",
   "metadata": {},
   "source": [
    "**5.3. Building Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7effe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "def _yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def create_vocab(train_dataset):\n",
    "    print(\"Building vocabulary ..\")\n",
    "    vocab = build_vocab_from_iterator(_yield_tokens(train_dataset),\n",
    "                                      min_freq=1,\n",
    "                                      specials=['<unk>']\n",
    "                                     )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf56ba97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary ..\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4302a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 95811\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size =\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20bf6bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 21, 5, 262, 4229, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['this', 'is', 'a', 'sports', 'article','<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f2b17",
   "metadata": {},
   "source": [
    "**5.4. Creating `nn.Embedding` related pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94791ebe",
   "metadata": {},
   "source": [
    "- The text pipeline purpose is `to convert text into tokens`\n",
    "- the label pipeline is to have labels from 0 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b707b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "_label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea383c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 21, 5, 262, 4229]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text_pipeline(\"this is a sports article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794c76fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label_pipeline('3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb480bb",
   "metadata": {},
   "source": [
    "**5.4.1 Exploring arguments for `nn.Embedding`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cfe4fe",
   "metadata": {},
   "source": [
    "- `nn.Embedding`: A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "- Let us create an Embedding module containing 10 tensors of size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5908d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2225,  0.7789, -1.1441]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[1.3428, 1.2356, 0.6745]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[-0.6605, -1.5354, -0.4195]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[-0.9991,  1.7851, -1.6268]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[0.7723, 2.0980, 0.3080]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "embedding = nn.Embedding(5, 3)\n",
    "for i in range(5):\n",
    "    print(embedding(torch.tensor([i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ffbdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3428,  1.2356,  0.6745],\n",
       "         [-0.6605, -1.5354, -0.4195],\n",
       "         [ 0.7723,  2.0980,  0.3080],\n",
       "         [-0.9991,  1.7851, -1.6268]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_array_input = torch.tensor([[1,2,4,3]])\n",
    "embedding(an_array_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7669d3",
   "metadata": {},
   "source": [
    "**5.4.2. Create Collate Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f117f97",
   "metadata": {},
   "source": [
    "**Dealing with Variable Sequence Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efdba3",
   "metadata": {},
   "source": [
    "- Every data point in a text corpus could have different number of tokens\n",
    "- For maintaining uniform number of input tokens in texts, we `padify` the text\n",
    "- `torch.nn.functional.pad` on a tokenized dataset can `padify` the dataset\n",
    "\n",
    "![](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-2.png)\n",
    "Source: Microsoft PyTorch Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7884e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(batch):\n",
    "    # batch is a list of (label, text) pair of tuples\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label_pipeline(_label))\n",
    "        tokenized_numericalized_text = torch.tensor(_text_pipeline(_text),\n",
    "                                                    dtype=torch.int64 \n",
    "                                                   )\n",
    "        text_list.append(tokenized_numericalized_text)\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    max_length = max(map(len,text_list))\n",
    "    label_list_overall = torch.tensor(label_list, \n",
    "                                      dtype=torch.int64\n",
    "                                     )\n",
    "    text_list_overall = torch.stack([torch.nn.functional.pad(torch.tensor(t),\n",
    "                                                             (0,max_length - len(t)),\n",
    "                                                             mode='constant',\n",
    "                                                             value=0) for t in text_list\n",
    "                                    ])\n",
    "    return label_list_overall.to(device), text_list_overall.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efade479",
   "metadata": {},
   "source": [
    "**5.5. Prepare DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9185aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4377e9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13748, 32598, 23674, 26304, 9007]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_train_, split_valid_ = random_split(train_dataset, \n",
    "                                          [num_train, len(train_dataset) - num_train]\n",
    "                                         )\n",
    "split_train_.indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f19086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(split_train_,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=padify\n",
    "                             ) \n",
    "\n",
    "valid_dataloader = DataLoader(split_valid_,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=padify\n",
    "                             )\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             collate_fn=padify\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baa9ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking batch 0\n",
      "torch.Size([4])\n",
      "torch.Size([4, 52])\n",
      "****\n",
      "Tracking batch 1\n",
      "torch.Size([4])\n",
      "torch.Size([4, 62])\n",
      "****\n",
      "Tracking batch 2\n",
      "torch.Size([4])\n",
      "torch.Size([4, 50])\n",
      "****\n",
      "Tracking batch 3\n",
      "torch.Size([4])\n",
      "torch.Size([4, 47])\n"
     ]
    }
   ],
   "source": [
    "for i, (labels, features) in enumerate(test_dataloader):\n",
    "    print(f\"Tracking batch {i}\")\n",
    "    print(labels.shape)\n",
    "    print(features.shape)\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(\"****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9628957",
   "metadata": {},
   "source": [
    "**5.6 Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a22b470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearTextClassifier_2(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class=4):\n",
    "        super(LinearTextClassifier_2,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                         embed_dim,\n",
    "                                     )\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        # initializing embedding weights as a uniform distribution\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        # initializing linear layer weights as a uniform distribution\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f54544",
   "metadata": {},
   "source": [
    "Initializing the model hyperaparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35eb773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set([label for (label, text) in train_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8af0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "\n",
    "# instantiating the class and pass on to device\n",
    "model_2 = LinearTextClassifier_2(vocab_size,\n",
    "                               embedding_dim,\n",
    "                               num_classes\n",
    "                              ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12ddceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearTextClassifier_2(\n",
      "  (embedding): Embedding(95811, 64)\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd1bde02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "# setting hyperparameters\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_size = 3 # setting a low number to see time consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eb061",
   "metadata": {},
   "source": [
    "**5.7. Define train_loop and test_loop functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f91bcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_2(model_2, \n",
    "               train_dataloader,\n",
    "               validation_dataloader,\n",
    "               epoch,\n",
    "               lr=lr,\n",
    "               optimizer=optimizer,\n",
    "               loss_fn=loss_fn,\n",
    "              ):\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "    validation_size = len(validation_dataloader.dataset)\n",
    "    training_loss_per_epoch = 0\n",
    "    validation_loss_per_epoch = 0\n",
    "    for batch_number, (labels, features) in enumerate(train_dataloader):\n",
    "        if batch_number %1000 == 0:\n",
    "            print(f\"In epoch {epoch}, training of {batch_number} batches are over\")\n",
    "        # following two lines are used while for testing if the fns are accurate\n",
    "        # if batch_number %10 == 0:\n",
    "        #    break\n",
    "        labels, features = labels.to(device), features.to(device)\n",
    "        # labels = labels.clone().detach().requires_grad_(True).long().to(device)        # compute prediction and prediction error\n",
    "        pred = model_2(features)\n",
    "        \n",
    "        # print(pred.dtype, pred.shape)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        # print(loss.dtype)\n",
    "        \n",
    "        # backpropagation steps\n",
    "        # key optimizer steps\n",
    "        # by default, gradients add up in PyTorch\n",
    "        # we zero out in every iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # performs the gradient computation steps (across the DAG)\n",
    "        loss.backward()\n",
    "        \n",
    "        # adjust the weights\n",
    "        # torch.nn.utils.clip_grad_norm_(model_2.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        training_loss_per_epoch += loss.item()\n",
    "        \n",
    "    for batch_number, (labels, features) in enumerate(validation_dataloader):\n",
    "        labels, features = labels.to(device), features.to(device)\n",
    "        # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n",
    "\n",
    "        # compute prediction error\n",
    "        pred = model_2(features)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        validation_loss_per_epoch += loss.item()\n",
    "    \n",
    "    avg_training_loss = training_loss_per_epoch / train_size\n",
    "    avg_validation_loss = validation_loss_per_epoch / validation_size\n",
    "    print(f\"Average Training Loss of {epoch}: {avg_training_loss}\")\n",
    "    print(f\"Average Validation Loss of {epoch}: {avg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb1d23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_2(model_2,test_dataloader, epoch, loss_fn=loss_fn):\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    # Failing to do eval can yield inconsistent inference results\n",
    "    model_2.eval()\n",
    "    test_loss_per_epoch, accuracy_per_epoch = 0, 0\n",
    "    # disabling gradient tracking while inference\n",
    "    with torch.no_grad():\n",
    "        for labels, features in test_dataloader:\n",
    "            labels, features = labels.to(device), features.to(device)\n",
    "            # labels = labels.clone().detach().requires_grad_(True).long().to(device)\n",
    "\n",
    "            # labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            pred = model_2(features)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            test_loss_per_epoch += loss.item()\n",
    "            accuracy_per_epoch += (pred.argmax(1)==labels).type(torch.float).sum().item()\n",
    "    # following two lines are used only while testing if the fns are accurate\n",
    "    # print(f\"Last Prediction \\n 1. {pred}, \\n 2.{pred.argmax()}, \\n 3.{pred.argmax(1)}, \\n 4.{pred.argmax(1)==labels}\")\n",
    "    # print(f\"Last predicted label: \\n {labels}\")\n",
    "    print(f\"Average Test Loss of {epoch}: {test_loss_per_epoch/test_size}\")\n",
    "    print(f\"Average Accuracy of {epoch}: {accuracy_per_epoch/test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abda654",
   "metadata": {},
   "source": [
    "**5.8 Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76d8a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 1, training of 0 batches are over\n",
      "In epoch 1, training of 1000 batches are over\n",
      "In epoch 1, training of 2000 batches are over\n",
      "In epoch 1, training of 3000 batches are over\n",
      ".....\n",
      "In epoch 1, training of 26000 batches are over\n",
      "In epoch 1, training of 27000 batches are over\n",
      "In epoch 1, training of 28000 batches are over\n",
      "Average Training Loss of 1: 0.08683817907764081\n",
      "Average Validation Loss of 1: 0.0605684169218495\n",
      "Average Test Loss of 1: 0.06346218769094585\n",
      "Average Accuracy of 1: 0.9201315789473684\n"
     ]
    }
   ],
   "source": [
    "# checking for 1 epoch, testing for 1 epoch\n",
    "epoch = 1\n",
    "train_loop_2(model_2,\n",
    "           train_dataloader, \n",
    "           valid_dataloader,\n",
    "           epoch\n",
    "          )\n",
    "\n",
    "test_loop_2(model_2, \n",
    "          test_dataloader,\n",
    "          epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "# it takes time to run this model\n",
    "for epoch in range(epoch_size):\n",
    "    print(f\"Epoch Number: {epoch} \\n---------------------\")\n",
    "    train_loop_2(model_2, \n",
    "               train_dataloader, \n",
    "               valid_dataloader,\n",
    "               epoch\n",
    "              )\n",
    "    test_loop_2(model_2, \n",
    "              test_dataloader,\n",
    "              epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae12188",
   "metadata": {},
   "source": [
    "**5.9. Test the model on sample text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d767cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "def predict_2(text, model):\n",
    "    batch = [(torch.tensor([0]),\n",
    "              text\n",
    "             )\n",
    "            ]\n",
    "    with torch.no_grad():\n",
    "        _, padded_sequence = padify(batch)\n",
    "        padded_sequence = padded_sequence.to(\"cpu\")\n",
    "        # tokenized_numericalized_vector = torch.tensor(_text_pipeline(text))\n",
    "        output = model_2(padded_sequence)\n",
    "        output_label = ag_news_label[output.argmax(1).item() + 1]\n",
    "        return output_label\n",
    "    \n",
    "sample_string = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "cpu_model_2 = model_2.to(\"cpu\")\n",
    "\n",
    "print(f\"This is a {predict_2(sample_string, model=cpu_model_2)} news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055e758",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "- In this blog piece, we looked at how we can build linear classifiers (without non-linear activation functions) over `nn.EmbeddingBag` and `nn.Embedding` modules\n",
    "- In the `nn.EmbeddingBag` method of embedding creation, we did not create `padding tokens` but have to track `offsets` for every minibatch.\n",
    "- In the `nn.Embedding` method of creating embeddings, we used `torch.nn.functional.pad` function to ensure all text sequences have fixed length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a8799",
   "metadata": {},
   "source": [
    "Sources <br>\n",
    "\n",
    "- MSFT PyTorch NLP Course | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/)\n",
    "- Official PyTorch Tutorial on Text Classification using `nn.EmbeddingBag` | [link](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "- MSFT PyTorch Text Classification using `nn.Embedding` | [link](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/4-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611c622",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98c3ded5f4c982d767ead9cded27e95b53d0df25404a508cedfb98865b9710c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
