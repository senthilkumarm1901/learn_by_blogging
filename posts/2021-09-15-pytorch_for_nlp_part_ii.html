<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Senthil Kumar">
<meta name="dcterms.date" content="2021-09-15">
<meta name="description" content="This blog post explains how to build Linear Text Classifiers using PyTorch’s modules such as nn.EmbeddingBag and nn.Embedding functions to convert tokenized text into embeddings">

<title>PyTorch Fundamentals for NLP - Part 2 – Learn by Blogging</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Learn by Blogging</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/senthilkumarm1901"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/senthilkumarm1901"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">PyTorch Fundamentals for NLP - Part 2</h1>
                  <div>
        <div class="description">
          This blog post explains how to build Linear Text Classifiers using PyTorch’s modules such as <code>nn.EmbeddingBag</code> and <code>nn.Embedding</code> functions to convert tokenized text into embeddings
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">NLP</div>
                <div class="quarto-category">Coding</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Senthil Kumar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 15, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">1. Introduction</a></li>
  <li><a href="#representing-text-as-tensors---a-quick-introduction" id="toc-representing-text-as-tensors---a-quick-introduction" class="nav-link" data-scroll-target="#representing-text-as-tensors---a-quick-introduction">2.Representing Text as Tensors - A Quick Introduction</a></li>
  <li><a href="#difference-between-nn.embeddingbag-vs-nn.embedding" id="toc-difference-between-nn.embeddingbag-vs-nn.embedding" class="nav-link" data-scroll-target="#difference-between-nn.embeddingbag-vs-nn.embedding">3. Difference between <code>nn.EmbeddingBag</code> vs <code>nn.Embedding</code></a></li>
  <li><a href="#a-text-classification-pipeline-using-nn.embeddingbag-nn.linear-layer" id="toc-a-text-classification-pipeline-using-nn.embeddingbag-nn.linear-layer" class="nav-link" data-scroll-target="#a-text-classification-pipeline-using-nn.embeddingbag-nn.linear-layer">4. A Text Classification Pipeline using <code>nn.EmbeddingBag</code> + <code>nn.linear</code> Layer</a>
  <ul class="collapse">
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">4.6. Model Architecture</a></li>
  <li><a href="#define-train_loop-and-test_loop-functions" id="toc-define-train_loop-and-test_loop-functions" class="nav-link" data-scroll-target="#define-train_loop-and-test_loop-functions">4.7. Define <code>train_loop</code> and <code>test_loop</code> functions</a></li>
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model">4.8 Training the Model</a></li>
  <li><a href="#test-the-model-on-sample-text" id="toc-test-the-model-on-sample-text" class="nav-link" data-scroll-target="#test-the-model-on-sample-text">4.9.Test the model on sample text</a></li>
  </ul></li>
  <li><a href="#a-text-classification-pipeline-using-nn.embedding-nn.linear-layer" id="toc-a-text-classification-pipeline-using-nn.embedding-nn.linear-layer" class="nav-link" data-scroll-target="#a-text-classification-pipeline-using-nn.embedding-nn.linear-layer">5. A Text Classification Pipeline using <code>nn.Embedding</code> + <code>nn.linear</code> Layer</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">6. Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>In this blog piece, let us cover how we can build a - <code>text classification</code> application using an embedding + fc layer</p>
</section>
<section id="representing-text-as-tensors---a-quick-introduction" class="level2">
<h2 class="anchored" data-anchor-id="representing-text-as-tensors---a-quick-introduction">2.Representing Text as Tensors - A Quick Introduction</h2>
<p><strong>How do computers represent text?</strong> - Using encodings such as ASCII values to represent each character</p>
<p><img src="https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/images/ascii-character-map.png" class="img-fluid"></p>
<p>Source: github.com/MicrosoftDocs/pytorchfundamentals</p>
<blockquote class="blockquote">
<p>Still computers cannot <code>interpret</code> the meaning of the words , they just <code>represent</code> text as ascii numbers in the above image</p>
</blockquote>
<p><strong>How is text converted into embeddings?</strong> <br></p>
<ul>
<li><p>Two types of representations to convert text into numbers</p>
<ul>
<li>Character-level representation</li>
<li>Word-level representation</li>
<li>Token or sub-word level representation</li>
</ul></li>
<li><p>While Character-level and Word-level representations are self explanatory, Token-level representation is a combination of the above two approaches.</p></li>
</ul>
<p><u>Some important terms</u>: <br></p>
<ul>
<li><p><strong>Tokenization</strong> (sentence/text –&gt; tokens): In the case sub-word level representations, for example, <code>unfriendly</code> will be <strong>tokenized</strong> as <code>un, #friend, #ly</code> where <code>#</code> indicates the token is a continuation of previous token.</p></li>
<li><p>This way of tokenization can make the model learnt/trained representations for <code>friend</code> and <code>unfriendly</code> to be closer to each other in the vector spacy</p></li>
<li><p><strong>Numericalization</strong> (tokens –&gt; numericals): This is the step where we convert tokens into integers.</p></li>
<li><p><strong>Vectorization</strong> (numericals –&gt; vectors): This is the process of creating vectors (typically sparse and equal to the length of the vocabulary of the corpus analyzed)</p></li>
<li><p><strong>Embedding</strong> (numericals –&gt; embeddings): For text data, embedding is a lower dimensional equivalent of a higher dimensional sparse vector. Embeddings are typically dense. Vectors are sparse.</p></li>
</ul>
<p><br></p>
<p><strong>Typical Process of Embedding Creation</strong> <br> - <code>text_data</code> &gt;&gt; <code>tokens</code> &gt;&gt; <code>numericals</code> &gt;&gt; sparse <code>vectors</code> or dense <code>embeddings</code></p>
</section>
<section id="difference-between-nn.embeddingbag-vs-nn.embedding" class="level2">
<h2 class="anchored" data-anchor-id="difference-between-nn.embeddingbag-vs-nn.embedding">3. Difference between <code>nn.EmbeddingBag</code> vs <code>nn.Embedding</code></h2>
<ul>
<li><p><code>nn.Embedding</code>: A simple lookup table that looks up embeddings in a fixed dictionary and size.</p></li>
<li><p><code>nn.EmbeddingBag</code>: Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.</p></li>
</ul>
<p>Source: PyTorch Official Documentation</p>
<p><img src="https://jamesmccaffrey.files.wordpress.com/2021/03/regular_embedding_vs_embedding_bag_diagram.jpg?w=640&amp;h=394" class="img-fluid"></p>
<p><strong><code>nn.Embedding</code> Explanation</strong>: - In the above pic, we can see that the encoding of <code>men write code</code> being embedded as <code>[(0.312,0.385), (0.543, 0.481), (0.203, 0.404)]</code> where <code>embed_dim=2</code>. - Looking closer, <code>men</code> is embedded as <code>(0.312,0.385)</code> and the trailing <code>&lt;pad&gt;</code> token is embedded as <code>(0.203, 0.404)</code></p>
<p><strong><code>nn.EmbeddingBag</code> Explanation</strong>: <br> - In here, there is no padding token. The sentences in a batch are connnected together and saved with their <code>offsets</code> array - Instead of each word being represented by an embedding vector, each sentence is embedded into a embedding vector - This above process of “computing a single vector for an entire sentence” is possible also from <code>nn.Embedding</code> followed by <code>torch.mean(dim=1)</code> or <code>torch.sum(dim=1)</code> or <code>torch.max(dim=1)</code></p>
<p><strong>So, when to use <code>nn.EmbeddingBag</code>?</strong> - nn.EmbeddingBag works better when sequential information of words is not needed. - Hence can be used with simple Feed forward NN and not with LSTMs or Transformers (all the embedded words are sent at once and they are sequentially processed either from both directions or unidirectional)</p>
<p>Sources: <br> - <code>nn.EmbeddingBag</code> vs <code>nn.Embedding</code> | <a href="https://jamesmccaffrey.wordpress.com/2021/04/14/explaining-the-pytorch-embeddingbag-layer/">link</a> - <code>nn.Emedding</code> followed by <code>torch.mean(dim=1)</code> | <a href="https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag">link</a></p>
</section>
<section id="a-text-classification-pipeline-using-nn.embeddingbag-nn.linear-layer" class="level2">
<h2 class="anchored" data-anchor-id="a-text-classification-pipeline-using-nn.embeddingbag-nn.linear-layer">4. A Text Classification Pipeline using <code>nn.EmbeddingBag</code> + <code>nn.linear</code> Layer</h2>
<ul>
<li>Dataset considered: <strong>AG_NEWS</strong> dataset that consists of 4 classes - <code>World, Sports, Business and Sci/Tech</code></li>
</ul>
<p>┣━━ <strong>1.Loading dataset</strong> <br> ┃ ┣━━ <code>torch.data.utils.datasets.AG_NEWS</code> <br> ┣━━ <strong>2.Load Tokenization</strong> <br> ┃ ┣━━ <code>torchtext.data.utils.get_tokenizer('basic_english')</code> <br> ┣━━ <strong>3.Build vocabulary</strong> <br> ┃ ┣━━ <code>torchtext.vocab.build_vocab_from_iterator(train_iterator)</code> <br> ┣━━ <strong>4.Create <code>EmbeddingsBag</code></strong><br> ┃ ┣━━ Create <code>collate_fn</code> to create triplets of label-feature-offsets tensors for every minibatch <br> ┣━━ <strong>5.Create train, validation and test <code>DataLoaders</code></strong><br> ┣━━ <strong>6.Define <code>Model_Architecture</code></strong><br> ┣━━ <strong>7.define <code>training_loop</code> and <code>testing_loop</code> functions</strong><br> ┣━━ <strong>8.Train the model and Evaluate on Test Data</strong><br> ┣━━ <strong>9.Test the model on sample text</strong><br></p>
<p>Importing basic modules</p>
<div id="117285ea" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchtext</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> collections</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> build_vocab_from_iterator</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data.utils <span class="im">import</span> get_tokenizer</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="loading-dataset" class="level4">
<h4 class="anchored" data-anchor-id="loading-dataset">4.1. Loading dataset</h4>
<div id="a79c6912" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_dataset(ngrams<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loading dataset ..."</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    train_dataset, test_dataset <span class="op">=</span> torchtext.datasets.AG_NEWS(root<span class="op">=</span><span class="st">'./data'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> <span class="bu">list</span>(train_dataset)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    test_dataset <span class="op">=</span> <span class="bu">list</span>(test_dataset)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_dataset, test_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="f7e00a58" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> load_dataset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading dataset ...</code></pre>
</div>
</div>
<div id="36af1092" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> [<span class="st">'World'</span>, <span class="st">'Sports'</span>, <span class="st">'Business'</span>, <span class="st">'Sci/Tech'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="loading-tokenizer" class="level4">
<h4 class="anchored" data-anchor-id="loading-tokenizer">4.2. Loading Tokenizer</h4>
<div id="20f33ae8" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> torchtext.data.utils.get_tokenizer(<span class="st">'basic_english'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="building-vocabulary" class="level4">
<h4 class="anchored" data-anchor-id="building-vocabulary">4.3. Building Vocabulary</h4>
<div id="88199ef5" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _yield_tokens(data_iter):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _, text <span class="kw">in</span> data_iter:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> tokenizer(text)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_vocab(train_dataset):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Building vocabulary .."</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> build_vocab_from_iterator(_yield_tokens(train_dataset),</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                                      min_freq<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                                      specials<span class="op">=</span>[<span class="st">'&lt;unk&gt;'</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                                     )</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    vocab.set_default_index(vocab[<span class="st">'&lt;unk&gt;'</span>])</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vocab</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="9fd36531" class="cell" data-scrolled="true" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> create_vocab(train_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Building vocabulary ..</code></pre>
</div>
</div>
<div id="b36ac45d" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocab size ="</span>, vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Vocab size = 95811</code></pre>
</div>
</div>
<div id="3d93b5c5" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>vocab([<span class="st">'this'</span>, <span class="st">'is'</span>, <span class="st">'a'</span>, <span class="st">'sports'</span>, <span class="st">'article'</span>,<span class="st">'&lt;unk&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>[52, 21, 5, 262, 4229, 0]</code></pre>
</div>
</div>
<p>Looking at some sample data</p>
<div id="727839c7" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, text <span class="kw">in</span> random.sample(train_dataset, <span class="dv">3</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(label,classes[label<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(text)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"******"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 World
EU-25 among least corrupt in global index Corruption is rampant in sixty countries of the world and the public sector continues to be plagued by bribery, says a report by a respected global corruption watchdog.
******
4 Sci/Tech
IDC Raises '04 PC Growth View, Trims '05 (Reuters) Reuters - Shipments of personal computers\this year will be higher than previously anticipated, boosted\by the strongest demand from businesses in five years, research\firm IDC said on Monday.
******
2 Sports
The not-so-great cover-up A crisis, they say, is the best way to test the efficiency of a system. At the Wankhede Stadium, there was a crisis on the first morning when unseasonal showers showed up on the first morning of the final Test.
******</code></pre>
</div>
</div>
</section>
<section id="creating-embeddingsbag-related-pipelines" class="level4">
<h4 class="anchored" data-anchor-id="creating-embeddingsbag-related-pipelines">4.4. Creating <code>EmbeddingsBag</code> related pipelines</h4>
<ul>
<li>The text pipeline purpose is <code>to convert text into tokens</code></li>
<li>the label pipeline is to have labels from 0 to 3</li>
</ul>
<div id="d58766f9" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>_text_pipeline <span class="op">=</span> <span class="kw">lambda</span> x: vocab(tokenizer(x))</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>_label_pipeline <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">int</span>(x) <span class="op">-</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="16f468b8" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>_text_pipeline(<span class="st">"this is a sports article"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>[52, 21, 5, 262, 4229]</code></pre>
</div>
</div>
<div id="8c6030ac" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>_label_pipeline(<span class="st">'3'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>2</code></pre>
</div>
</div>
</section>
<section id="exploring-arguments-for-nn.embeddingbag" class="level4">
<h4 class="anchored" data-anchor-id="exploring-arguments-for-nn.embeddingbag">4.4.1 Exploring arguments for nn.EmbeddingBag</h4>
<p><code>nn.EmbeddingBag()(input_tensor, offsets)</code></p>
<div id="4139b19b" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>], dtype<span class="op">=</span>torch.int64)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>offsets <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">5</span>], dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> nn.EmbeddingBag(num_embeddings<span class="op">=</span><span class="dv">10</span>,embedding_dim<span class="op">=</span><span class="dv">3</span>,sparse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>embedding_layer(input_tensor,offsets)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[ 0.0383,  0.0984, -0.4766],
        [-0.5284,  0.3360, -0.5838]], grad_fn=&lt;EmbeddingBagBackward&gt;)</code></pre>
</div>
</div>
<section id="create-collate-function" class="level5">
<h5 class="anchored" data-anchor-id="create-collate-function">4.4.2 Create Collate Function</h5>
<p><img src="https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-4.png" class="img-fluid"></p>
<div id="0030e1dd" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create collate batch function </span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># to club labels, tokenized_text_converted_into_numbers and token_offsets</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collate_batch(batch):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    label_list, text_list, offsets <span class="op">=</span> [], [], [<span class="dv">0</span>]</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (_label, _text) <span class="kw">in</span> batch:</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        label_list.append(_label_pipeline(_label))</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        processed_text <span class="op">=</span> torch.tensor(_text_pipeline(_text),</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>                                      dtype<span class="op">=</span>torch.int64</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>                                     )</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        text_list.append(processed_text)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        offsets.append(processed_text.size(<span class="dv">0</span>))</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    label_list_overall <span class="op">=</span> torch.tensor(label_list, dtype<span class="op">=</span>torch.int64)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    text_list_overall <span class="op">=</span> torch.cat(text_list)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    offsets_overall <span class="op">=</span> torch.tensor(offsets[:<span class="op">-</span><span class="dv">1</span>]).cumsum(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> label_list_overall.to(device), text_list_overall.to(device), offsets_overall.to(device) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="prepare-dataloaders" class="level4">
<h4 class="anchored" data-anchor-id="prepare-dataloaders">4.5. Prepare DataLoaders</h4>
<div id="fe1eb7e9" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataset <span class="im">import</span> random_split</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>num_train <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(train_dataset) <span class="op">*</span> <span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d0bd750e" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>num_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>114000</code></pre>
</div>
</div>
<div id="d8a35b90" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>split_train_, split_valid_ <span class="op">=</span> random_split(train_dataset, </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>                                          [num_train, <span class="bu">len</span>(train_dataset) <span class="op">-</span> num_train]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>                                         )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="eddc72ac" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(split_train_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>torch.utils.data.dataset.Subset</code></pre>
</div>
</div>
<div id="28c28301" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>split_train_.indices[<span class="dv">0</span>:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>[1565, 113376, 44093, 96738, 56856]</code></pre>
</div>
</div>
<div id="62a38979" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(split_train_,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>                              batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                              shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>                              collate_fn<span class="op">=</span>collate_batch</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>                             ) </span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>valid_dataloader <span class="op">=</span> DataLoader(split_valid_,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>                              batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>                              shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>                              collate_fn<span class="op">=</span>collate_batch</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>                             )</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset,</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>                             batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>                             shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>                             collate_fn<span class="op">=</span>collate_batch</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>                            )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture">4.6. Model Architecture</h3>
<div id="b4fd4a75" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearTextClassifier(nn.Module):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, num_class<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(LinearTextClassifier,<span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.EmbeddingBag(vocab_size,</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>                                         embed_dim,</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>                                         sparse<span class="op">=</span><span class="va">True</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>                                        )</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fully connected layer</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(embed_dim, num_class)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_weights()</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init_weights(<span class="va">self</span>):</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        initrange <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initializing embedding weights as a uniform distribution</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initializing linear layer weights as a uniform distribution</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc.bias.data.zero_()</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, text, offsets):</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.embedding(text, offsets)</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc(embedded)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Initializing model and embedding dimension</p>
<div id="c90c2332" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>([label <span class="cf">for</span> (label, text) <span class="kw">in</span> train_dataset]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="25e1e8e4" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>num_classes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>4</code></pre>
</div>
</div>
<div id="725a2e0e" class="cell" data-execution_count="180">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiating the class and pass on to device</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearTextClassifier(vocab_size,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>                             embedding_dim,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>                             num_classes</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>                            ).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="61a0150d" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LinearTextClassifier(
  (embedding): EmbeddingBag(95811, 64, mode=mean)
  (fc): Linear(in_features=64, out_features=4, bias=True)
)</code></pre>
</div>
</div>
</section>
<section id="define-train_loop-and-test_loop-functions" class="level3">
<h3 class="anchored" data-anchor-id="define-train_loop-and-test_loop-functions">4.7. Define <code>train_loop</code> and <code>test_loop</code> functions</h3>
<div id="b56b62b1" class="cell" data-execution_count="181">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># setting hyperparameters</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>epoch_size <span class="op">=</span> <span class="dv">10</span> <span class="co"># setting a low number to see time consumption</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="161e6234" class="cell" data-execution_count="182">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(model, </span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>               train_dataloader,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>               validation_dataloader,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>               epoch,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>               lr<span class="op">=</span>lr,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>               optimizer<span class="op">=</span>optimizer,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>               loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>              ):</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    train_size <span class="op">=</span> <span class="bu">len</span>(train_dataloader.dataset)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    validation_size <span class="op">=</span> <span class="bu">len</span>(validation_dataloader.dataset)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    training_loss_per_epoch <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    validation_loss_per_epoch <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_number, (labels, features, offsets) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch_number <span class="op">%</span><span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"In epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, training of </span><span class="sc">{</span>batch_number<span class="sc">}</span><span class="ss"> batches are over"</span>)</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># following two lines are used while for testing if the fns are accurate</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if batch_number %10 == 0:</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    break</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>        labels, features, offsets <span class="op">=</span> labels.to(device), features.to(device), offsets.to(device)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels = labels.clone().detach().requires_grad_(True).long().to(device)        # compute prediction and prediction error</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(features, offsets)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(pred.dtype, pred.shape)</span></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, labels)</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(loss.dtype)</span></span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backpropagation steps</span></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># key optimizer steps</span></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># by default, gradients add up in PyTorch</span></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we zero out in every iteration</span></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># performs the gradient computation steps (across the DAG)</span></span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># adjust the weights</span></span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="fl">0.1</span>)</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>        training_loss_per_epoch <span class="op">+=</span> loss.item()</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_number, (labels, features, offsets) <span class="kw">in</span> <span class="bu">enumerate</span>(validation_dataloader):</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>        labels, features, offsets <span class="op">=</span> labels.to(device), features.to(device), offsets.to(device)</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels = labels.clone().detach().requires_grad_(True).long().to(device)</span></span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute prediction error</span></span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(features, offsets)</span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, labels)</span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>        validation_loss_per_epoch <span class="op">+=</span> loss.item()</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>    avg_training_loss <span class="op">=</span> training_loss_per_epoch <span class="op">/</span> train_size</span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>    avg_validation_loss <span class="op">=</span> validation_loss_per_epoch <span class="op">/</span> validation_size</span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Training Loss of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>avg_training_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Validation Loss of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>avg_validation_loss<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e6f09656" class="cell" data-execution_count="183">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_loop(model,test_dataloader, epoch, loss_fn<span class="op">=</span>loss_fn):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="bu">len</span>(test_dataloader.dataset)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Failing to do eval can yield inconsistent inference results</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    test_loss_per_epoch, accuracy_per_epoch <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># disabling gradient tracking while inference</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> labels, features, offsets <span class="kw">in</span> test_dataloader:</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>            labels, features, offsets <span class="op">=</span> labels.to(device), features.to(device), offsets.to(device)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># labels = labels.clone().detach().requires_grad_(True).long().to(device)</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># labels = torch.tensor(labels, dtype=torch.float32)</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model(features, offsets)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(pred, labels)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>            test_loss_per_epoch <span class="op">+=</span> loss.item()</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>            accuracy_per_epoch <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>)<span class="op">==</span>labels).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># following two lines are used only while testing if the fns are accurate</span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Last Prediction \n 1. {pred}, \n 2.{pred.argmax()}, \n 3.{pred.argmax(1)}, \n 4.{pred.argmax(1)==labels}")</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Last predicted label: \n {labels}")</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Test Loss of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>test_loss_per_epoch<span class="op">/</span>test_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Accuracy of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>accuracy_per_epoch<span class="op">/</span>test_size<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">4.8 Training the Model</h3>
<div id="027a7ddf" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># checking for 1 epoch, testing for 1 epoch</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>train_loop(model,</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>           train_dataloader, </span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>           valid_dataloader,</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>           epoch</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>          )</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>test_loop(model, </span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>          test_dataloader,</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>          epoch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0beaad7b" class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>epoch_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>10</code></pre>
</div>
</div>
<div id="7415a88b" class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="co"># it takes time to run this model</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epoch_size):</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch Number: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">---------------------"</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    train_loop(model, </span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>               train_dataloader, </span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>               valid_dataloader,</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>               epoch</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>              )</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    test_loop(model, </span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>              test_dataloader,</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>              epoch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch Number: 0 
---------------------
In epoch 0, training of 0 batches are over
In epoch 0, training of 100 batches are over
In epoch 0, training of 200 batches are over
In epoch 0, training of 300 batches are over
In epoch 0, training of 400 batches are over
In epoch 0, training of 500 batches are over
In epoch 0, training of 600 batches are over
In epoch 0, training of 700 batches are over
In epoch 0, training of 800 batches are over
In epoch 0, training of 900 batches are over
In epoch 0, training of 1000 batches are over
.....
In epoch 4, training of 28000 batches are over
In epoch 4, training of 28100 batches are over
In epoch 4, training of 28200 batches are over
In epoch 4, training of 28300 batches are over
In epoch 4, training of 28400 batches are over
Average Training Loss of 4: 0.06850743169194017
Average Validation Loss of 4: 0.10275975785597817
Average Test Loss of 4: 0.10988466973246012
Average Accuracy of 4: 0.9142105263157895
Epoch Number: 5 
---------------------
In epoch 5, training of 0 batches are over
.....
In epoch 9, training of 28200 batches are over
In epoch 9, training of 28300 batches are over
In epoch 9, training of 28400 batches are over
Average Training Loss of 9: 0.05302847929670939
Average Validation Loss of 9: 0.12290485648680452
Average Test Loss of 9: 0.1306164133289306
Average Accuracy of 9: 0.9111842105263158
CPU times: user 5min 56s, sys: 25.8 s, total: 6min 22s
Wall time: 6min 13s</code></pre>
</div>
</div>
</section>
<section id="test-the-model-on-sample-text" class="level3">
<h3 class="anchored" data-anchor-id="test-the-model-on-sample-text">4.9.Test the model on sample text</h3>
<div id="1fa8266e" class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>ag_news_label <span class="op">=</span> {<span class="dv">1</span>: <span class="st">"World"</span>,</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>                 <span class="dv">2</span>: <span class="st">"Sports"</span>,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                 <span class="dv">3</span>: <span class="st">"Business"</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>                 <span class="dv">4</span>: <span class="st">"Sci/Tec"</span>}</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(text, model):</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>        tokenized_numericalized_vector <span class="op">=</span> torch.tensor(_text_pipeline(text))</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        offsets <span class="op">=</span> torch.tensor([<span class="dv">0</span>])</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(tokenized_numericalized_vector, </span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>                       offsets)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        output_label <span class="op">=</span> ag_news_label[output.argmax(<span class="dv">1</span>).item() <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output_label</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>sample_string <span class="op">=</span> <span class="st">"MEMPHIS, Tenn. – Four days ago, Jon Rahm was </span><span class="ch">\</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="st">    enduring the season’s worst weather conditions on Sunday at The </span><span class="ch">\</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="st">    Open on his way to a closing 75 at Royal Portrush, which </span><span class="ch">\</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="st">    considering the wind and the rain was a respectable showing. </span><span class="ch">\</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="st">    Thursday’s first round at the WGC-FedEx St. Jude Invitational </span><span class="ch">\</span></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a><span class="st">    was another story. With temperatures in the mid-80s and hardly any </span><span class="ch">\</span></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a><span class="st">    wind, the Spaniard was 13 strokes better in a flawless round. </span><span class="ch">\</span></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="st">    Thanks to his best putting performance on the PGA Tour, Rahm </span><span class="ch">\</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a><span class="st">    finished with an 8-under 62 for a three-stroke lead, which </span><span class="ch">\</span></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="st">    was even more impressive considering he’d never played the </span><span class="ch">\</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a><span class="st">    front nine at TPC Southwind."</span></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>cpu_model <span class="op">=</span> model.to(<span class="st">"cpu"</span>)</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is a </span><span class="sc">{</span>predict(sample_string, model<span class="op">=</span>cpu_model)<span class="sc">}</span><span class="ss"> news"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>This is a Sports news</code></pre>
</div>
</div>
</section>
</section>
<section id="a-text-classification-pipeline-using-nn.embedding-nn.linear-layer" class="level2">
<h2 class="anchored" data-anchor-id="a-text-classification-pipeline-using-nn.embedding-nn.linear-layer">5. A Text Classification Pipeline using <code>nn.Embedding</code> + <code>nn.linear</code> Layer</h2>
<ul>
<li>Dataset considered: <strong>AG_NEWS</strong> dataset that consists of 4 classes - <code>World, Sports, Business and Sci/Tech</code></li>
</ul>
<p>(same architecture as previous one, except the change in step 4)</p>
<p>┣━━ <strong>1.Loading dataset</strong> <br> ┃ ┣━━ <code>torch.data.utils.datasets.AG_NEWS</code> <br> ┣━━ <strong>2.Load Tokenization</strong> <br> ┃ ┣━━ <code>torchtext.data.utils.get_tokenizer('basic_english')</code> <br> ┣━━ <strong>3.Build vocabulary</strong> <br> ┃ ┣━━ <code>torchtext.vocab.build_vocab_from_iterator(train_iterator)</code> <br> ┣━━ <strong>4.Create <code>Embedding</code></strong> layer<br> ┃ ┣━━ Create <code>collate_fn</code> (<code>padify</code>) to create pairs of label-feature tensors for every minibatch <br> ┣━━ <strong>5.Create train, validation and test <code>DataLoaders</code></strong><br> ┣━━ <strong>6.Define <code>Model_Architecture</code></strong><br> ┣━━ <strong>7.define <code>training_loop</code> and <code>testing_loop</code> functions</strong><br> ┣━━ <strong>8.Train the model and Evaluate on Test Data</strong><br> ┣━━ <strong>9.Test the model on sample text</strong><br></p>
<p>Importing basic modules</p>
<div id="a9e84656" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchtext</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> collections</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> build_vocab_from_iterator</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data.utils <span class="im">import</span> get_tokenizer</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>5.1. Loading dataset</strong></p>
<div id="38a0597f" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_dataset(ngrams<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loading dataset ..."</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    train_dataset, test_dataset <span class="op">=</span> torchtext.datasets.AG_NEWS(root<span class="op">=</span><span class="st">'./data'</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> <span class="bu">list</span>(train_dataset)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    test_dataset <span class="op">=</span> <span class="bu">list</span>(test_dataset)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_dataset, test_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="7d797629" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> load_dataset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading dataset ...</code></pre>
</div>
</div>
<div id="51c48ca9" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> [<span class="st">'World'</span>, <span class="st">'Sports'</span>, <span class="st">'Business'</span>, <span class="st">'Sci/Tech'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>5.2. Loading Tokenizer</strong></p>
<div id="fa821388" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> torchtext.data.utils.get_tokenizer(<span class="st">'basic_english'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>5.3. Building Vocabulary</strong></p>
<div id="f7effe8f" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _yield_tokens(data_iter):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _, text <span class="kw">in</span> data_iter:</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> tokenizer(text)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_vocab(train_dataset):</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Building vocabulary .."</span>)</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> build_vocab_from_iterator(_yield_tokens(train_dataset),</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>                                      min_freq<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>                                      specials<span class="op">=</span>[<span class="st">'&lt;unk&gt;'</span>]</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>                                     )</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    vocab.set_default_index(vocab[<span class="st">'&lt;unk&gt;'</span>])</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vocab</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cf56ba97" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> create_vocab(train_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Building vocabulary ..</code></pre>
</div>
</div>
<div id="5a4302a0" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocab size ="</span>, vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Vocab size = 95811</code></pre>
</div>
</div>
<div id="20bf6bb4" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>vocab([<span class="st">'this'</span>, <span class="st">'is'</span>, <span class="st">'a'</span>, <span class="st">'sports'</span>, <span class="st">'article'</span>,<span class="st">'&lt;unk&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>[52, 21, 5, 262, 4229, 0]</code></pre>
</div>
</div>
<p><strong>5.4. Creating <code>nn.Embedding</code> related pipelines</strong></p>
<ul>
<li>The text pipeline purpose is <code>to convert text into tokens</code></li>
<li>the label pipeline is to have labels from 0 to 3</li>
</ul>
<div id="3b707b3e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>_text_pipeline <span class="op">=</span> <span class="kw">lambda</span> x: vocab(tokenizer(x))</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>_label_pipeline <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">int</span>(x) <span class="op">-</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ea383c64" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>_text_pipeline(<span class="st">"this is a sports article"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>[52, 21, 5, 262, 4229]</code></pre>
</div>
</div>
<div id="794c76fa" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>_label_pipeline(<span class="st">'3'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>2</code></pre>
</div>
</div>
<p><strong>5.4.1 Exploring arguments for <code>nn.Embedding</code></strong></p>
<ul>
<li><code>nn.Embedding</code>: A simple lookup table that stores embeddings of a fixed dictionary and size.</li>
<li>Let us create an Embedding module containing 10 tensors of size 3</li>
</ul>
<div id="5908d9e9" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(<span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(embedding(torch.tensor([i])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 1.2225,  0.7789, -1.1441]], grad_fn=&lt;EmbeddingBackward&gt;)
tensor([[1.3428, 1.2356, 0.6745]], grad_fn=&lt;EmbeddingBackward&gt;)
tensor([[-0.6605, -1.5354, -0.4195]], grad_fn=&lt;EmbeddingBackward&gt;)
tensor([[-0.9991,  1.7851, -1.6268]], grad_fn=&lt;EmbeddingBackward&gt;)
tensor([[0.7723, 2.0980, 0.3080]], grad_fn=&lt;EmbeddingBackward&gt;)</code></pre>
</div>
</div>
<div id="03ffbdd5" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>an_array_input <span class="op">=</span> torch.tensor([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">3</span>]])</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>embedding(an_array_input)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>tensor([[[ 1.3428,  1.2356,  0.6745],
         [-0.6605, -1.5354, -0.4195],
         [ 0.7723,  2.0980,  0.3080],
         [-0.9991,  1.7851, -1.6268]]], grad_fn=&lt;EmbeddingBackward&gt;)</code></pre>
</div>
</div>
<p><strong>5.4.2. Create Collate Function</strong></p>
<p><strong>Dealing with Variable Sequence Size</strong></p>
<ul>
<li>Every data point in a text corpus could have different number of tokens</li>
<li>For maintaining uniform number of input tokens in texts, we <code>padify</code> the text</li>
<li><code>torch.nn.functional.pad</code> on a tokenized dataset can <code>padify</code> the dataset</li>
</ul>
<p><img src="https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/4-embedding-2.png" class="img-fluid"> Source: Microsoft PyTorch Docs</p>
<div id="b7884e7d" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> padify(batch):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># batch is a list of (label, text) pair of tuples</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>    label_list, text_list <span class="op">=</span> [], []</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (_label, _text) <span class="kw">in</span> batch:</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        label_list.append(_label_pipeline(_label))</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        tokenized_numericalized_text <span class="op">=</span> torch.tensor(_text_pipeline(_text),</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>                                                    dtype<span class="op">=</span>torch.int64 </span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>                                                   )</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>        text_list.append(tokenized_numericalized_text)</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute max length of a sequence in this minibatch</span></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    max_length <span class="op">=</span> <span class="bu">max</span>(<span class="bu">map</span>(<span class="bu">len</span>,text_list))</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    label_list_overall <span class="op">=</span> torch.tensor(label_list, </span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>                                      dtype<span class="op">=</span>torch.int64</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>                                     )</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>    text_list_overall <span class="op">=</span> torch.stack([torch.nn.functional.pad(torch.tensor(t),</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>                                                             (<span class="dv">0</span>,max_length <span class="op">-</span> <span class="bu">len</span>(t)),</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>                                                             mode<span class="op">=</span><span class="st">'constant'</span>,</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>                                                             value<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> t <span class="kw">in</span> text_list</span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>                                    ])</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> label_list_overall.to(device), text_list_overall.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>5.5. Prepare DataLoaders</strong></p>
<div id="9185aab8" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataset <span class="im">import</span> random_split</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>num_train <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(train_dataset) <span class="op">*</span> <span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4377e9a1" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>split_train_, split_valid_ <span class="op">=</span> random_split(train_dataset, </span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>                                          [num_train, <span class="bu">len</span>(train_dataset) <span class="op">-</span> num_train]</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>                                         )</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>split_train_.indices[<span class="dv">0</span>:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>[13748, 32598, 23674, 26304, 9007]</code></pre>
</div>
</div>
<div id="3f19086d" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(split_train_,</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>                              batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                              shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>                              collate_fn<span class="op">=</span>padify</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>                             ) </span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>valid_dataloader <span class="op">=</span> DataLoader(split_valid_,</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>                              batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>                              shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>                              collate_fn<span class="op">=</span>padify</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>                             )</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset,</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>                             batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>                             shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>                             collate_fn<span class="op">=</span>padify</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>                            )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="baa9ca7d" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (labels, features) <span class="kw">in</span> <span class="bu">enumerate</span>(test_dataloader):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Tracking batch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(labels.shape)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(features.shape)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"****"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Tracking batch 0
torch.Size([4])
torch.Size([4, 52])
****
Tracking batch 1
torch.Size([4])
torch.Size([4, 62])
****
Tracking batch 2
torch.Size([4])
torch.Size([4, 50])
****
Tracking batch 3
torch.Size([4])
torch.Size([4, 47])</code></pre>
</div>
</div>
<p><strong>5.6 Model Architecture</strong></p>
<div id="a22b470c" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearTextClassifier_2(nn.Module):</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, num_class<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(LinearTextClassifier_2,<span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size,</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>                                         embed_dim,</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>                                     )</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fully connected layer</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(embed_dim, num_class)</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_weights()</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init_weights(<span class="va">self</span>):</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        initrange <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initializing embedding weights as a uniform distribution</span></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initializing linear layer weights as a uniform distribution</span></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc.bias.data.zero_()</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.mean(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Initializing the model hyperaparameters</p>
<div id="35eb773c" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>([label <span class="cf">for</span> (label, text) <span class="kw">in</span> train_dataset]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e8af0f6b" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiating the class and pass on to device</span></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>model_2 <span class="op">=</span> LinearTextClassifier_2(vocab_size,</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>                               embedding_dim,</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>                               num_classes</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>                              ).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="12ddceb8" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LinearTextClassifier_2(
  (embedding): Embedding(95811, 64)
  (fc): Linear(in_features=64, out_features=4, bias=True)
)</code></pre>
</div>
</div>
<div id="dd1bde02" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># setting hyperparameters</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model_2.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>epoch_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># setting a low number to see time consumption</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>5.7. Define train_loop and test_loop functions</strong></p>
<div id="f91bcf90" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop_2(model_2, </span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>               train_dataloader,</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>               validation_dataloader,</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>               epoch,</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>               lr<span class="op">=</span>lr,</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>               optimizer<span class="op">=</span>optimizer,</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>               loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>              ):</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    train_size <span class="op">=</span> <span class="bu">len</span>(train_dataloader.dataset)</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    validation_size <span class="op">=</span> <span class="bu">len</span>(validation_dataloader.dataset)</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>    training_loss_per_epoch <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>    validation_loss_per_epoch <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_number, (labels, features) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch_number <span class="op">%</span><span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"In epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, training of </span><span class="sc">{</span>batch_number<span class="sc">}</span><span class="ss"> batches are over"</span>)</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># following two lines are used while for testing if the fns are accurate</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if batch_number %10 == 0:</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    break</span></span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a>        labels, features <span class="op">=</span> labels.to(device), features.to(device)</span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels = labels.clone().detach().requires_grad_(True).long().to(device)        # compute prediction and prediction error</span></span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model_2(features)</span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(pred.dtype, pred.shape)</span></span>
<span id="cb86-24"><a href="#cb86-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, labels)</span>
<span id="cb86-25"><a href="#cb86-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(loss.dtype)</span></span>
<span id="cb86-26"><a href="#cb86-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-27"><a href="#cb86-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backpropagation steps</span></span>
<span id="cb86-28"><a href="#cb86-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># key optimizer steps</span></span>
<span id="cb86-29"><a href="#cb86-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># by default, gradients add up in PyTorch</span></span>
<span id="cb86-30"><a href="#cb86-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we zero out in every iteration</span></span>
<span id="cb86-31"><a href="#cb86-31" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb86-32"><a href="#cb86-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-33"><a href="#cb86-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># performs the gradient computation steps (across the DAG)</span></span>
<span id="cb86-34"><a href="#cb86-34" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb86-35"><a href="#cb86-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-36"><a href="#cb86-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># adjust the weights</span></span>
<span id="cb86-37"><a href="#cb86-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># torch.nn.utils.clip_grad_norm_(model_2.parameters(), 0.1)</span></span>
<span id="cb86-38"><a href="#cb86-38" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb86-39"><a href="#cb86-39" aria-hidden="true" tabindex="-1"></a>        training_loss_per_epoch <span class="op">+=</span> loss.item()</span>
<span id="cb86-40"><a href="#cb86-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-41"><a href="#cb86-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_number, (labels, features) <span class="kw">in</span> <span class="bu">enumerate</span>(validation_dataloader):</span>
<span id="cb86-42"><a href="#cb86-42" aria-hidden="true" tabindex="-1"></a>        labels, features <span class="op">=</span> labels.to(device), features.to(device)</span>
<span id="cb86-43"><a href="#cb86-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels = labels.clone().detach().requires_grad_(True).long().to(device)</span></span>
<span id="cb86-44"><a href="#cb86-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-45"><a href="#cb86-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute prediction error</span></span>
<span id="cb86-46"><a href="#cb86-46" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model_2(features)</span>
<span id="cb86-47"><a href="#cb86-47" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, labels)</span>
<span id="cb86-48"><a href="#cb86-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-49"><a href="#cb86-49" aria-hidden="true" tabindex="-1"></a>        validation_loss_per_epoch <span class="op">+=</span> loss.item()</span>
<span id="cb86-50"><a href="#cb86-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb86-51"><a href="#cb86-51" aria-hidden="true" tabindex="-1"></a>    avg_training_loss <span class="op">=</span> training_loss_per_epoch <span class="op">/</span> train_size</span>
<span id="cb86-52"><a href="#cb86-52" aria-hidden="true" tabindex="-1"></a>    avg_validation_loss <span class="op">=</span> validation_loss_per_epoch <span class="op">/</span> validation_size</span>
<span id="cb86-53"><a href="#cb86-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Training Loss of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>avg_training_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb86-54"><a href="#cb86-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Validation Loss of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>avg_validation_loss<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fb1d23b1" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_loop_2(model_2,test_dataloader, epoch, loss_fn<span class="op">=</span>loss_fn):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="bu">len</span>(test_dataloader.dataset)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Failing to do eval can yield inconsistent inference results</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    model_2.<span class="bu">eval</span>()</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    test_loss_per_epoch, accuracy_per_epoch <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># disabling gradient tracking while inference</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> labels, features <span class="kw">in</span> test_dataloader:</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>            labels, features <span class="op">=</span> labels.to(device), features.to(device)</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># labels = labels.clone().detach().requires_grad_(True).long().to(device)</span></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># labels = torch.tensor(labels, dtype=torch.float32)</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model_2(features)</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(pred, labels)</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>            test_loss_per_epoch <span class="op">+=</span> loss.item()</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>            accuracy_per_epoch <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>)<span class="op">==</span>labels).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># following two lines are used only while testing if the fns are accurate</span></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Last Prediction \n 1. {pred}, \n 2.{pred.argmax()}, \n 3.{pred.argmax(1)}, \n 4.{pred.argmax(1)==labels}")</span></span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Last predicted label: \n {labels}")</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Test Loss of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>test_loss_per_epoch<span class="op">/</span>test_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average Accuracy of </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>accuracy_per_epoch<span class="op">/</span>test_size<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>5.8 Training the model</strong></p>
<div id="76d8a4bd" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># checking for 1 epoch, testing for 1 epoch</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>train_loop_2(model_2,</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>           train_dataloader, </span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>           valid_dataloader,</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>           epoch</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>          )</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>test_loop_2(model_2, </span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>          test_dataloader,</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>          epoch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>In epoch 1, training of 0 batches are over
In epoch 1, training of 1000 batches are over
In epoch 1, training of 2000 batches are over
In epoch 1, training of 3000 batches are over
.....
In epoch 1, training of 26000 batches are over
In epoch 1, training of 27000 batches are over
In epoch 1, training of 28000 batches are over
Average Training Loss of 1: 0.08683817907764081
Average Validation Loss of 1: 0.0605684169218495
Average Test Loss of 1: 0.06346218769094585
Average Accuracy of 1: 0.9201315789473684</code></pre>
</div>
</div>
<div id="1fe7a8ed" class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># it takes time to run this model</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epoch_size):</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch Number: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">---------------------"</span>)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>    train_loop_2(model_2, </span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>               train_dataloader, </span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>               valid_dataloader,</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>               epoch</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>              )</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>    test_loop_2(model_2, </span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>              test_dataloader,</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>              epoch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>5.9. Test the model on sample text</strong></p>
<div id="2d767cd3" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>ag_news_label <span class="op">=</span> {<span class="dv">1</span>: <span class="st">"World"</span>,</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>                 <span class="dv">2</span>: <span class="st">"Sports"</span>,</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>                 <span class="dv">3</span>: <span class="st">"Business"</span>,</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>                 <span class="dv">4</span>: <span class="st">"Sci/Tec"</span>}</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_2(text, model):</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> [(torch.tensor([<span class="dv">0</span>]),</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>              text</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>             )</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>        _, padded_sequence <span class="op">=</span> padify(batch)</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>        padded_sequence <span class="op">=</span> padded_sequence.to(<span class="st">"cpu"</span>)</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tokenized_numericalized_vector = torch.tensor(_text_pipeline(text))</span></span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model_2(padded_sequence)</span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>        output_label <span class="op">=</span> ag_news_label[output.argmax(<span class="dv">1</span>).item() <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output_label</span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a>sample_string <span class="op">=</span> <span class="st">"MEMPHIS, Tenn. – Four days ago, Jon Rahm was </span><span class="ch">\</span></span>
<span id="cb92-21"><a href="#cb92-21" aria-hidden="true" tabindex="-1"></a><span class="st">    enduring the season’s worst weather conditions on Sunday at The </span><span class="ch">\</span></span>
<span id="cb92-22"><a href="#cb92-22" aria-hidden="true" tabindex="-1"></a><span class="st">    Open on his way to a closing 75 at Royal Portrush, which </span><span class="ch">\</span></span>
<span id="cb92-23"><a href="#cb92-23" aria-hidden="true" tabindex="-1"></a><span class="st">    considering the wind and the rain was a respectable showing. </span><span class="ch">\</span></span>
<span id="cb92-24"><a href="#cb92-24" aria-hidden="true" tabindex="-1"></a><span class="st">    Thursday’s first round at the WGC-FedEx St. Jude Invitational </span><span class="ch">\</span></span>
<span id="cb92-25"><a href="#cb92-25" aria-hidden="true" tabindex="-1"></a><span class="st">    was another story. With temperatures in the mid-80s and hardly any </span><span class="ch">\</span></span>
<span id="cb92-26"><a href="#cb92-26" aria-hidden="true" tabindex="-1"></a><span class="st">    wind, the Spaniard was 13 strokes better in a flawless round. </span><span class="ch">\</span></span>
<span id="cb92-27"><a href="#cb92-27" aria-hidden="true" tabindex="-1"></a><span class="st">    Thanks to his best putting performance on the PGA Tour, Rahm </span><span class="ch">\</span></span>
<span id="cb92-28"><a href="#cb92-28" aria-hidden="true" tabindex="-1"></a><span class="st">    finished with an 8-under 62 for a three-stroke lead, which </span><span class="ch">\</span></span>
<span id="cb92-29"><a href="#cb92-29" aria-hidden="true" tabindex="-1"></a><span class="st">    was even more impressive considering he’d never played the </span><span class="ch">\</span></span>
<span id="cb92-30"><a href="#cb92-30" aria-hidden="true" tabindex="-1"></a><span class="st">    front nine at TPC Southwind."</span></span>
<span id="cb92-31"><a href="#cb92-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-32"><a href="#cb92-32" aria-hidden="true" tabindex="-1"></a>cpu_model_2 <span class="op">=</span> model_2.to(<span class="st">"cpu"</span>)</span>
<span id="cb92-33"><a href="#cb92-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-34"><a href="#cb92-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is a </span><span class="sc">{</span>predict_2(sample_string, model<span class="op">=</span>cpu_model_2)<span class="sc">}</span><span class="ss"> news"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>This is a Sports news</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).</code></pre>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">6. Conclusion</h2>
<ul>
<li>In this blog piece, we looked at how we can build linear classifiers (without non-linear activation functions) over <code>nn.EmbeddingBag</code> and <code>nn.Embedding</code> modules</li>
<li>In the <code>nn.EmbeddingBag</code> method of embedding creation, we did not create <code>padding tokens</code> but have to track <code>offsets</code> for every minibatch.</li>
<li>In the <code>nn.Embedding</code> method of creating embeddings, we used <code>torch.nn.functional.pad</code> function to ensure all text sequences have fixed length</li>
</ul>
<p>Sources <br></p>
<ul>
<li>MSFT PyTorch NLP Course | <a href="https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/">link</a></li>
<li>Official PyTorch Tutorial on Text Classification using <code>nn.EmbeddingBag</code> | <a href="https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html">link</a></li>
<li>MSFT PyTorch Text Classification using <code>nn.Embedding</code> | <a href="https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/4-embeddings">link</a></li>
</ul>
<hr>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("git@github\.com-learning:senthilkumarm1901\/learn_by_blogging\.git");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="senthilkumarm1901/QuartoBlogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>