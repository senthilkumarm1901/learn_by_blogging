{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /Transfer-Learning-in-NLP/2022/04/11/Evolution_of_TL_in_NLP\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- NLP\n",
    "- Transformers\n",
    "date: '2022-04-11'\n",
    "description: An overview covering important NLP milestones from pre-transfer learning\n",
    "  era until the era of BERT (before GPT)\n",
    "hide: false\n",
    "image: images/transfer_learning_evolution/Transfer_learning_evolution_pic_1.png\n",
    "output-file: 2022-04-11-evolution_of_tl_in_nlp.html\n",
    "title: The Evolution of Transfer Learning until the Advent of the First Tranformers in NLP\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60cc26a",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae31a0f",
   "metadata": {},
   "source": [
    "Want to read this blog in a easier to digest slides format? Refer [Link](https://senthilkumarm1901.github.io/UnderstandingTransferLearning4NLP/docs/Evolution_of_TL_in_NLP.html#/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750e52a",
   "metadata": {},
   "source": [
    "## 1. Quick Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d44bb8",
   "metadata": {},
   "source": [
    "### What is powering the emergence of better models in NLP?\n",
    "- Better representation of the text data (with no supervision) by grasping the `context` better\n",
    "     - From `Word2Vec` to `BERT` and beyond, this is the underlying logic!\n",
    "\n",
    "<hr>\n",
    "\n",
    "### How are better text representations produced?\n",
    "- Better contextual representation of words using Transfer Learning\n",
    "\n",
    "<hr>\n",
    "\n",
    "### What is Transfer Learning?\n",
    "![](https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_1.png)\n",
    "Source: ULMFiT Paper | [link](https://humboldt-wi.github.io/blog/research/information_systems_1819/group4_ulmfit/)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### What forms the crux of Transfer Learning models in NLP?\n",
    "- Language Models! <br> \n",
    "    - (1) Build a **Language Model** that understands the underlying features of the text \n",
    "    - (2) Fine-tune the Language Model with additional layers for downstream tasks\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Why Language Model? \n",
    "> *Language modeling can be seen as the ideal source task as it captures many facets of language relevant for downstream tasks, such as long-term dependencies, hierarchical relations and sentiment* <br>\n",
    "> \n",
    ">Ruder et al in the ULMFiT paper\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Ok, What is a Language Model?\n",
    "- A language model (LM) is a model that generates a probability distribution over sequences of words\n",
    "- In simpler words, LM is a model to predict the next word in a sequence of words\n",
    "- It is `unsupervised` or `self-supervised` (since we already know what is the next word in the corpus)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### What are those Language Models?\n",
    "- Examples of Language Models: Word2Vec, Glove, ELMo, ULMFiT, BERT, and many more \n",
    "\n",
    "![](https://ruder.io/content/images/2021/02/pretraining_finetuning.png)\n",
    "Source: A article by Sebastian Ruder: `State of Transfer Learning in NLP` | [Link](https://ruder.io/state-of-transfer-learning-in-nlp/)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### What are the two types of Transfer Learning built using the LMs?\n",
    "\n",
    "- Type 1: `Feature Extraction`\n",
    "    - Example: `Universal Sentence Encoder` produces just an embedding/numerical representation and that gets used by a downstream application\n",
    "    ![](https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-classification.png)\n",
    "    \n",
    "Source of image: TF Hub Article on Universal Sentence Encoder | [Link](https://www.tensorflow.org/hub/tutorials/text_cookbook)\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Type 2: `Fine Tuning`\n",
    "    - E.g.: `BERT` Fine-tuned for Text Classification\n",
    "    ![](https://www.researchgate.net/publication/351386823/figure/fig4/AS:1024183752478725@1621195843655/BERT-Fine-tuning-pipeline-for-a-sample-sentiment-identification-task.jpg)\n",
    "\n",
    "Source of image: An article in Research Gate | [Link](https://www.researchgate.net/figure/BERT-Fine-tuning-pipeline-for-a-sample-sentiment-identification-task_fig4_351386823)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a56309",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58532845",
   "metadata": {},
   "source": [
    "## 2. Types of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba87ea2",
   "metadata": {},
   "source": [
    "### 2A. Count-based Language Models\n",
    "\n",
    "![](images/transfer_learning_evolution/Count_based_LMs.png)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 2B. Context-prediction based `Pre-trained` Language Models\n",
    "\n",
    "![](images/transfer_learning_evolution/context_prediction_based_language_models.png)\n",
    "\n",
    "\n",
    "Sources: <br>\n",
    "- Advanced NLP and Deep Learning course on Udemy (by LazyProgrammer)\n",
    "- Idea: http://www.marekrei.com/blog/dont-count-predict/\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 2C. LSTM-based `Pre-trained` Language Models \n",
    "\n",
    "#### Evolution of RNN Architecture till LSTM\n",
    "\n",
    "**Why RNNs came into existence?** <br>\n",
    "- Models such as the Multi-layer Perceptron Network, vector machines and logistic regression did not perform well on sequence modelling tasks (e.g.: text_sequence2sentiment_classification) \n",
    "- Why? Lack of memory element ; No information retention\n",
    "- RNNs attempted to redress this shortcoming by introducing loops within the network, thus allowing the retention of information.\n",
    "\n",
    "![An un-rolled RNN Cell](images/transfer_learning_evolution/an_unrolled_RNN.png)\n",
    "\n",
    "**Advantage of a vanilla RNN**: <br>\n",
    "- Better than traditional ML algos in retaining information\n",
    "\n",
    "**Limitations of a vanilla RNN**: <br>\n",
    "- RNNs fail to model long term dependencies.\n",
    "- the information was often **\"forgotten\"** after the unit activations were multiplied several times by small numbers\n",
    "- Vanishing gradient and exploding gradient problems\n",
    "\n",
    "**Long Short Term Memory (LSTM)**: <br>\n",
    "- A special type of RNN architecture\n",
    "- Designed to keep information retained for extended number of timesteps\n",
    "\n",
    "**Advantage of a LSTM**: <br>\n",
    "- Better equipped for long range dependencies\n",
    "- Resists better than RNNs for vanishing gradient problem\n",
    "\n",
    "**Limitations of LSTM**: <br>\n",
    "- Added gates lead to more computation requirement and LSTMs tend to be slower\n",
    "- Difficult to train\n",
    "- Transfer Learning never really worked\n",
    "- Very long gradient paths. LSTM on 100-word doc has gradients 100-layer network\n",
    "            \n",
    "<br>            \n",
    "\n",
    "#### Seq2Seq Models - A higher form of LMs\n",
    "\n",
    "![](images/transfer_learning_evolution/Seq2Seq_Model_A_Higher_form_of_LM.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### The `ImageNet` moment in NLP; advent of LSTM models `ULMFiT` and `ELMo`\n",
    "\n",
    "- ELMo comes up with better `word representations/embeddings` using Language Models that learn the `context` of the word in focus\n",
    "![](https://ahmedhanibrahim.files.wordpress.com/2019/07/52861-1pb5hxsxogjrnda_si4nj9q.png?w=775)\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 2D. Transformer-based `Pre-trained` Language Models\n",
    "#### LSTM Seq2Seq Model with Attention\n",
    "\n",
    "![](images/transfer_learning_evolution/seq2seq_model_with_attention.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Transformer - A Seq2Seq Model with Attention \n",
    "\n",
    "**Transformer**: \n",
    "- It is a sequence model forgoes the recurrent structure of RNN to adopt attention-based approach\n",
    "- In other words, Transformer is an attention Model without Recurrent LSTMs\n",
    "\n",
    "**Transformer** vs **LSTM**\n",
    "- Recurrent Structure: Processes all input elements SEQUENTIALLY\n",
    "- Attention-based Approach: Process all input elements SIMULTANEOUSLY\n",
    "\n",
    "<hr>\n",
    "\n",
    "**The BERT Mountain by Chris McCormick**: <br>\n",
    "\n",
    "![](http://www.mccormickml.com/assets/BERT/BERT_Mountain.png)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Transformer for a Seq2Seq Task like Machine Translation**: <br>\n",
    "\n",
    "![](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### The Advent of BERT and similar Transformers\n",
    "\n",
    "##### What has been the trend of recent Pre-trained Tranformer-based LMs?\n",
    "\n",
    "- Exponentially increasing model complexity (number of parameters)\n",
    "\n",
    "![](https://4.bp.blogspot.com/-v0xrp7eJRfM/Xr77DD85ObI/AAAAAAAADDY/KjIlWlFZExQA84VRDrMEMrB534euKAzlgCLcBGAsYHQ/s1600/NLP%2Bmodels.png)\n",
    "\n",
    "- Exponentially increasing data\n",
    "\n",
    "![](images/transfer_learning_evolution/different_transformer_models_and_their_corpora.png)\n",
    "\n",
    "**Question to ponder**: \n",
    "- Are models bettering in performance because of more data or more model complexity? How much is the contribution from each? \n",
    "- Are models built with efficiency in mind? (not a lot can replicate these models given the large number of GPUs necessary)\n",
    "\n",
    "<br>\n",
    "\n",
    "##### What direction should future Pre-trained Transformer-based LMs go?\n",
    "\n",
    "- Computational Compexity is quadratic compared to input length. We curb input length to 512 tokens for most transformer models.\n",
    "> Better model architectures are needed to capture long-range information\n",
    "- As models become bigger and complex, their explainability becomes difficult\n",
    "- There are models/methods/explaining the workings of attention mechanism but much more is needed in this space\n",
    "> Need more efficient models with explainability in mind as well\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e2f3a",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "\n",
    "### In summary, how has transfer learning evolved in NLP?\n",
    "\n",
    "- Step -2: NLP started with rule-based and statistical methodologies\n",
    "- Step -1: ML algorithms such as Naive Bayes, SVM, LR and Trees were fed with bag-of-words word representations \n",
    "- Step 0: Initial Success of better representations using pre-trained LMs like Word2Vec which were built using shallow Neural Network\n",
    "- Step 1: (Re)Emergence of RNN Architectures in NLP\n",
    "- Step 2: Evolution of Sequence-to-Sequence Models built with RNN architectures from Language Models | [source](https://indicodata.ai/blog/sequence-modeling-neuralnets-part1/)\n",
    "- Step 3: `ImageNet` moment in NLP called upon by the first pre-Transformer era Transfer Learning models - ULMFiT and ELMo\n",
    "- Step 4: Cometh the hour, cometh the Transformers ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b1931",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842bc53",
   "metadata": {},
   "source": [
    "## 4. Want to try Transfer Learning hands-on? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba198d3",
   "metadata": {},
   "source": [
    "**Example notebooks for `Text Classification` Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad7e3b",
   "metadata": {},
   "source": [
    "Jay Alamar's Post: DistilBERT for `Feature Extraction` + Logitic Regression for classification | [Link](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n",
    "\n",
    "![DistilBERT Sentiment Classifier](https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7bf11",
   "metadata": {},
   "source": [
    "Jay Alamar's Post: BERT Fine-tuned for Classification | [Picture_Link](https://raw.githubusercontent.com/jalammar/jalammar.github.io/master/images/bert-classifier.png) | [HuggingFace Example Fine-tuning Notebook](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb![image.png](attachment:image.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8ba2a",
   "metadata": {},
   "source": [
    "References:\n",
    "- A suevey paper on `Evolution of Transfer Learning in Natural Language Processing` | [Link](https://arxiv.org/pdf/1910.07370.pdf) <br>\n",
    "- A survey paper on `Pre-trained Models for NLP` | [Link](https://arxiv.org/pdf/2003.08271.pdf) <br>\n",
    "- `The State of Transfer Learning in NLP` | Article by Sebastian Ruder | [Link](https://ruder.io/state-of-transfer-learning-in-nlp/) <br>\n",
    "- `NLP's ImageNet Moment has arrived` | Article by Sebastian Ruder | [Link](https://ruder.io/nlp-imagenet/) <br>\n",
    "- `Recent Advances in LMs` | Article by Sebastian | [Link](https://ruder.io/recent-advances-lm-fine-tuning/) <br>\n",
    "- `Sequence Modeling with Neural Networks` <br>\n",
    "     - [Part 1](https://indicodata.ai/blog/sequence-modeling-neuralnets-part1/): `Evolution of Seq2Seq Models from Language Models` <br>\n",
    "     - [Part 2](https://indicodata.ai/blog/sequence-modeling-neural-networks-part2-attention-models/): `Seq2Seq with Attention`\n",
    "- `LSTM is dead. Long Live Transformers`  | [YouTube Video by Leo Dirac](https://www.youtube.com/watch?v=S27pHKBEp30&t=1073s) |  [Presentation on the same title](https://rbouadjenek.github.io/papers/transformers_Reda@Deakin.pdf) <br>\n",
    "- `The Future of NLP` video and slides by Thomas Wolf, HugginngFace Co-Founder | [YouTube Video](https://www.youtube.com/watch?v=G5lmya6eKtc) <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98c3ded5f4c982d767ead9cded27e95b53d0df25404a508cedfb98865b9710c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
