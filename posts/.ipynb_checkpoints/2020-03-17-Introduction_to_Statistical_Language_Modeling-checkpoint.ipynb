{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4a4f7ab9",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /Markov_Langugage_Model/Log_Probability/Perplexity/2020/03/17/Introduction_to_Statistical_Language_Modeling\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- Statistics\n",
    "date: '2020-03-17'\n",
    "description: A gentle introduction to understand the ABCs of NLP in the era of Transformer\n",
    "  LMs generating poems.\n",
    "hide: false\n",
    "image: images/language_model_intro/introduction_to_statistical_language_model.png\n",
    "output-file: 2020-03-17-introduction_to_statistical_language_modeling.html\n",
    "title: Introduction to Statistical Language Modeling\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99416c8",
   "metadata": {},
   "source": [
    "## 1. What is a Language Model?\n",
    "- A model that generates a probability distribution over sequences of words\n",
    "- In simpler words, LM is a model to predict the next word in a sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbdf48",
   "metadata": {},
   "source": [
    "## 2. Applications of LMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf6a68",
   "metadata": {},
   "source": [
    "- Predicting upcoming words or estimating probability of a phrase or sentence is useful in noisy, ambiguous text data sources \n",
    "    - Speech Recognition: E.g.: P('recognize speech') >> P('wreck a nice beach')\n",
    "    - Spelling Correction: E.g.: P('I have a gub') << P('I have a gun')\n",
    "    - Machine Translation: E.g.: P('strong winds') > P('large winds')\n",
    "    - Optical Character Recognition/ Handwriting Recognition\n",
    "    - Autoreply Suggestions\n",
    "    - Text Classification (discussed with python implementation of a simple N-gram model)\n",
    "    - Text Generation (discussed this with Char-level and Word-level language models)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960cdce",
   "metadata": {},
   "source": [
    "## 3. N-gram Language Modelling\n",
    "\n",
    "\n",
    "**Sample Corpus**: <br>\n",
    "> This is **the house** that Jack built.<br>\n",
    "> This is **the** malt<br>\n",
    "> That lay in **the house** that Jack built.<br>\n",
    "> This is **the** rat,<br>\n",
    "> That ate **the** malt<br>\n",
    "> That lay in **the house** that Jack built.<br>\n",
    "> This is **the** cat,<br>\n",
    "> That killed **the** rat,<br>\n",
    "> That ate **the** malt<br>\n",
    "> That lay in **the house** that Jack built.<br>\n",
    "\n",
    "Source: \"*The house that jack built*\" Nursery Rhyme\n",
    "\n",
    "What is the probability of **house** occurring given **the** before it?\n",
    "\n",
    "Intuitively, we count,\n",
    "\n",
    "$$ Prob \\ ({ house \\over the }) = { Count \\ ( the \\ house ) \\over Count \\ (the) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70abcf5a",
   "metadata": {},
   "source": [
    "Mathematically, the above formula can be writen as $$ P({ B \\over A }) = { P( A, \\ B ) \\over \\ P(B) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591836e",
   "metadata": {},
   "source": [
    "What we have computed above is a **Bigram Language Model**:\n",
    "\n",
    "$$ Bigram\\:Model : { p\\,( W_t|W_{t-1} ) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68dd6b",
   "metadata": {},
   "source": [
    "**How do ngrams help us calculate the prob of a sentence?**:\n",
    "> Sentence, S = 'A B'<br>\n",
    "> We know that, probability of this sentence 'A B' as,<br>\n",
    "> P (S) = Prob (A before B) = P(A , B) = Joint Probability of A and B = **P(B|A)* P(A)**<br>\n",
    ">\n",
    "> Let us assume a three word sentence, S = 'A B C'<br>\n",
    "> P(S) = Prob (A before B before C ) = P (A , B , C)<br>\n",
    ">      = P(C | A , B) * P (A n B)<br>\n",
    ">      = P (C| A , B) * P(B | A) * P (A)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b61d6",
   "metadata": {},
   "source": [
    "Even if there are more words, we could keep applying Conditional Probability and compute the prob of a sentence.\n",
    "The above rule is called **`Chain Rule of Probability`**\n",
    "$$ Prob \\ ({ A_n,\\ A_{n-1},\\ ....,A_1}) = Prob\\ ( {A_n \\ |\\ {A_{n-1},\\ ....,A_1}} ) \\  \\times \\  Prob\\ (A_{n-1},\\ ....,A_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0cc1e",
   "metadata": {},
   "source": [
    "With four vairables the chain rule of probability is: \n",
    "$$ Prob \\ ({ A_4,\\ A_3,\\ A_2,\\ A_1}) = Prob\\ ( {A_4 \\ |\\ {A_{3},\\ A_2,\\ A_1}} ) \\  \\times \\  Prob\\ ( {A_3 \\ |\\ {A_{2},\\ A_1}})\\ \\times \\ Prob\\ ({A_2 \\ |\\ A_1})\\ \\times \\ Prob\\ (A_1) $$\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14781611",
   "metadata": {},
   "source": [
    "## 4. Out of Vocabulary\n",
    "\n",
    "Suppose we have a new sentence like the below one:<br>\n",
    "<br>\n",
    "> This is **the dog**,<br>\n",
    "> That scared the cat,<br>\n",
    "> That killed the rat,<br>\n",
    "> That ate the malt,<br>\n",
    "> That lay in the house that Jack built.<br>\n",
    "\n",
    "If we calculate the Probability of the above sentence, based on the language model trained on the previous **sample corpus (*The house that Jack built* Nursery Rhyme)**, it would be zero (because \"the dog\" has not occurred in the LM training corpus.\n",
    "\n",
    "By our **chain rule of probabilities** where we keep multiplying probabilities,\n",
    "we would encounter $ P({dog \\over the}) = 0 $ , hence the overall probability of the above example sentence would be zero\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2e548",
   "metadata": {},
   "source": [
    "## One way to avoid, Smoothing!\n",
    "\n",
    "$$ MLE: P({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ ) \\over Count\\ (\\ B\\ )} $$\n",
    "\n",
    "**Why is it called MLE?**\n",
    "* Suppose a bigram “the dog” occurs 5 times in 100 documents.\n",
    "* Given that we have this 100 document corpus (which the language model represents), the maximum likelihood of the bigram parameter “the dog” appearing in the text is 0.05\n",
    "\n",
    "**Add 1 Smoothing (Laplace Smoothing)**\n",
    "$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ 1 \\over Count\\ (\\ B\\ )\\ +\\ |V|} $$\n",
    "* We pretend that each unique bigram occurs once more than it actually did! \n",
    "* Since we have added 1 to each bigram, we have added |V| bigrams in total. Hence normalizing by adding |V| to the denominator\n",
    "* Disadvantage: It reduces the probability of high frequency words\n",
    "\n",
    "\n",
    "**Add- $ \\delta $ Smoothing!**\n",
    "$$ P_{smooth}({B\\  | \\ A}) = {Count\\ (\\ A,\\ B\\ )\\ +\\ \\delta \\over Count\\ (\\ B\\ )\\ +\\ \\delta * |V|} $$\n",
    "* $\\delta$ is any fraction such as 0.1, 0.05, etc.,\n",
    "* Unlike Add one smoothing, this solves the zero prob issue and avoids reducing the prob of high frequency words\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace376d7",
   "metadata": {},
   "source": [
    "## 5. What is Markov Assumption?  \n",
    "\n",
    "So the probability of occurrence of a 4-word sentence, by markov assumption is: \n",
    "$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B,\\ C,\\ D}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C,\\ D}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n",
    "\n",
    "\n",
    "\n",
    "> “What I see NOW depends only on what I saw in the PREVIOUS step”\n",
    "\n",
    "$$ P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1}) = P(w_i,\\ |\\ {w_{i-1}}) $$\n",
    "\n",
    "Hence the probability of occurrence of a 4-word sentence with Markov Assumption is: \n",
    "$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n",
    "\n",
    "- What are its advantages?\n",
    " - Probability of an entire sentence could be very low but individual phrases could be more probable. <br>\n",
    "\tFor e.g.: <br>\n",
    "\t- Actual Data: “The quick fox jumps over the lazy dog”  \n",
    "\t- Probability of a new sentence: Prob(“The quick fox jumps over the lazy cat”) = 0 (though probable to occur,'cat' is not there in vocab, hence zero)\n",
    "\t- In Markov assumption (with additive smoothing), the above sentence will have a realistic probability\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f84c89",
   "metadata": {},
   "source": [
    "## 6. Evaluation \n",
    "\n",
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42aeb2",
   "metadata": {},
   "source": [
    "- Perplexity is a measurement of how well a probability model predicts a sample. \n",
    "- It is used to compare probability models. \n",
    "- A low perplexity indicates the probability distribution is good at predicting the sample       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26733017",
   "metadata": {},
   "source": [
    "Definition: \n",
    "- Perplexity is the inverse probability of  the test set, normalized by the number  of words. <br>\n",
    "Perplexity of test data = PP(test data) =\n",
    "$$ P(w_1,\\ w_2,\\ ....,\\ w_N)^{1 \\over N}  $$\n",
    "$$ $$ \n",
    "$$ {\\sqrt[N]{1 \\over P(w_1,\\ w_2,\\ ....,\\ w_N)}} $$\n",
    "$$ $$\n",
    "$$ {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1})}}}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be245fab",
   "metadata": {},
   "source": [
    "**the lower the perplexity of a LM for predicting a sample, higher is the confidence for that sample to occur in the distribution**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66968538",
   "metadata": {},
   "source": [
    "### Log Probability\n",
    "- But multiplying probabilities like these could end up giving you the result closer to zero. \n",
    " - For e.g.: P(D|C) = 0.1, P(C|B) = 0.07, P(B|A) = 0.05, P(A) = 0.2\n",
    " - P(A before B before C before D before E) = 0.00007 $-->$ too low or almost zero !\n",
    "\n",
    "- Logarithm to the rescue ! \n",
    " - Logarithm is a monotonically increasing function ! \n",
    " - Meaning: If P(E|D) > P(D|C), then log(P(E|D)) > log (P(D|C))\n",
    " - Also, log (A *B) = log (A) + log (B) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be6a6c",
   "metadata": {},
   "source": [
    "![LogarithmChart](https://upload.wikimedia.org/wikipedia/commons/1/17/Binary_logarithm_plot_with_ticks.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9088ec",
   "metadata": {},
   "source": [
    "$$ \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) $$ \n",
    "- where N  is the number of words in the sentence. \n",
    "- Since log (probabilities) are always negative (see graph: log of values less than 1 is negative), shorter sentences will have higher probability of occurrence. \n",
    "\n",
    "**To normalize for length of sentences**, \n",
    "$$ {1 \\over N} \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n",
    "\n",
    "**Higher the log probability value of a LM in predicting a sample, higher is the confidence for that sample to occur in the distribution**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce07f3",
   "metadata": {},
   "source": [
    "**Perplexity and Log-Probability metrics for a Bigram Markov Language Model**\n",
    "\n",
    "$$ Perplexity\\ for\\ a\\ bigram\\ model = {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1}})}}}$$ \n",
    "\n",
    "Comparing with normalized log probability for a bigram model:  \n",
    "$$ LogProbability\\ for\\ a\\ bigram\\ model = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642392a",
   "metadata": {},
   "source": [
    "## 7. Application\n",
    "### Text Classification\n",
    "\n",
    "In this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Bigram_Models--Text_Classification.ipynb), Markov Bigram model is implemented on a text classification problem. We can build a deterministic classifier where class is attributed to the class with highest log probability score.\n",
    "\n",
    "```python\n",
    "print(bigram_markov(\"economic growth slows down\"))\n",
    "```\n",
    "\n",
    "```\n",
    "# log probability score of different classes\n",
    "[('business', -3.9692388514802905),\n",
    " ('politics', -6.7677569438805945),\n",
    " ('tech', -8.341093396600021),\n",
    " ('sport', -9.000286606679415),\n",
    " ('entertainment', -9.09610496174359)]\n",
    "\n",
    "Class attributed: 'business'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "print(bigram_markov(\"prime minister has gone to the US on an official trip\"))\n",
    "```\n",
    "\n",
    "```\n",
    "# log probability score of different classes\n",
    "[('politics', -3.5746871267381852),\n",
    " ('business', -7.440089097987748),\n",
    " ('tech', -10.252850263607193),\n",
    " ('sport', -11.981405120960808),\n",
    " ('entertainment', -12.124273481509913)]\n",
    " \n",
    " Class attributed: 'politics'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06c73f",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "In this [notebook](https://github.com/senthilkumarm1901/LanguageModels_pre_W2V/blob/master/Codes/Char-level%20N-gram%20Language%20Generation%20Model.ipynb), a char-level LM, using trigrams and bigrams of characters, was used to generate dianosour names.\n",
    "\n",
    "```python\n",
    ">> print(names.iloc[:,0].head())\n",
    "\n",
    "# Names of Dianosours used as input\n",
    "0     Aachenosaurus\n",
    "1          Aardonyx\n",
    "2    Abdallahsaurus\n",
    "3       Abelisaurus\n",
    "4     Abrictosaurus\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    ">> char_level_trigram_markov_model(names)\n",
    "\n",
    "# Names of Dianosours generated as output\n",
    "broplisaurus\n",
    "prorkhorpsaurus\n",
    "jintagasaurus\n",
    "carsaurus\n",
    "gapophongasaurus\n",
    "teglangsaurus\n",
    "borudomachsaurus\n",
    "zheisaurus\n",
    "horachillisaurus\n",
    "aveiancsaurus\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03a019",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Wikipedia\n",
    "- [Udemy Course on Deep NLP by Lazy Programmer](https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/) <br>\n",
    "- [NLP Course on Coursera by National Research University Higher School of Economics](https://www.coursera.org/learn/language-processing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98c3ded5f4c982d767ead9cded27e95b53d0df25404a508cedfb98865b9710c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
