{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dfcdf44f",
   "metadata": {},
   "source": [
    "---\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- LLM\n",
    "- AWS\n",
    "- Serverless\n",
    "\n",
    "date: '2024-06-19'\n",
    "description: In the blog post, I explore the compute environments for the plethora of OpenSource LLMs in the market \n",
    "output-file: 2024-06-17-how-to-host-open-source-llms-in-aws.html\n",
    "title: The Mental Model for Leveraging LLMs in Cloud\n",
    "toc: true\n",
    "image: images/opensource_llm/the_mental_model_for_open_source_llms_3.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee81bc",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "I. `The Tasks, Models and Compute` Thought Process <br>\n",
    "II. **Tasks**: Predictive vs Generative Tasks <br>\n",
    "III. **Models**: Evolution of LLMs - from a size-centric view <br>\n",
    "IV. **Compute**: Architecting LLM Applications in Cloud <br>\n",
    "- The Mental Model for Hosting in Cloud <br>\n",
    "- The Compute Environments in AWS <br>\n",
    "- Marriage between **Models** and **Compute** <br>\n",
    "V. Conclusion <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652146e",
   "metadata": {},
   "source": [
    "## I.The Initial Thought Process\n",
    "\n",
    "- The below thought process diagram is a simplified first step to a Model Selection. \n",
    "- I have kept the following thoughts out of scope, for now. But for a reader, they may be super important\n",
    "    - Do you need domain-specific models (relevant if your application is run in healthcare/finance/law etc.,)?\n",
    "    - Why does it have to be just one of the 2 options - `Purpose-built/Customized LLM` and `Prompt-based General Purpose LLM`? \n",
    "    - Why not a `Prompt-based General Purpose LLM customized with RAG or Fine-tuning`\n",
    "\n",
    "![](./images/opensource_llm/initial_thought_process.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23c1e3",
   "metadata": {},
   "source": [
    "##  II. The Debate on `Tasks`\n",
    "\n",
    "![](./images/opensource_llm/the_tasks.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: My fav NLP Researcher / SpaCy Founder - Ines Motwani in a PyCon'24 </sup> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ddc572",
   "metadata": {},
   "source": [
    "##  III. The Debate on `Models`: Evolution of LLMs - from the pov of their Memory Footprint\n",
    "\n",
    "![](./images/opensource_llm/the_mental_model_for_open_source_llms_3.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](./images/opensource_llm/quantizations_model_size.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i> <br>\n",
    "\n",
    "<i> <sup> For a better read on Quantizations: Refer [Introduction to Post Training Quantization Medium Article](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c) </sup>  </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e10dd4",
   "metadata": {},
   "source": [
    "##  IV. The Debate on `Compute`: Architecting LLM Applications in Cloud\n",
    "\n",
    "\n",
    "<h4 style=\"text-align: center;\"> A Cloud Agnostic View </h4>\n",
    "\n",
    "![](./images/opensource_llm/compute_environments_in_cloud_2.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>\n",
    "\n",
    "---\n",
    "\n",
    "<h4 style=\"text-align: center;\"> An AWS-specific View </h4>\n",
    "\n",
    "![](./images/opensource_llm/compute_environments_in_aws.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03110db7",
   "metadata": {},
   "source": [
    "###  The Marriage between Compute Environments and LLMs \n",
    "\n",
    "<h4 style=\"text-align: center;\"> Different Sized LLMs and their Compute Options in AWS </h4>\n",
    "\n",
    "![](./images/opensource_llm/size_of_llms_vs_compute_env_2.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e265e",
   "metadata": {},
   "source": [
    "##  V. Conclusion \n",
    "\n",
    "- In this blog, we have seen the different LLM sizes out in the market and their compute capacity needed above. \n",
    "- ToDo: In a future blog, I could focus also on the right sized EC2/ SageMaker instances for different LLMs discussed above. \n",
    "- ToDo: It would also be a good continuation of this blog to focus on `Efficient LLM Inferencing` options like below\n",
    "\n",
    "```\n",
    "llama.cpp\n",
    "ollama\n",
    "mistral.rs\n",
    "vLLM\n",
    "PyTorch Mobile\n",
    "Tensorflow Lite\n",
    "Apple Core ML\n",
    "Windows DirectML\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
