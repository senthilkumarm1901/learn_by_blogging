{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dfcdf44f",
   "metadata": {},
   "source": [
    "---\n",
    "author: Senthil Kumar\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- LLM\n",
    "- AWS\n",
    "- Serverless\n",
    "\n",
    "date: '2024-06-19'\n",
    "description: In this blog post, we are exploring the intersection of different sized LLMs and their optimal compute environments for deployment\n",
    "output-file: 2024-06-17-how-to-host-open-source-llms-in-aws.html\n",
    "title: The Mental Model for Leveraging LLMs in Cloud\n",
    "toc: true\n",
    "image: images/opensource_llm/the_mental_model_for_open_source_llms_3.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee81bc",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "I. `The Tasks, Models and Compute` Thought Process <br>\n",
    "II. **Tasks**: Predictive vs Generative Tasks <br>\n",
    "III. **Models**: Evolution of LLMs - from a size-centric view <br>\n",
    "IV. **Compute**: Cloud Deployment Landscape for LLMs <br>\n",
    "- The Mental Model for Deploying in Cloud <br>\n",
    "- The Compute Environments in AWS <br>\n",
    "- Marriage between **Models** and **Compute** <br>\n",
    "V. Conclusion <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652146e",
   "metadata": {},
   "source": [
    "## I. The Thought Process for use of LLM in applications  -  `Task, Model & Compute`\n",
    "\n",
    "- The below thought process diagram is inspired from the [`GenAI Project Life Cycle Diagram by Deeplearning.ai course in Coursera`](https://global.discourse-cdn.com/dlai/original/3X/0/b/0bdc336f4a2f98054469d7a73618993f18aead1a.png)\n",
    "\n",
    "---\n",
    "\n",
    "![](./images/opensource_llm/initial_thought_process_simple.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](./images/opensource_llm/initial_thought_process.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate | Inspired from the [`GenAI Project Life Cycle Diagram by Deeplearning.ai course in Coursera`](http://localhost:8888/?token=9333dede515651d759db8d4bfd175dd316abc03cb275a301) </sup> </i>\n",
    "\n",
    "\n",
    "**Out of Scope**: \n",
    "\n",
    "- I have kept the following thoughts out of scope, for now. But for a reader, they may be very important, hence could be factored into the Thought Process\n",
    "    - Do you need domain-specific models (relevant if your application is run in healthcare/finance/law etc.,)?\n",
    "    - Why does it have to be just one of the 2 options - `Purpose-built/Customized LLM` and `Prompt-based General Purpose LLM`? \n",
    "        - Why not a `Prompt-based General Purpose LLM customized with RAG or Fine-tuning`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b988db",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23c1e3",
   "metadata": {},
   "source": [
    "##  II. The Discussion on `Tasks`\n",
    "\n",
    "![](./images/opensource_llm/the_tasks.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Inspired from my fav NLP Researcher / SpaCy Founder - Ines Motwani in a QCon'24 London Talk | Refer [Slide 50](https://speakerdeck.com/inesmontani/the-ai-revolution-will-not-be-monopolized-how-open-source-beats-economies-of-scale-even-for-llms-qcon-london?slide=50) </sup> </i>\n",
    "\n",
    "\n",
    "**Some examples when Generative AI Is Not Needed (Traditional ML Suffices)**:\n",
    "\n",
    "- Predictive Analytics: Forecasting future values, like stock prices or equipment failure.\n",
    "- Classification: Assigning labels to data, such as spam detection or image classification.\n",
    "- Recommendation Systems: Suggesting products or content based on user behavior.\n",
    "- Anomaly Detection: Identifying outliers, like fraud detection or quality control.\n",
    "- Optimization and Causal Inference: Solving problems like route optimization or understanding cause-and-effect relationships.\n",
    "\n",
    "\n",
    "**Some examples when Generative AI Is Needed (Traditional ML won't be enough)**:\n",
    "- Content Creation: Generating new text, images, music, or other creative outputs.\n",
    "- Data Augmentation: Creating synthetic data to enhance training datasets.\n",
    "- Personalization: Customizing content or interactions based on user preferences.\n",
    "- Simulations and Scenario Generation: Creating dynamic and realistic training or testing environments.\n",
    "- Creative Problem Solving and Design: Exploring innovative solutions, designs, or artistic ideas.\n",
    "\n",
    "Source: Reply from GPT 4o for the prompt - \"Can you summarize in 5 bullet points when is Gen AI needed and when it is not\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedd67b",
   "metadata": {},
   "source": [
    "> [1] Food for thought: Would you use a bull-dozer to mow a lawn? That is just a waste of resource, honestly. <br> \n",
    "> [2] Food for thought: A slide from Ines Montani's presentation in Data Hack Summit 2024 on Generative vs Predictive Tasks. Refer [here](https://speakerdeck.com/inesmontani/applied-nlp-in-the-age-of-generative-ai?slide=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449c339",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ddc572",
   "metadata": {},
   "source": [
    "##  III. The Discussion on `Models`\n",
    "\n",
    "### Evolution of LLMs - A Memory Footprint centric View\n",
    "\n",
    "![](./images/opensource_llm/the_mental_model_for_open_source_llms_4.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>\n",
    "\n",
    "\n",
    "**Key Interpretations**: \n",
    "- As we go right in the above diagram, the models become bigger and the tasks become more generic and complex\n",
    "- As models become bigger, they are highly likely to be Closed Source than Open Source (LLM weights, training methodology are shared)\n",
    "\n",
    "#### Firstly, how is model memory size computed: \n",
    "\n",
    "- $$\\text{Parameters in billions}   \\times  \\, \\text{Floating Point(FP) Precision of each parameter in byte} =  \\, \\text{Model Memory Size in GB}$$ <br>\n",
    "\n",
    "- $$\\text{Total Memory taken by the model} = \\text{Model Memory Size in GB} + \\text{Memory for other components of the model}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### The Model Sizes can be reduced by Quantization\n",
    "\n",
    "![](./images/opensource_llm/quantizations_model_size.png)\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's simplified take | Open for debate </sup> </i> <br>\n",
    "\n",
    "\n",
    "\n",
    "#### How is a model quantized:\n",
    "\n",
    "Let us look at a **Post-training Quantization called `Weight Quantization` used by Ollama**: \n",
    "\n",
    "- In weight-based quantization, the weights of a trained model are converted to lower precision without requiring any retraining. While this approach is simple, it can result in model performance degradation. \n",
    "\n",
    "Below is a list of **Model Options** possible to served by [Ollama](https://github.com/ollama/ollama/) - one of the popular LLM inferencing framework options.\n",
    "\n",
    "\n",
    "| Model              | Parameters | Size  | Download                       |\n",
    "| ------------------ | ---------- | ----- | ------------------------------ |\n",
    "| Llama 3            | 8B         | 4.7GB | `ollama run llama3`            |\n",
    "| Llama 3            | 70B        | 40GB  | `ollama run llama3:70b`        |\n",
    "| Phi 3 Mini         | 3.8B       | 2.3GB | `ollama run phi3`              |\n",
    "| Phi 3 Medium       | 14B        | 7.9GB | `ollama run phi3:medium`       |\n",
    "\n",
    "\n",
    "- The default quantization offered by Ollama is INT4 (specifically `Q4_0`) [1](https://github.com/ollama/ollama/issues/5425)\n",
    "- For example, for Phi 3 Mini consisting of 3.8B parameters, the math comes to: $\\text{Memory} = 3.8B \\times 0.5 \\, \\text{byte} = 1.9 \\, \\text{GB}$ for Storage. \n",
    "- In the case of Phi3 Mini in Ollama's implementation, there could be additional memory occupied by **tokenizer**, **embeddings**, **metadata**, and **any additional layers or features** included in the model - making it 2.3 GB when using inside Ollama\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sources: <br>\n",
    "\n",
    "<i> <sup> 1. There are other quantization options offered by Ollama as well. Refer [GitHub Ollama Issue Discussion here](https://github.com/ollama/ollama/issues/5425) for an interesting debate</sup>  </i>\n",
    "\n",
    "\n",
    "<i> <sup> 2. Another Quantization Method `Dense and Sparse` Refer [GitHub link](https://github.com/SqueezeAILab/SqueezeLLM) and [paper link](https://arxiv.org/pdf/2306.07629v2) </sup>  </i>\n",
    "\n",
    "\n",
    "<i> <sup> 3. For a better read on the Math behind Quantizations: Refer [Introduction to Post Training Quantization Medium Article](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c)</sup>  </i>\n",
    "\n",
    "<i> <sup> 4. Do try the Deeplearning.ai short courses centered around Quantization, if interested more</sup>  </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e10dd4",
   "metadata": {},
   "source": [
    "##  IV. The Debate on `Compute` power needed for the Models\n",
    "\n",
    "### Architecting LLM Applications in Cloud :: A Cloud Agnostic View\n",
    "\n",
    "\n",
    "<h4 style=\"text-align: center;\"> A Cloud Agnostic View </h4>\n",
    "\n",
    "![](./images/opensource_llm/compute_environments_in_cloud_3.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>\n",
    "\n",
    "\n",
    "**Key Interpretations**: \n",
    "\n",
    "- Typically, `Purpose-built Models` are deployed in dedicated instances\n",
    "- The Cloud providers have access to Foundation Models (some of those models are even Open Source as well) which are provided to users as pre-built APIs\n",
    "- Typically Serverless APIs scale with demand, but their performance cannot be guaranteed during sudden spikes. \n",
    "- In those cases, `Provisioned Throughput` can guarantee to meet those higher demands without degradation in performance. Provisioned Throughput comes with higher baseline cost compared to truly Serverless options\n",
    "- One nuance to note: Mostly the Fine-tuned LLMs, even if the foundation model is Serverless, needs `Provisioned Throughput`. Imagine as if one of the nodes/instances saves your fine-tuned model and the `Provisioned Throughput` scales the contents of that node when the demand increases\n",
    "\n",
    "Cloud Providers offering the LLMs as APIs:\n",
    "- [Azure AI Foundation Models](https://azure.microsoft.com/en-us/products/ai-model-catalog)\n",
    "- [AWS Bedrock Foundation Models](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html)\n",
    "- [GCP Model Garden](https://cloud.google.com/model-garden)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### How are the Compute Environments stacked in AWS for LLM Inferencing\n",
    "\n",
    "<h4 style=\"text-align: center;\"> An AWS-specific View </h4>\n",
    "\n",
    "![](./images/opensource_llm/compute_environments_in_aws.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aaa93b",
   "metadata": {},
   "source": [
    "**Key Interpretations**:\n",
    "\n",
    "- More Devops/MLOps skills are needed for Dedicated Instances. Serverless eases that skill set burden by helping with `AWS Managed Scaling` \n",
    "- Serverless options like `Lambda` and `Fargate` can work for Task specific (Purpose-built Small models)^ and Small Language Models^ \n",
    "- Refer my Serverless attempts for \n",
    "    - [Task-specific Model in AWS Lambda](https://github.com/senthilkumarm1901/aws_serverless_recipes/tree/main/container_lambda_anonymize_text), \n",
    "    - [Small LM example in AWS Lambda](https://github.com/senthilkumarm1901/aws_serverless_recipes/tree/main/container_lambda_to_run_slm) and \n",
    "    - [Small LM with RAG in AWS Lambda](https://github.com/senthilkumarm1901/aws_serverless_recipes/tree/main/container_lambda_to_run_rag_slm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86a37d",
   "metadata": {},
   "source": [
    "###  The Marriage between Compute Environments and LLMs \n",
    "\n",
    "<h4 style=\"text-align: center;\"> Different Sized LLMs and their Compute Options in AWS </h4>\n",
    "\n",
    "![](./images/opensource_llm/size_of_llms_vs_compute_env_2.png)\n",
    "\n",
    "<i style=\"text-align: center;\"> <sup> Source: Author's take | Open for debate </sup> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be447ef",
   "metadata": {},
   "source": [
    "**Key Interpretations**: \n",
    "- The upper half of the diagram comprising different Cloud Deployment options is mapped to the lower half consisting of different sized models.\n",
    "\n",
    "- For lower sized models, \n",
    "    - Task-specific Models can work with Serverless In-house Option like Serverless SageMaker Inference (there are limitations like only CPU and utmost only 6GB memory | refer) and AWS Lambda (with upto 10 GB memory possible).\n",
    "\n",
    "- On the other end of the size, \n",
    "    - Very Large Language Models are currently possible only as Serverless APIs across Cloud providers. In other words, we cannot host a GPT 4 model in our cloud environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e265e",
   "metadata": {},
   "source": [
    "##  V. Conclusion \n",
    "\n",
    "- In this blog, we have seen the different types of tasks that a model addresses, and which among those tasks have the need for Generative AI.\n",
    "- Also, we have covered how the LLMs, coming in various sizes, can be deployed in Cloud. \n",
    "\n",
    "Potential **Next Steps** for the Author (even for the reader):\n",
    "\n",
    "- Good to focus on the right sized EC2/ SageMaker instances for different LLMs discussed above. \n",
    "    - For example, what is the minimum-sized and recommended compute instances for \n",
    "        - [`Phi-3-mini-4k-instruct-gguf`](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) and \n",
    "        - [Phi-3-mini-128k-instruct-onnx](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)\n",
    "\n",
    "- It would also be a good continuation of this blog to focus on `Efficient LLM Inferencing` options like below\n",
    "\n",
    "```\n",
    "# some popular LLM inference frameworks\n",
    "\n",
    "- llama.cpp\n",
    "- ollama\n",
    "- mistral.rs\n",
    "- vLLM\n",
    "```\n",
    "\n",
    "```\n",
    "# some popular technologies to make machine learning models more portable and efficient \n",
    "# across different hardware and software requirements\n",
    "\n",
    "- ONNX (Open Neural Network Exchange) - an open format designed to represent machine learning models that provides interoperability between different ML frameworks like PyTorch and Tensorflow.\n",
    "- GGUF (Generic Graph Update Format) - a format used for representing and updating machine learning models, particularly useful for smaller language models that can run effectively on CPUs with 4-8bit quantization.\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "# some popular Efficient Machine learning frameworks or libraries \n",
    "# designed to run ML models efficiently on mobile and edge devices\n",
    "\n",
    "- PyTorch Mobile\n",
    "- Tensorflow Lite\n",
    "- Apple Core ML\n",
    "- Windows DirectML\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
